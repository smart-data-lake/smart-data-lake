/*
 * Smart Data Lake - Build your data lake the smart way.
 *
 * Copyright Â© 2019-2020 ELCA Informatique SA (<https://www.elca.ch>)
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program. If not, see <http://www.gnu.org/licenses/>.
 */
package io.smartdatalake.workflow.dataobject

import com.typesafe.config.Config
import io.smartdatalake.config.SdlConfigObject.{ConnectionId, DataObjectId}
import io.smartdatalake.config.{FromConfigFactory, InstanceRegistry}
import io.smartdatalake.definitions.SDLSaveMode
import io.smartdatalake.definitions.SDLSaveMode.SDLSaveMode
import io.smartdatalake.util.hdfs.{PartitionValues, SparkRepartitionDef}
import io.smartdatalake.util.misc.AclDef
import io.smartdatalake.util.spark.DataFrameUtil
import io.smartdatalake.workflow.ActionPipelineContext
import io.smartdatalake.workflow.dataframe.GenericSchema
import org.apache.spark.sql.DataFrame

/**
 * A [[DataObject]] backed by an Microsoft Excel data source.
 *
 * It manages read and write access and configurations required for [[io.smartdatalake.workflow.action.Action]]s to
 * work on Microsoft Excel (.xslx) formatted files.
 *
 * Reading and writing details are delegated to Apache Spark [[org.apache.spark.sql.DataFrameReader]]
 * and [[org.apache.spark.sql.DataFrameWriter]] respectively. The reader and writer implementation is provided by the
 * [[https://github.com/crealytics/spark-excel Crealytics spark-excel]] project.
 *
 * Read Schema:
 *
 * When `useHeader` is set to true (default), the reader will use the first row of the Excel sheet as column names for
 * the schema and not include the first row as data values. Otherwise the column names are taken from the schema.
 * If the schema is not provided or inferred, then each column name is defined as "_c#" where "#" is the column index.
 *
 * When a data object schema is provided, it is used as the schema for the DataFrame. Otherwise if `inferSchema` is
 * enabled (default), then the data types of the columns are inferred based on the first `excerptSize` rows
 * (excluding the first).
 * When no schema is provided and `inferSchema` is disabled, all columns are assumed to be of string type.
 *
 * @param excelOptions Settings for the underlying [[org.apache.spark.sql.DataFrameReader]] and [[org.apache.spark.sql.DataFrameWriter]].
 */
case class ExcelFileDataObject(override val id: DataObjectId,
                               override val path: String,
                               excelOptions: ExcelOptions = ExcelOptions(),
                               override val partitions: Seq[String] = Seq(),
                               override val schema: Option[GenericSchema] = None,
                               override val schemaMin: Option[GenericSchema] = None,
                               override val saveMode: SDLSaveMode = SDLSaveMode.Overwrite,
                               override val sparkRepartition: Option[SparkRepartitionDef] = Some(SparkRepartitionDef(numberOfTasksPerPartition = 1)),
                               override val acl: Option[AclDef] = None,
                               override val connectionId: Option[ConnectionId] = None,
                               override val filenameColumn: Option[String] = None,
                               override val expectedPartitionsCondition: Option[String] = None,
                               override val housekeepingMode: Option[HousekeepingMode] = None,
                               override val metadata: Option[DataObjectMetadata] = None
                              )(@transient implicit override val instanceRegistry: InstanceRegistry)
  extends SparkFileDataObject {

  override val format = "com.crealytics.spark.excel"

  override val fileName: String = "*.xls*"

  // spark excel data source doesnt support reading all files in a directory. Each file must be read one by one.
  override val handleFilesOneByOne: Boolean = true

  override val options: Map[String, String] = excelOptions.toMap(schema).filter {
      case (_, v) => v.isDefined
  }.mapValues(_.get.toString).map(identity) // make serializable

  override def afterRead(df: DataFrame)(implicit context: ActionPipelineContext): DataFrame = {
    val dfSuper = super.afterRead(df)

    // cleanup header names
    val newNames = dfSuper.columns.map(name => DataFrameUtil.strCamelCase2LowerCaseWithUnderscores(cleanHeaderName(name)))
    dfSuper.toDF(newNames: _ *)
  }

  /**
   * Checks preconditions before writing.
   */
  override def beforeWrite(df: DataFrame)(implicit context: ActionPipelineContext): DataFrame = {
    val dfSuper = super.beforeWrite(df)

    // check for unsupported write options
    require(excelOptions.startColumn.isEmpty, s"($id) Writing Excel Files with startColumn defined is not supported.")
    require(excelOptions.numLinesToSkip.isEmpty, s"($id) Writing Excel Files with numLinesToSkip defined is not supported.")

    // return
    dfSuper
  }

  private val validHeaderChars = ('a' to 'z') ++ ('A' to 'Z') ++ ('0' to '9') ++ Seq('_')

  private val cleanHeaderName = (name: String) => {
    name.map {
      case c if " -.".contains(c) => '_' case c => c
    }.filter(validHeaderChars.contains)
  }

  override def factory: FromConfigFactory[DataObject] = ExcelFileDataObject
}

object ExcelFileDataObject extends FromConfigFactory[DataObject] {
  override def fromConfig(config: Config)(implicit instanceRegistry: InstanceRegistry): ExcelFileDataObject = {
    extract[ExcelFileDataObject](config)
  }
}

/**
 * Options passed to [[org.apache.spark.sql.DataFrameReader]] and [[org.apache.spark.sql.DataFrameWriter]] for
 * reading and writing Microsoft Excel files. Excel support is provided by the spark-excel project (see link below).
 *
 * @param sheetName Optional name of the Excel Sheet to read from/write to.
 * @param numLinesToSkip Optional number of rows in the excel spreadsheet to skip before any data is read.
 *                       This option must not be set for writing.
 * @param startColumn Optional first column in the specified Excel Sheet to read from (as string, e.g B).
 *                    This option must not be set for writing.
 * @param endColumn Optional last column in the specified Excel Sheet to read from (as string, e.g. F).
 * @param rowLimit Optional limit of the number of rows being returned on read.
 *                 This is applied after `numLinesToSkip`.
 * @param useHeader If `true`, the first row of the excel sheet specifies the column names (default: true).
 * @param treatEmptyValuesAsNulls Empty cells are parsed as `null` values (default: true).
 * @param inferSchema Infer the schema of the excel sheet automatically (default: true).
 * @param timestampFormat A format string specifying the format to use when writing timestamps (default: dd-MM-yyyy HH:mm:ss).
 * @param dateFormat A format string specifying the format to use when writing dates.
 * @param maxRowsInMemory The number of rows that are stored in memory.
 *                        If set, a streaming reader is used which can help with big files.
 * @param excerptSize Sample size for schema inference.
 * @see [[https://github.com/crealytics/spark-excel]]
 */
case class ExcelOptions(
                         sheetName: Option[String] = None,
                         numLinesToSkip: Option[Int] = None,
                         startColumn: Option[String] = None,
                         endColumn: Option[String] = None,
                         rowLimit: Option[Int] = None,
                         useHeader: Boolean = true,
                         treatEmptyValuesAsNulls: Option[Boolean] = Some(true),
                         inferSchema: Option[Boolean] = Some(true),
                         timestampFormat: Option[String] = Some("dd-MM-yyyy HH:mm:ss"),
                         dateFormat: Option[String] = None,
                         maxRowsInMemory: Option[Int] = None,
                         excerptSize: Option[Int] = None
                       ) {

  require(!startColumn.exists(_.exists(c => !c.isLetter)), s"ExcelOptions.startColumn must contain only letters (A-Z)+, but is ${startColumn.get}")
  require(!endColumn.exists(_.exists(c => !c.isLetter)), s"ExcelOptions.endColumn must contain only letters (A-Z)+, but is ${endColumn.get}")

  def getDataAddress: Option[String] = {
    if (sheetName.isDefined || startColumn.isDefined || endColumn.isDefined || numLinesToSkip.isDefined || rowLimit.isDefined) {
      val startLine = numLinesToSkip.map(_+1)
      val endLine = rowLimit.map(_+startLine.getOrElse(1))
      val xSheet = sheetName.map(name => s"'${name.trim}'!")
      val startAreaDefined = xSheet.orElse(startColumn).orElse(startLine).orElse(endColumn).orElse(endLine).isDefined
      val xStartArea = if (startAreaDefined) Some(startColumn.getOrElse("A") + startLine.getOrElse(1)) else None
      val endAreaDefined = endColumn.orElse(endLine).isDefined
      val xEndArea = if (endAreaDefined) Some(":" + endColumn.getOrElse("ZZ") + endLine.getOrElse(100000)) else None
      Some( xSheet.getOrElse("") + xStartArea.getOrElse("") + xEndArea.getOrElse(""))
    } else None
  }

  def toMap(schema: Option[GenericSchema]): Map[String, Option[Any]] = Map(
      "dataAddress" -> getDataAddress,
      "treatEmptyValuesAsNulls" -> treatEmptyValuesAsNulls,
      "header" -> Some(useHeader),
      "inferSchema" -> Some(schema.isEmpty && inferSchema.getOrElse(true)),
      "treatEmptyValuesAsNulls" -> treatEmptyValuesAsNulls,
      "timestampFormat" -> timestampFormat,
      "dateFormat" -> dateFormat,
      "maxRowsInMemory" -> maxRowsInMemory,
      "excerptSize" -> excerptSize
    )
}
