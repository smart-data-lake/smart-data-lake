<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Smart Data Lake Builder Blog</title>
        <link>https://www.smartdatalake.ch/blog</link>
        <description>Smart Data Lake Builder Blog</description>
        <lastBuildDate>Wed, 06 Apr 2022 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Combine Spark and Snowpark to ingest and transform data in one pipeline]]></title>
            <link>https://www.smartdatalake.ch/blog/sdl-snowpark</link>
            <guid>sdl-snowpark</guid>
            <pubDate>Wed, 06 Apr 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[An example to create one unified data pipeline that uses Spark to ingest data into Snowflake, and Snowpark to transform data inside Snowflake.]]></description>
            <content:encoded><![CDATA[<p>This article shows how to create one unified data pipeline that uses Spark to ingest data into Snowflake, and Snowpark to transform data inside Snowflake.</p><p>Recent developments in Smart Data Lake Builder (SDLB) included refactorings to integrate alternative execution engines to Spark.
In particular <a href="https://docs.snowflake.com/en/developer-guide/snowpark/index.html" target="_blank" rel="noopener noreferrer">Snowpark</a> integration was implemented, a Spark like DataFrame API for implementing transformations in Snowflake.</p><p>Implementing transformations in Snowflake has big performance and cost benefits. And using a DataFrame API is much more powerful than coding in SQL, see also <a href="https://medium.com/towards-data-science/modern-data-stack-which-place-for-spark-8e10365a8772" target="_blank" rel="noopener noreferrer">Modern Data Stack: Which Place for Spark?</a>.</p><p>Snowpark is good for transforming data inside Snowflake, but not all data might be located in Snowflake and suitable for Snowflake.
Here it is interesting to use Spark and its many connectors, in particular to ingest and export data.</p><p>Combining Spark and Snowpark in a smart data pipeline using a DataFrame API would be the ideal solution.
With the integration of Snowpark as engine in SDLB we created just that. </p><p>This blog post will show how to migrate our example data pipeline of the <a href="/docs/getting-started/setup">Getting Started</a> guide Part 1 to use Spark for ingestion and Snowpark for transformation.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="prerequisites">Prerequisites<a class="hash-link" href="#prerequisites" title="Direct link to heading">​</a></h2><ul><li>Create a Snowflake trial account on <a href="https://signup.snowflake.com/" target="_blank" rel="noopener noreferrer">https://signup.snowflake.com/</a> and note the following connection informations:<ul><li>Account URL (copy by navigating to "Organization" and clicking the link symbol on the right of the account name)</li><li>Username</li><li>Password</li></ul></li><li>Create database "testdb" in Snowflake: <code>create database testdb;</code></li><li>Create schema "testdb.test" in Snowflake: <code>create schema testdb.test;</code></li><li>Setup running SDLB docker image with part-1 configuration as described in <a href="/docs/getting-started/setup">Getting Started</a><ul><li>build sdl-spark image</li><li>copy final application.conf of part-1: <code>cp config/application.conf.part-1-solution config/application.conf</code></li><li>run download actions with parameter <code>--feed-sel download</code></li><li>run compute actions with parameter <code>--feed-sel compute</code></li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="goal">Goal<a class="hash-link" href="#goal" title="Direct link to heading">​</a></h2><p>The example of part-1 has the following DataObjects</p><p>Staging Layer</p><ul><li>stg-departures: JsonFileDataObject</li><li>stg-airports: CsvFileDataObject</li></ul><p>Integration Layer</p><ul><li>int-airports: CsvFileDataObject</li></ul><p>Business Transformation Layer</p><ul><li>btl-departures-arrivals-airports: CsvFileDataObject</li><li>btl-distances: CsvFileDataObject</li></ul><p>In this example we will migrate Integration and Business Transformation Layer to Snowflake.
We will use Spark to fill Staging and Integration Layer, and Snowpark for transformation from Integration to Business Transformation Layer.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="prepare-the-snowflake-library">Prepare the Snowflake library<a class="hash-link" href="#prepare-the-snowflake-library" title="Direct link to heading">​</a></h2><p>First we have add SDLBs Snowflake library to the projects pom.xml dependencies section:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;dependencies&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ....</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;dependency&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &lt;groupId&gt;io.smartdatalake&lt;/groupId&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &lt;artifactId&gt;sdl-snowflake_${scala.minor.version}&lt;/artifactId&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &lt;version&gt;${project.parent.version}&lt;/version&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;/dependency&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;/dependencies&gt;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Then SDLB version needs to be updated to version 2.3.0-SNAPSHOT at least in the parent section:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;parent&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;groupId&gt;io.smartdatalake&lt;/groupId&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;artifactId&gt;sdl-parent&lt;/artifactId&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;version&gt;2.3.0-SNAPSHOT&lt;/version&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;/parent&gt;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="define-snowflake-connection">Define Snowflake connection<a class="hash-link" href="#define-snowflake-connection" title="Direct link to heading">​</a></h2><p>To define the Snowflake connection in config/application.conf, add connections section with connection "sf-con", and fill in informations according to prerequisits:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  connections {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    sf-con {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      type = SnowflakeTableConnection</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      url = "&lt;accountUrl&gt;",</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      warehouse = "COMPUTE_WH",</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      database = "testdb",</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      role = "ACCOUNTADMIN",</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      authMode = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        type = BasicAuthMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        userVariable = "CLEAR#&lt;username&gt;"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        passwordVariable = "CLEAR#&lt;pwd&gt;"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="migrate-dataobjects">Migrate DataObjects<a class="hash-link" href="#migrate-dataobjects" title="Direct link to heading">​</a></h2><p>Now we can change the DataObject type to SnowflakeTableDataObject and the new Snowflake connection, adding the definition of the table:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  int-airports {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = SnowflakeTableDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    connectionId = sf-con</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      db = "test"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      name = "int_airports"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  btl-departures-arrivals-airports {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = SnowflakeTableDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    connectionId = sf-con</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      db = "test"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      name = "btl_departures_arrivals_airports"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  btl-distances {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = SnowflakeTableDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    connectionId = sf-con</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      db = "test"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      name = "btl_distances"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the attribute <code>db</code> of the SnowflakeTableDataObject should be filled with the schema of the Snowflake table and that this is <em>not</em> the same as the attribute <code>database</code> of SnowflakeTableConnection. </p><h2 class="anchor anchorWithStickyNavbar_mojV" id="migrating-actions">Migrating Actions<a class="hash-link" href="#migrating-actions" title="Direct link to heading">​</a></h2><p>The new SDLB version introduced some naming changes:</p><ul><li><p>The CustomSparkAction can now also process Snowpark-DataFrames and is therefore renamed to CustomDataFrameAction.</p></li><li><p>The ScalaClassDfTransformer was specific for Spark. In the new SDLB version there is a specific scala-class DataFrame transformer for Spark and Snowpark, e.g. ScalaClassSparkDfTransformer and ScalaClassSnowparkDfTransformer. And there is even a ScalaClassGenericDfTransformer to implement transformations using a unified API. In our case we will migrate the transformation to use Snowpark and set the type to ScalaClassSnowparkDfTransformer.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">join-departures-airports {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  type = CustomSparkAction -&gt; CustomDataFrameAction</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">compute-distances {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  transformers = [{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = ScalaClassDfTransformer -&gt; ScalaClassSnowparkDfTransformer</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div></li></ul><p>There is no need to change the SQL transformtions of join-departures-airport, as the SQL should run on Snowpark aswell.</p><p>On the other hand the ComputeDistanceTransformer was implemented with the Spark DataFrame API. We need to migrate it to Snowpark DataFrame API to run this Action with Snowpark. Luckily the API's are very similar. Often it's sufficient to change the import statement, the class we're extending and the session parameters type:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  import com.snowflake.snowpark.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import com.snowflake.snowpark.{DataFrame, Session}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import io.smartdatalake.workflow.action.snowflake.customlogic.CustomSnowparkDfTransformer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  class ComputeDistanceTransformer extends CustomSnowparkDfTransformer {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def transform(session: Session, options: Map[String, String], df: DataFrame, dataObjectId: String) : DataFrame = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ...</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>If you have UDFs in your code, it gets trickier. The UDF Code gets serialized to Snowflake, details see <a href="https://docs.snowflake.com/en/developer-guide/snowpark/scala/creating-udfs.html" target="_blank" rel="noopener noreferrer">Snowpark UDFs</a>. Special care must be taken to minimize the scope the UDF is defined in. Thats why we move the function into the companion object.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  object ComputeDistanceTransformer {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def calculateDistanceInKilometer(depLat: Double, depLng: Double, arrLat: Double, arrLng: Double): Double = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val AVERAGE_RADIUS_OF_EARTH_KM = 6371</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val latDistance = Math.toRadians(depLat - arrLat)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val lngDistance = Math.toRadians(depLng - arrLng)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val a = Math.sin(latDistance / 2) * Math.sin(latDistance / 2) + Math.cos(Math.toRadians(depLat)) * Math.cos(Math.toRadians(arrLat)) * Math.sin(lngDistance / 2) * Math.sin(lngDistance / 2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      AVERAGE_RADIUS_OF_EARTH_KM * c</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def getCalculateDistanceInKilometerUdf(session: Session) = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      // using only udf(...) function results in "SnowparkClientException: Error Code: 0207, Error message: No default Session found. Use &lt;session&gt;.udf.registerTemporary() to explicitly refer to a session."</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      session.udf.registerTemporary(ComputeDistanceTransformer.calculateDistanceInKilometer _)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that we need to pass the Session to a function for registering the UDF. There is an Error 0207 if we use "udf" function (at least in snowpark version 1.2.0).
Finally we need to adapt the call of the UDF as follows:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">df.withColumn("distance", ComputeDistanceTransformer.getCalculateDistanceInKilometerUdf(session)(col("dep_latitude_deg"),col("dep_longitude_deg"),col("arr_latitude_deg"), col("arr_longitude_deg")))</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="compile-and-run">Compile and run<a class="hash-link" href="#compile-and-run" title="Direct link to heading">​</a></h2><p>Time to see if it works.
Lets build an update SDLB docker image with the updated SDLB version:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  podman build -t sdl-spark .</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Then compile the code with the UDF:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  mkdir .mvnrepo</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  podman run -v ${PWD}:/mnt/project -v ${PWD}/.mvnrepo:/mnt/.mvnrepo maven:3.6.0-jdk-11-slim -- mvn -f /mnt/project/pom.xml "-Dmaven.repo.local=/mnt/.mvnrepo" package</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Download initial data with <code>--feed-sel download</code>:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  podman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel download</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Compute with <code>--feed-sel compute</code>:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  podman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel compute</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>If the SDLB run was SUCCESSFUL, you should now see TEST.BTL_DISTANCES table in Snowpark.
To check that Spark was used for Action select-airport-cols and Snowpark for Action compute-distances, look for the following logs, e.g. SnowparkSubFeed for Action~compute-distances: </p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  INFO  CopyAction - (Action~compute-distances) selected subFeedType SnowparkSubFeed [init-compute-distances]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h1>Engine selection - uncover the magic</h1><p>Browsing through the logs it turns out that the Action~join-departures-airports was still executed with Spark (SparkSubFeed)!</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  INFO  CustomDataFrameAction - (Action~join-departures-airports) selected subFeedType SparkSubFeed [init-join-departures-airports]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>An Action determines the engine to use in Init-phase by checking the supported types of inputs, outputs and transformations. In our case we have input DataObject stg-departures which is still a JsonFileDataObject, that can not create a Snowpark-DataFrame. As we would like to execute this join as well in Snowflake with Snowpark for performance reasons, lets create a SnowflakeTableDataObject int-departures and use it as input for Action~join-departures-airports.</p><p>Add a DataObject int-departures:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  int-departures {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = SnowflakeTableDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    connectionId = sf-con</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      db = "test"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      name = "int_departures"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Add an Action copy-departures:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  copy-departures {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = CopyAction</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    inputId = stg-departures</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    outputId = int-departures</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    metadata {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      feed = compute</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Fix inputs of Action join-departures-airports:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  inputIds = [int-departures, int-airports]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>... and code of the first SQL transformer:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  code = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    btl-connected-airports = """</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      select int_departures.estdepartureairport, int_departures.estarrivalairport, airports.*</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      from int_departures join int_airports airports on int_departures.estArrivalAirport = airports.ident</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    """</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Compute with Spark and Snowpark again by using <code>--feed-sel compute</code> and browsing the logs, we can see that Action~join-departures-airports was executed with Snowpark:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  (Action~join-departures-airports) selected subFeedType SnowparkSubFeed [init-join-departures-airports]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h1>Summary</h1><p>We have seen that its quite easy to migrate SDLB pipelines to use Snowpark instead of Spark, also only partially for selected Actions. SDLB's support of different DataFrame-API-Engines allows to still benefit of all other features of SDLB, like having full early validation over the whole pipeline by checking the schemas needed by Actions later in the pipeline.</p><p>Migrating Scala code of custom transformations using Spark DataFrame API needs some adaptions of import statements, but the rest stays mostly 1:1 the same. UDFs are also supported and dont need changes, but there might be surprises regarding data types (Snowparks Variant-type is not the same as Sparks nested datatypes) and deployment of needed libraries. We might investigate that in future blog post.</p>]]></content:encoded>
            <category>Snowpark</category>
            <category>Snowflake</category>
        </item>
        <item>
            <title><![CDATA[Using Airbyte connector to inspect github data]]></title>
            <link>https://www.smartdatalake.ch/blog/sdl-airbyte</link>
            <guid>sdl-airbyte</guid>
            <pubDate>Wed, 30 Mar 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[A short example using Airbyte github connector]]></description>
            <content:encoded><![CDATA[<p>This article presents the deployment of an <a href="https://airbyte.com" target="_blank" rel="noopener noreferrer">Airbyte Connector</a> with Smart Data Lake Builder (SDLB).
In particular the <a href="https://docs.airbyte.com/integrations/sources/github" target="_blank" rel="noopener noreferrer">github connector</a> is implemented using the python sources.</p><p>Airbyte is a framework to sync data from a variety of sources (APIs and databases) into data warehouses and data lakes.
In this example an Airbyte connector is utilized to stream data into Smart Data Lake (SDL).
Therefore, the <a href="http://smartdatalake.ch/json-schema-viewer/index.html#viewer-page?v=2-2" target="_blank" rel="noopener noreferrer">Airbyte dataObject</a> is used and will be configured.
The general <a href="https://docs.airbyte.com/understanding-airbyte/airbyte-specification#source" target="_blank" rel="noopener noreferrer">Airbyte connector handling</a> is implemented in SDL, which includes the 4 main steps:</p><ul><li><code>spec</code>: receiving the specification of the connector</li><li><code>check</code>: validating the specified configuration</li><li><code>discover</code>: gather a catalog of available streams and its schemas</li><li><code>read</code>: collect the actual data</li></ul><p>The actual connector is not provided in the SDL repository and needs to be obtained from the <a href="https://github.com/airbytehq/airbyte" target="_blank" rel="noopener noreferrer">Airbyte repository</a>. Besides the <a href="https://docs.airbyte.com/integrations" target="_blank" rel="noopener noreferrer">list of existing connectors</a>, custom connectors could be implemented in Python or Javascript. </p><p>The following description builds on top of the example setup from the <a href="/docs/getting-started/setup">getting-started</a> guide, using <a href="https://docs.podman.io" target="_blank" rel="noopener noreferrer">Podman</a> as container engine within a <a href="https://docs.microsoft.com/en-us/windows/wsl/install" target="_blank" rel="noopener noreferrer">WSL</a> Ubuntu image. </p><p>The <a href="https://docs.airbyte.com/integrations/sources/github" target="_blank" rel="noopener noreferrer">github connector</a> is utilized to gather data about a specific repository.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="prerequisites">Prerequisites<a class="hash-link" href="#prerequisites" title="Direct link to heading">​</a></h2><p>After downloading and installing all necessary packages, the connector is briefly tested:</p><ul><li>Python</li><li><a href="/docs/getting-started/troubleshooting/docker-on-windows">Podman including <code>podman-compose</code></a> or <a href="https://www.docker.com/get-started" target="_blank" rel="noopener noreferrer">Docker</a></li><li><a href="https://github.com/smart-data-lake/getting-started/archive/refs/heads/master.zip" target="_blank" rel="noopener noreferrer">SDL example</a>, download and unpack: <div class="codeBlockContainer_I0IT language-Bash theme-code-block"><div class="codeBlockContent_wNvx Bash"><pre tabindex="0" class="prism-code language-Bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/smart-data-lake/getting-started.git SDL_airbyte</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cd SDL_airbyte</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div></li><li>download the <a href="https://github.com/airbytehq/airbyte" target="_blank" rel="noopener noreferrer">Airbyte repository</a> <div class="codeBlockContainer_I0IT language-Bash theme-code-block"><div class="codeBlockContent_wNvx Bash"><pre tabindex="0" class="prism-code language-Bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/airbytehq/airbyte.git</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div>Alternatively, only the target connector can be downloaded:<div class="codeBlockContainer_I0IT language-Bash theme-code-block"><div class="codeBlockContent_wNvx Bash"><pre tabindex="0" class="prism-code language-Bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">svn checkout https://github.com/airbytehq/airbyte/trunk/airbyte-integrations/connectors/source-github</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div>Here the Airbyte <code>airbyte/airbyte-integrations/connectors/source-github/</code> directory is copied into the <code>SDL_airbyte</code> directory for handy calling the connector.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="optional-inspect-the-connector-specification">[Optional]<!-- --> Inspect the connector specification<a class="hash-link" href="#optional-inspect-the-connector-specification" title="Direct link to heading">​</a></h2><p>The first connector command <code>spec</code> provides the connector specification. This is the basis to create a connector configuration. To run the connector as is, the Python <code>airbyte-cdk</code> package needs to be installed and the connector can be launched:</p><ul><li>Install Python airbyte-cdk: <code>pip install airbyte_cdk</code></li><li>try the connector: <div class="codeBlockContainer_I0IT language-Bash theme-code-block"><div class="codeBlockContent_wNvx Bash"><pre tabindex="0" class="prism-code language-Bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">cd SDL_airbyte</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python source_github/main.py spec | python -m json.tool</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div>This provides a <a target="_blank" href="/assets/files/github_spec_out-e78cbc05ed4f12e8414017c3698a8edb.json">JSON string</a> with the connector specification. The fields listed under <code>properties</code> are relevant for the configuration (compare with the configuration  used later). </li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="configuration">Configuration<a class="hash-link" href="#configuration" title="Direct link to heading">​</a></h2><p>To launch Smart Data Lake Builder (SDLB) with the Airbyte connector the following needs to be modified:</p><ul><li><p>add the Airbyte <strong><em>dataObject</em></strong> with its configuration to the <code>config/application.conf</code>:</p><div class="codeBlockContainer_I0IT language-Python theme-code-block"><div class="codeBlockContent_wNvx Python"><pre tabindex="0" class="prism-code language-Python codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">dataObjects {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ext-commits {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = AirbyteDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    config = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      "credentials": {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        "personal_access_token": "&lt;yourPersonalAccessToken&gt;" ### enter your personal access token here</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      "repository": "smart-data-lake/smart-data-lake",</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      "start_date": "2021-02-01T00:00:00Z",</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      "branch": "documentation develop-spark3 develop-spark2",</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      "page_size_for_large_streams": 100</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    streamName = "commits",</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cmd = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      type = CmdScript</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      name = "airbyte_connector_github"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      linuxCmd = "python3 /mnt/source-github/main.py"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  stg-commits {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   type = DeltaLakeTableDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   path = "~{id}"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   table {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    db = "default"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    name = "stg_commits"</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    primaryKey = [created_at]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note the options set for <code>ext-commits</code> which define the Airbyte connector settings.
While the <code>config</code> varies from connector to connector, the remaining fields are SDL specific.
The <code>streamName</code> selects the stream, exactly one.
If multiple streams should be collected, multiple dataObjects need to be defined.
In <code>linuxCmd</code> the actual connector script is called.
In our case we will mount the connector directory into the SDL container. </p></li><li><p>also add the definition of the data stream <strong><em>action</em></strong> to pipe the coming data stream into a <code>DeltaLakeTableDataObject</code>:</p><div class="codeBlockContainer_I0IT language-Bash theme-code-block"><div class="codeBlockContent_wNvx Bash"><pre tabindex="0" class="prism-code language-Bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  actions {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    download-commits {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      type = CopyAction</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      inputId = ext-commits</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      outputId = stg-commits</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      metadata {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        feed = download</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">...</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div></li><li><p>Since Airbyte will be called as Python script in the sdl container, we need to (re-)build the container with Python support and the Python <code>airbyte-cdk</code> package.
Therefore, in the Dockerfile we add:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">RUN \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">apt update &amp;&amp; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">apt --assume-yes install python3 python3-pip &amp;&amp; \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">pip3 install airbyte-cdk~=0.1.25</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>and rebuild </p><div class="codeBlockContainer_I0IT language-Bash theme-code-block"><div class="codeBlockContent_wNvx Bash"><pre tabindex="0" class="prism-code language-Bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">podman build . -t sdl-spark</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div></li></ul><p>Now we are ready to go. My full <a target="_blank" href="/assets/files/application-b6a5a6494622e5ff40741395d829f558.conf">SDLB config file</a> additionally includes the pull-request stream.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="run-and-inspect-results">Run and inspect results<a class="hash-link" href="#run-and-inspect-results" title="Direct link to heading">​</a></h2><p>Since the data will be streamed into a <code>DeltaLakeTableDataObject</code>, the metastore container is necessary. Further, we aim to inspect the data using the Polynote notebook. Thus, first these containers are launched using (in the SDL example base directory):</p><div class="codeBlockContainer_I0IT language-Bash theme-code-block"><div class="codeBlockContent_wNvx Bash"><pre tabindex="0" class="prism-code language-Bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">podman-compose up</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">podman pod ls</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>With the second command we can verify the pod name and both running containers in it (should be three including the infra container).</p><p>Then, the SDLB can be launched using the additional option to mount the Airbyte connector directory:</p><div class="codeBlockContainer_I0IT language-Bash theme-code-block"><div class="codeBlockContent_wNvx Bash"><pre tabindex="0" class="prism-code language-Bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">podman run --hostname localhost --rm --pod sdl_airbyte -v ${PWD}/source-github/:/mnt/source-github -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel download</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>The output presents the successful run of the workflow:</p><div class="codeBlockContainer_I0IT language-Bash theme-code-block"><div class="codeBlockContent_wNvx Bash"><pre tabindex="0" class="prism-code language-Bash codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">2022-03-16 07:54:03 INFO  ActionDAGRun$ActionEventListener - Action~download-commits[CopyAction]: Exec succeeded [dag-1-80]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2022-03-16 07:54:03 INFO  ActionDAGRun$ - exec SUCCEEDED for dag 1:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                 ┌─────┐</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                 │start│</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                 └───┬─┘</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     │</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                     v</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> ┌───────────────────────────────────────┐</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> │download-commits SUCCEEDED PT11.686865S│</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> └───────────────────────────────────────┘</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     [main]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2022-03-16 07:54:03 INFO  LocalSmartDataLakeBuilder$ - LocalSmartDataLakeBuilder finished successfully: SUCCEEDED=1 [main]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2022-03-16 07:54:03 INFO  SparkUI - Stopped Spark web UI at http://localhost:4040 [shutdown-hook-0]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Launching Polynote <code>localhost:8192</code> in the browser, we can inspect data and develop further workflows. Here an example, where the commits are listed, which were committed in the name of someone else, excluding the web-flow. See <a target="_blank" href="/assets/files/SelectingData-fc913585bab1ef41a6b1f32ee50d5adb.ipynb">Polynote Notebook</a>
<img alt="polynote example" src="/assets/images/polynote_commits-0877fa6cab02c46db63471b8220af7ea.png" width="1018" height="810"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="summary">Summary<a class="hash-link" href="#summary" title="Direct link to heading">​</a></h2><p>The Airbyte connectors provide easy access to a variety of data sources. The connectors can be utilized in SDLB with just a few settings. This also works great for more complex interfaces.</p>]]></content:encoded>
            <category>Airbyte</category>
            <category>Connector</category>
        </item>
    </channel>
</rss>