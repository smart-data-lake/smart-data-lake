<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Smart Data Lake Builder RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Smart Data Lake Builder Atom Feed"><title data-react-helmet="true">Combine Spark and Snowpark to ingest and transform data in one pipeline | Smart Data Lake Builder</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://www.smartdatalake.ch/blog/sdl-snowpark"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><meta data-react-helmet="true" property="og:title" content="Combine Spark and Snowpark to ingest and transform data in one pipeline | Smart Data Lake Builder"><meta data-react-helmet="true" name="description" content="An example to create one unified data pipeline that uses Spark to ingest data into Snowflake, and Snowpark to transform data inside Snowflake."><meta data-react-helmet="true" property="og:description" content="An example to create one unified data pipeline that uses Spark to ingest data into Snowflake, and Snowpark to transform data inside Snowflake."><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="article:published_time" content="2022-04-06T00:00:00.000Z"><meta data-react-helmet="true" property="article:author" content="https://www.linkedin.com/in/zacharias-kull-94705886/"><meta data-react-helmet="true" property="article:tag" content="Snowpark,Snowflake"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://www.smartdatalake.ch/blog/sdl-snowpark"><link data-react-helmet="true" rel="alternate" href="https://www.smartdatalake.ch/blog/sdl-snowpark" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://www.smartdatalake.ch/blog/sdl-snowpark" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.dc3a20c1.css">
<link rel="preload" href="/assets/js/runtime~main.4845f3db.js" as="script">
<link rel="preload" href="/assets/js/main.ce8ffd04.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/sdl_logo.png" alt="Smart Data Lake Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/sdl_logo.png" alt="Smart Data Lake Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Smart Data Lake</b></a><a class="navbar__item navbar__link" href="/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/smart-data-lake/smart-data-lake" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/sdl-databricks">Deployment on Databricks</a></li><li class="sidebarItem_CF0Q"><a aria-current="page" class="sidebarItemLink_miNk sidebarItemLinkActive_RRTD" href="/blog/sdl-snowpark">Combine Spark and Snowpark to ingest and transform data in one pipeline</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/sdl-airbyte">Using Airbyte connector to inspect github data</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_rzP5" itemprop="headline">Combine Spark and Snowpark to ingest and transform data in one pipeline</h1><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2022-04-06T00:00:00.000Z" itemprop="datePublished">April 6, 2022</time> Â· <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.linkedin.com/in/zacharias-kull-94705886/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Zach Kull</span></a></div><small class="avatar__subtitle" itemprop="description">Data Expert</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p>This article shows how to create one unified data pipeline that uses Spark to ingest data into Snowflake, and Snowpark to transform data inside Snowflake.</p><p>Recent developments in Smart Data Lake Builder (SDLB) included refactorings to integrate alternative execution engines to Spark.
In particular <a href="https://docs.snowflake.com/en/developer-guide/snowpark/index.html" target="_blank" rel="noopener noreferrer">Snowpark</a> integration was implemented, a Spark like DataFrame API for implementing transformations in Snowflake.</p><p>Implementing transformations in Snowflake has big performance and cost benefits. And using a DataFrame API is much more powerful than coding in SQL, see also <a href="https://medium.com/towards-data-science/modern-data-stack-which-place-for-spark-8e10365a8772" target="_blank" rel="noopener noreferrer">Modern Data Stack: Which Place for Spark?</a>.</p><p>Snowpark is good for transforming data inside Snowflake, but not all data might be located in Snowflake and suitable for Snowflake.
Here it is interesting to use Spark and its many connectors, in particular to ingest and export data.</p><p>Combining Spark and Snowpark in a smart data pipeline using a DataFrame API would be the ideal solution.
With the integration of Snowpark as engine in SDLB we created just that. </p><p>This blog post will show how to migrate our example data pipeline of the <a href="/docs/getting-started/setup">Getting Started</a> guide Part 1 to use Spark for ingestion and Snowpark for transformation.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="prerequisites">Prerequisites<a class="hash-link" href="#prerequisites" title="Direct link to heading">â€‹</a></h2><ul><li>Create a Snowflake trial account on <a href="https://signup.snowflake.com/" target="_blank" rel="noopener noreferrer">https://signup.snowflake.com/</a> and note the following connection informations:<ul><li>Account URL (copy by navigating to &quot;Organization&quot; and clicking the link symbol on the right of the account name)</li><li>Username</li><li>Password</li></ul></li><li>Create database &quot;testdb&quot; in Snowflake: <code>create database testdb;</code></li><li>Create schema &quot;testdb.test&quot; in Snowflake: <code>create schema testdb.test;</code></li><li>Setup running SDLB docker image with part-1 configuration as described in <a href="/docs/getting-started/setup">Getting Started</a><ul><li>build sdl-spark image</li><li>copy final application.conf of part-1: <code>cp config/application.conf.part-1-solution config/application.conf</code></li><li>run download actions with parameter <code>--feed-sel download</code></li><li>run compute actions with parameter <code>--feed-sel compute</code></li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="goal">Goal<a class="hash-link" href="#goal" title="Direct link to heading">â€‹</a></h2><p>The example of part-1 has the following DataObjects</p><p>Staging Layer</p><ul><li>stg-departures: JsonFileDataObject</li><li>stg-airports: CsvFileDataObject</li></ul><p>Integration Layer</p><ul><li>int-airports: CsvFileDataObject</li></ul><p>Business Transformation Layer</p><ul><li>btl-departures-arrivals-airports: CsvFileDataObject</li><li>btl-distances: CsvFileDataObject</li></ul><p>In this example we will migrate Integration and Business Transformation Layer to Snowflake.
We will use Spark to fill Staging and Integration Layer, and Snowpark for transformation from Integration to Business Transformation Layer.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="prepare-the-snowflake-library">Prepare the Snowflake library<a class="hash-link" href="#prepare-the-snowflake-library" title="Direct link to heading">â€‹</a></h2><p>First we have add SDLBs Snowflake library to the projects pom.xml dependencies section:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;dependencies&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ....</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;dependency&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &lt;groupId&gt;io.smartdatalake&lt;/groupId&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &lt;artifactId&gt;sdl-snowflake_${scala.minor.version}&lt;/artifactId&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      &lt;version&gt;${project.parent.version}&lt;/version&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;/dependency&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;/dependencies&gt;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Then SDLB version needs to be updated to version 2.3.0-SNAPSHOT at least in the parent section:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;parent&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;groupId&gt;io.smartdatalake&lt;/groupId&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;artifactId&gt;sdl-parent&lt;/artifactId&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &lt;version&gt;2.3.0-SNAPSHOT&lt;/version&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &lt;/parent&gt;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="define-snowflake-connection">Define Snowflake connection<a class="hash-link" href="#define-snowflake-connection" title="Direct link to heading">â€‹</a></h2><p>To define the Snowflake connection in config/application.conf, add connections section with connection &quot;sf-con&quot;, and fill in informations according to prerequisits:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  connections {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    sf-con {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      type = SnowflakeTableConnection</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      url = &quot;&lt;accountUrl&gt;&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      warehouse = &quot;COMPUTE_WH&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      database = &quot;testdb&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      role = &quot;ACCOUNTADMIN&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      authMode = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        type = BasicAuthMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        userVariable = &quot;CLEAR#&lt;username&gt;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        passwordVariable = &quot;CLEAR#&lt;pwd&gt;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="migrate-dataobjects">Migrate DataObjects<a class="hash-link" href="#migrate-dataobjects" title="Direct link to heading">â€‹</a></h2><p>Now we can change the DataObject type to SnowflakeTableDataObject and the new Snowflake connection, adding the definition of the table:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  int-airports {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = SnowflakeTableDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    connectionId = sf-con</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      db = &quot;test&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      name = &quot;int_airports&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  btl-departures-arrivals-airports {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = SnowflakeTableDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    connectionId = sf-con</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      db = &quot;test&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      name = &quot;btl_departures_arrivals_airports&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  btl-distances {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = SnowflakeTableDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    connectionId = sf-con</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      db = &quot;test&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      name = &quot;btl_distances&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that the attribute <code>db</code> of the SnowflakeTableDataObject should be filled with the schema of the Snowflake table and that this is <em>not</em> the same as the attribute <code>database</code> of SnowflakeTableConnection. </p><h2 class="anchor anchorWithStickyNavbar_mojV" id="migrating-actions">Migrating Actions<a class="hash-link" href="#migrating-actions" title="Direct link to heading">â€‹</a></h2><p>The new SDLB version introduced some naming changes:</p><ul><li><p>The CustomSparkAction can now also process Snowpark-DataFrames and is therefore renamed to CustomDataFrameAction.</p></li><li><p>The ScalaClassDfTransformer was specific for Spark. In the new SDLB version there is a specific scala-class DataFrame transformer for Spark and Snowpark, e.g. ScalaClassSparkDfTransformer and ScalaClassSnowparkDfTransformer. And there is even a ScalaClassGenericDfTransformer to implement transformations using a unified API. In our case we will migrate the transformation to use Snowpark and set the type to ScalaClassSnowparkDfTransformer.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">join-departures-airports {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  type = CustomSparkAction -&gt; CustomDataFrameAction</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">compute-distances {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ...</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  transformers = [{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = ScalaClassDfTransformer -&gt; ScalaClassSnowparkDfTransformer</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div></li></ul><p>There is no need to change the SQL transformtions of join-departures-airport, as the SQL should run on Snowpark aswell.</p><p>On the other hand the ComputeDistanceTransformer was implemented with the Spark DataFrame API. We need to migrate it to Snowpark DataFrame API to run this Action with Snowpark. Luckily the API&#x27;s are very similar. Often it&#x27;s sufficient to change the import statement, the class we&#x27;re extending and the session parameters type:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  import com.snowflake.snowpark.functions._</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import com.snowflake.snowpark.{DataFrame, Session}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  import io.smartdatalake.workflow.action.snowflake.customlogic.CustomSnowparkDfTransformer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  class ComputeDistanceTransformer extends CustomSnowparkDfTransformer {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def transform(session: Session, options: Map[String, String], df: DataFrame, dataObjectId: String) : DataFrame = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      ...</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>If you have UDFs in your code, it gets trickier. The UDF Code gets serialized to Snowflake, details see <a href="https://docs.snowflake.com/en/developer-guide/snowpark/scala/creating-udfs.html" target="_blank" rel="noopener noreferrer">Snowpark UDFs</a>. Special care must be taken to minimize the scope the UDF is defined in. Thats why we move the function into the companion object.</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  object ComputeDistanceTransformer {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def calculateDistanceInKilometer(depLat: Double, depLng: Double, arrLat: Double, arrLng: Double): Double = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val AVERAGE_RADIUS_OF_EARTH_KM = 6371</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val latDistance = Math.toRadians(depLat - arrLat)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val lngDistance = Math.toRadians(depLng - arrLng)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val a = Math.sin(latDistance / 2) * Math.sin(latDistance / 2) + Math.cos(Math.toRadians(depLat)) * Math.cos(Math.toRadians(arrLat)) * Math.sin(lngDistance / 2) * Math.sin(lngDistance / 2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      AVERAGE_RADIUS_OF_EARTH_KM * c</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def getCalculateDistanceInKilometerUdf(session: Session) = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      // using only udf(...) function results in &quot;SnowparkClientException: Error Code: 0207, Error message: No default Session found. Use &lt;session&gt;.udf.registerTemporary() to explicitly refer to a session.&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      session.udf.registerTemporary(ComputeDistanceTransformer.calculateDistanceInKilometer _)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Note that we need to pass the Session to a function for registering the UDF. There is an Error 0207 if we use &quot;udf&quot; function (at least in snowpark version 1.2.0).
Finally we need to adapt the call of the UDF as follows:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">df.withColumn(&quot;distance&quot;, ComputeDistanceTransformer.getCalculateDistanceInKilometerUdf(session)(col(&quot;dep_latitude_deg&quot;),col(&quot;dep_longitude_deg&quot;),col(&quot;arr_latitude_deg&quot;), col(&quot;arr_longitude_deg&quot;)))</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="compile-and-run">Compile and run<a class="hash-link" href="#compile-and-run" title="Direct link to heading">â€‹</a></h2><p>Time to see if it works.
Lets build an update SDLB docker image with the updated SDLB version:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  podman build -t sdl-spark .</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Then compile the code with the UDF:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  mkdir .mvnrepo</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  podman run -v ${PWD}:/mnt/project -v ${PWD}/.mvnrepo:/mnt/.mvnrepo maven:3.6.0-jdk-11-slim -- mvn -f /mnt/project/pom.xml &quot;-Dmaven.repo.local=/mnt/.mvnrepo&quot; package</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Download initial data with <code>--feed-sel download</code>:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  podman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel download</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Compute with <code>--feed-sel compute</code>:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  podman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel compute</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>If the SDLB run was SUCCESSFUL, you should now see TEST.BTL_DISTANCES table in Snowpark.
To check that Spark was used for Action select-airport-cols and Snowpark for Action compute-distances, look for the following logs, e.g. SnowparkSubFeed for Action~compute-distances: </p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  INFO  CopyAction - (Action~compute-distances) selected subFeedType SnowparkSubFeed [init-compute-distances]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h1>Engine selection - uncover the magic</h1><p>Browsing through the logs it turns out that the Action~join-departures-airports was still executed with Spark (SparkSubFeed)!</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  INFO  CustomDataFrameAction - (Action~join-departures-airports) selected subFeedType SparkSubFeed [init-join-departures-airports]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>An Action determines the engine to use in Init-phase by checking the supported types of inputs, outputs and transformations. In our case we have input DataObject stg-departures which is still a JsonFileDataObject, that can not create a Snowpark-DataFrame. As we would like to execute this join as well in Snowflake with Snowpark for performance reasons, lets create a SnowflakeTableDataObject int-departures and use it as input for Action~join-departures-airports.</p><p>Add a DataObject int-departures:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  int-departures {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = SnowflakeTableDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    connectionId = sf-con</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      db = &quot;test&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      name = &quot;int_departures&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Add an Action copy-departures:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  copy-departures {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = CopyAction</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    inputId = stg-departures</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    outputId = int-departures</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    metadata {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      feed = compute</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Fix inputs of Action join-departures-airports:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  inputIds = [int-departures, int-airports]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>... and code of the first SQL transformer:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  code = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    btl-connected-airports = &quot;&quot;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      select int_departures.estdepartureairport, int_departures.estarrivalairport, airports.*</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      from int_departures join int_airports airports on int_departures.estArrivalAirport = airports.ident</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;&quot;&quot;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Compute with Spark and Snowpark again by using <code>--feed-sel compute</code> and browsing the logs, we can see that Action~join-departures-airports was executed with Snowpark:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">  (Action~join-departures-airports) selected subFeedType SnowparkSubFeed [init-join-departures-airports]</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h1>Summary</h1><p>We have seen that its quite easy to migrate SDLB pipelines to use Snowpark instead of Spark, also only partially for selected Actions. SDLB&#x27;s support of different DataFrame-API-Engines allows to still benefit of all other features of SDLB, like having full early validation over the whole pipeline by checking the schemas needed by Actions later in the pipeline.</p><p>Migrating Scala code of custom transformations using Spark DataFrame API needs some adaptions of import statements, but the rest stays mostly 1:1 the same. UDFs are also supported and dont need changes, but there might be surprises regarding data types (Snowparks Variant-type is not the same as Sparks nested datatypes) and deployment of needed libraries. We might investigate that in future blog post.</p></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_h6_j"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/snowpark">Snowpark</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/snowflake">Snowflake</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/smart-data-lake/smart-data-lake/tree/documentation/blog/2022-04-06-SDL-snowpark/2022-04-06-SDL-snowpark.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/sdl-databricks"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Deployment on Databricks</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/sdl-airbyte"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Using Airbyte connector to inspect github data</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_cNA8 thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#prerequisites" class="table-of-contents__link toc-highlight">Prerequisites</a></li><li><a href="#goal" class="table-of-contents__link toc-highlight">Goal</a></li><li><a href="#prepare-the-snowflake-library" class="table-of-contents__link toc-highlight">Prepare the Snowflake library</a></li><li><a href="#define-snowflake-connection" class="table-of-contents__link toc-highlight">Define Snowflake connection</a></li><li><a href="#migrate-dataobjects" class="table-of-contents__link toc-highlight">Migrate DataObjects</a></li><li><a href="#migrating-actions" class="table-of-contents__link toc-highlight">Migrating Actions</a></li><li><a href="#compile-and-run" class="table-of-contents__link toc-highlight">Compile and run</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/">Getting started</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/smart-data-lake/smart-data-lake" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/smart-data-lake/smart-data-lake/issues" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub Issues<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://www.elca.ch" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>ELCA<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 Smart Data Lake, Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.4845f3db.js"></script>
<script src="/assets/js/main.ce8ffd04.js"></script>
</body>
</html>