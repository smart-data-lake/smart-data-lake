global {
  statusInfo {
    stopOnEnd = false
    port = 4440
  }
  uiBackend {
    baseUrl = "https://nn92uqy13e.execute-api.eu-central-1.amazonaws.com/DEV/api/v1"
    tenant = PrivateTenant
    repo = int-testing
    env = std
    authMode {
      type = AWSUserPwdAuthMode
      region = eu-central-1
      userPool = sdlb-ui
      useIdToken = true
      clientId = "###FILE#ui-auth;clientId###"
      user = "###FILE#ui-auth;user###"
      password = "###FILE#ui-auth;pwd###"
    }
  }
}

dataObjects {

  dataObjectCsv1 {
    type = CsvFileDataObject
    path = "target/~{id}"
    csv-options {
      delimiter = ","
      header = true
      mode = failfast
    }
    schema = """a string, b string, c string"""
  }

  dataObjectCsv2 {
    type = CsvFileDataObject
    path = "target/~{id}"
    csv-options {
      delimiter = ","
      header = true
      mode = failfast
    }
    schema = """a string, b string, c string"""
  }

  dataObjectCsv3 {
    type = CsvFileDataObject
    path = "target/~{id}"
    csv-options {
      delimiter = ","
      header = true
      mode = failfast
    }
    schema = """a string, b string, c string"""
  }

  dataObjectCsv4 {
    type = CsvFileDataObject
    path = "target/~{id}"
    csv-options {
      delimiter = ","
      header = true
      mode = failfast
    }
    schema = """a string, b string, c string"""
  }

  dataObjectCsv5 {
    type = CsvFileDataObject
    path = "target/~{id}"
    csv-options {
      delimiter = ","
      header = true
      mode = failfast
    }
    schema = """a string, b string, c string"""
  }

  dataObjectParquet6 {
    type = ParquetFileDataObject
    path = "target/~{id}"
    schema = """a string, b array<struct<b1: string, b2: long>>, c struct<c1: string, c2: long>"""
  }

  dataObjectParquet7 {
    type = ParquetFileDataObject
    path = "target/~{id}"
    schema = """a string, b string, c string"""
  }
  dataObjectParquet8 {
    type = ParquetFileDataObject
    path = "target/~{id}"
    schema = """a string, b string, c string"""
  }

  dataObjectParquet9 {
    type = ParquetFileDataObject
    path = "target/~{id}"
    schema = """a string, b string, c string"""
  }

  dataObjectParquet10 {
    type = ParquetFileDataObject
    path = "target/~{id}"
    schema = """a string, b string, c string"""
  }
  dataObjectParquet11 {
    type = ParquetFileDataObject
    path = "target/~{id}"
    schema = """a string, b string, c string"""
  }
  dataObjectParquet12 {
    type = ParquetFileDataObject
    path = "target/~{id}"
    schema = """a string, b string, c string"""
  }
  dataObjectParquet13 {
    type = ParquetFileDataObject
    path = "target/~{id}"
    schema = """a string, b string, c string"""
  }

  dataObjectHive14 {
    type = HiveTableDataObject
    path = "target/~{id}"
    table = {
      db = default
      name = hive14
    }
  }
}

actions {
  actionId1 {
    type = CustomDataFrameAction
    inputIds = [dataObjectCsv1, dataObjectCsv2]
    outputIds = [dataObjectParquet6]
    transformers = [{
      type = ScalaCodeSparkDfsTransformer
      code = """
        import org.apache.spark.sql.{DataFrame, SparkSession}
        import org.apache.spark.sql.functions._

        (session: SparkSession, options: Map[String,String], dfs: Map[String,DataFrame]) => {
          import session.implicits._
          val df = dfs("dataObjectCsv1")
          Map("dataObjectParquet6" -> df)
        }
      """
    }]
    metadata.feed = actionId
  }

  actionId2 {
    type = CopyAction
    inputId = dataObjectParquet6
    outputId = dataObjectParquet7
    metadata.feed = actionId
  }

  actionId3 {
    type = CopyAction
    inputId = dataObjectCsv3
    outputId = dataObjectParquet7
    metadata.feed = actionId
  }

  actionId4 {
    type = CustomDataFrameAction
    inputIds = [dataObjectParquet7, dataObjectParquet8]
    outputIds = [dataObjectParquet9]
    transformers = [{
      type = ScalaCodeSparkDfsTransformer
      code = """
        import org.apache.spark.sql.{DataFrame, SparkSession}
        import org.apache.spark.sql.functions._

        (session: SparkSession, options: Map[String,String], dfs: Map[String,DataFrame]) => {
          import session.implicits._
          val df = dfs("dataObjectParquet7")
          Map("dataObjectParquet9" -> df)
        }
      """
    }]
    metadata.feed = actionId
  }

  actionId5 {
    type = CustomDataFrameAction
    inputIds = [dataObjectCsv4]
    outputIds = [dataObjectParquet9, dataObjectParquet10]
    transformers = [{
      type = ScalaCodeSparkDfsTransformer
      code = """
        import org.apache.spark.sql.{DataFrame, SparkSession}
        import org.apache.spark.sql.functions._

        (session: SparkSession, options: Map[String,String], dfs: Map[String,DataFrame]) => {
          import session.implicits._
          val df = dfs("dataObjectCsv4")
          Map("dataObjectParquet9" -> df, "dataObjectParquet10" -> df)
        }
      """
    }]
    metadata.feed = actionId
  }

  actionId6 {
    type = CustomDataFrameAction
    inputIds = [dataObjectParquet9, dataObjectParquet10]
    outputIds = [dataObjectCsv5]
    transformers = [{
      type = ScalaClassSparkDfsTransformer
      className = io.smartdatalake.meta.configexporter.DynamicTestTransformer
      options = {
        long = 9
        isExec = true
      }
    }]
    metadata.feed = actionId
  }

  actionId7 {
    type = CopyAction
    inputId = dataObjectParquet11
    outputId = dataObjectParquet12
    metadata.feed = actionId
  }

  actionId8 {
    type = CopyAction
    inputId = dataObjectParquet12
    outputId = dataObjectParquet13
    transformers = [{
      type = ScalaClassGenericDfTransformer
      className = io.smartdatalake.meta.configexporter.ScalaDocExportTestTransformer
    }]
    metadata.feed = actionId
  }
}
