{
  "type" : "object",
  "additionalProperties" : true,
  "required" : [ "dataObjects", "actions" ],
  "properties" : {
    "global" : {
      "type" : "object",
      "description" : "Global configuration options\nNote that global configuration is responsible to hold SparkSession, so that its created once and only once per SDLB job.\nThis is especially important if JVM is shared between different SDL jobs (e.g. Databricks cluster), because sharing SparkSession in object Environment survives the current SDLB job.",
      "additionalProperties" : false,
      "title" : "GlobalConfig",
      "properties" : {
        "kryoClasses" : {
          "type" : "array",
          "description" : "classes to register for spark kryo serialization",
          "items" : {
            "type" : "string"
          }
        },
        "sparkOptions" : {
          "type" : "object",
          "description" : "spark options",
          "additionalProperties" : {
            "type" : "string"
          }
        },
        "statusInfo" : {
          "type" : "object",
          "description" : "Configuration for the Server that provides live status info of the current DAG Execution",
          "additionalProperties" : false,
          "title" : "StatusInfoConfig",
          "properties" : {
            "port" : {
              "type" : "integer",
              "description" : ": port with which the first connection attempt is made"
            },
            "maxPortRetries" : {
              "type" : "integer",
              "description" : ": If port is already in use, we will increment port by one and try with that new port.\nmaxPortRetries describes how many times this should be attempted. If set to 0 it will not be attempted.\nValues below 0 are not allowed."
            },
            "stopOnEnd" : {
              "type" : "string",
              "description" : ": Set to false if the Server should remain online even after SDL has finished its execution.\nIn that case, the Application needs to be stopped manually. Useful for debugging."
            }
          }
        },
        "enableHive" : {
          "type" : "string",
          "description" : "enable hive for spark session"
        },
        "memoryLogTimer" : {
          "type" : "object",
          "description" : "Configuration for periodic memory usage logging",
          "additionalProperties" : false,
          "required" : [ "intervalSec" ],
          "title" : "MemoryLogTimerConfig",
          "properties" : {
            "intervalSec" : {
              "type" : "integer",
              "description" : "interval in seconds between memory usage logs"
            },
            "logLinuxMem" : {
              "type" : "string",
              "description" : "enable logging linux memory"
            },
            "logLinuxCGroupMem" : {
              "type" : "string",
              "description" : "enable logging details about linux cgroup memory"
            },
            "logBuffers" : {
              "type" : "string",
              "description" : "enable logging details about different jvm buffers"
            }
          }
        },
        "shutdownHookLogger" : {
          "type" : "string",
          "description" : "enable shutdown hook logger to trace shutdown cause"
        },
        "stateListeners" : {
          "type" : "array",
          "description" : "Define state listeners to be registered for receiving events of the execution of SmartDataLake job",
          "items" : {
            "type" : "object",
            "description" : "Configuration to notify interested parties about action results & metric",
            "additionalProperties" : false,
            "required" : [ "className" ],
            "title" : "StateListenerConfig",
            "properties" : {
              "className" : {
                "type" : "string",
                "description" : "fully qualified class name of class implementing StateListener interface. The class needs a constructor with one parameter`options: Map[String,String]` ."
              },
              "options" : {
                "type" : "object",
                "description" : "Options are passed to StateListener constructor.",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          }
        },
        "sparkUDFs" : {
          "type" : "object",
          "description" : "Define UDFs to be registered in spark session. The registered UDFs are available in Spark SQL transformations\nand expression evaluation, e.g. configuration of ExecutionModes.",
          "additionalProperties" : {
            "type" : "object",
            "description" : "Configuration to register a UserDefinedFunction in the spark session of SmartDataLake.",
            "additionalProperties" : false,
            "required" : [ "className" ],
            "title" : "SparkUDFCreatorConfig",
            "properties" : {
              "className" : {
                "type" : "string",
                "description" : "fully qualified class name of class implementing SparkUDFCreator interface. The class needs a constructor without parameters."
              },
              "options" : {
                "type" : "object",
                "description" : "Options are passed to SparkUDFCreator apply method.",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          }
        },
        "pythonUDFs" : {
          "type" : "object",
          "description" : "Define UDFs in python to be registered in spark session. The registered UDFs are available in Spark SQL transformations\nbut not for expression evaluation.",
          "additionalProperties" : {
            "type" : "object",
            "description" : "Configuration to register a Python UDF in the spark session of SmartDataLake.\nDefine a python function with type hints i python code and register it in global configuration.\nThe name of the function must match the name you use to declare it in GlobalConf.\nThe Python function can then be used in Spark SQL expressions.",
            "additionalProperties" : false,
            "title" : "PythonUDFCreatorConfig",
            "properties" : {
              "pythonFile" : {
                "type" : "string",
                "description" : "Optional pythonFile to use for python UDF."
              },
              "pythonCode" : {
                "type" : "string",
                "description" : "Optional pythonCode to use for python UDF."
              },
              "options" : {
                "type" : "object",
                "description" : "Options are available in your python code as variable options.",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          }
        },
        "secretProviders" : {
          "type" : "object",
          "description" : "Define SecretProvider\\'s to be registered.",
          "additionalProperties" : {
            "type" : "object",
            "description" : "Configuration to register a SecretProvider.",
            "additionalProperties" : false,
            "required" : [ "className" ],
            "title" : "SecretProviderConfig",
            "properties" : {
              "className" : {
                "type" : "string",
                "description" : "fully qualified class name of class implementing SecretProvider interface. The class needs a constructor with parameter \\\"options: Map[String,String]\\\"."
              },
              "options" : {
                "type" : "object",
                "description" : "Options are passed to SecretProvider apply method.",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          }
        },
        "allowOverwriteAllPartitionsWithoutPartitionValues" : {
          "type" : "array",
          "description" : "Configure a list of exceptions for partitioned DataObject id\\'s,\nwhich are allowed to overwrite the all partitions of a table if no partition values are set.\nThis is used to override/avoid a protective error when using SDLSaveMode.OverwriteOptimized|OverwritePreserveDirectories.\nDefine it as a list of DataObject id\\'s.",
          "items" : {
            "type" : "string"
          }
        },
        "synchronousStreamingTriggerIntervalSec" : {
          "type" : "integer",
          "description" : "Trigger interval for synchronous actions in streaming mode in seconds (default = 60 seconds)\nThe synchronous actions of the DAG will be executed with this interval if possile.\nNote that for asynchronous actions there are separate settings, e.g. SparkStreamingMode.triggerInterval."
        }
      }
    },
    "connections" : {
      "type" : "object",
      "additionalProperties" : {
        "description" : "Map Connection name : definition",
        "oneOf" : [ {
          "$ref" : "#/definitions/Connection/DeltaLakeTableConnection"
        }, {
          "$ref" : "#/definitions/Connection/HadoopFileConnection"
        }, {
          "$ref" : "#/definitions/Connection/HiveTableConnection"
        }, {
          "$ref" : "#/definitions/Connection/JdbcTableConnection"
        }, {
          "$ref" : "#/definitions/Connection/KafkaConnection"
        }, {
          "$ref" : "#/definitions/Connection/SftpFileRefConnection"
        }, {
          "$ref" : "#/definitions/Connection/SnowflakeConnection"
        }, {
          "$ref" : "#/definitions/Connection/SplunkConnection"
        } ]
      }
    },
    "dataObjects" : {
      "type" : "object",
      "additionalProperties" : {
        "description" : "Map of DataObject name and definition",
        "oneOf" : [ {
          "$ref" : "#/definitions/DataObject/AccessTableDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/ActionsExporterDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/AirbyteDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/AvroFileDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/CsvFileDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/CustomDfDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/CustomFileDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/DataObjectsExporterDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/DeltaLakeTableDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/ExcelFileDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/HiveTableDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/JdbcTableDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/JmsDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/JsonFileDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/KafkaTopicDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/PKViolatorsDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/ParquetFileDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/RawFileDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/RelaxedCsvFileDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/SFtpFileRefDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/SnowflakeTableDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/SplunkDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/TickTockHiveTableDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/WebserviceFileDataObject"
        }, {
          "$ref" : "#/definitions/DataObject/XmlFileDataObject"
        } ]
      }
    },
    "actions" : {
      "type" : "object",
      "additionalProperties" : {
        "description" : "Map of Action name and definition",
        "oneOf" : [ {
          "$ref" : "#/definitions/Action/CopyAction"
        }, {
          "$ref" : "#/definitions/Action/CustomFileAction"
        }, {
          "$ref" : "#/definitions/Action/CustomScriptAction"
        }, {
          "$ref" : "#/definitions/Action/CustomSnowparkAction"
        }, {
          "$ref" : "#/definitions/Action/CustomSparkAction"
        }, {
          "$ref" : "#/definitions/Action/DeduplicateAction"
        }, {
          "$ref" : "#/definitions/Action/FileTransferAction"
        }, {
          "$ref" : "#/definitions/Action/HistorizeAction"
        } ]
      }
    }
  },
  "definitions" : {
    "ExecutionMode" : {
      "CustomPartitionMode" : {
        "type" : "object",
        "description" : "Execution mode to create custom partition execution mode logic.\nDefine a function which receives main input&output DataObject and returns partition values to process as Seq[Map[String,String]\\\\]",
        "additionalProperties" : false,
        "required" : [ "className" ],
        "title" : "CustomPartitionMode",
        "properties" : {
          "type" : {
            "const" : "CustomPartitionMode"
          },
          "className" : {
            "type" : "string",
            "description" : "class name implementing trait[[CustomPartitionModeLogic]]"
          },
          "alternativeOutputId" : {
            "type" : "string",
            "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
          },
          "options" : {
            "type" : "object",
            "description" : "Options specified in the configuration for this execution mode",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      },
      "DataObjectStateIncrementalMode" : {
        "type" : "object",
        "description" : "An execution mode for incremental processing by remembering DataObjects state from last increment.",
        "additionalProperties" : false,
        "title" : "DataObjectStateIncrementalMode",
        "properties" : {
          "type" : {
            "const" : "DataObjectStateIncrementalMode"
          }
        }
      },
      "FailIfNoPartitionValuesMode" : {
        "type" : "object",
        "description" : "An execution mode which just validates that partition values are given.\nNote: For start nodes of the DAG partition values can be defined by command line, for subsequent nodes partition values are passed on from previous nodes.",
        "additionalProperties" : false,
        "title" : "FailIfNoPartitionValuesMode",
        "properties" : {
          "type" : {
            "const" : "FailIfNoPartitionValuesMode"
          }
        }
      },
      "FileIncrementalMoveMode" : {
        "type" : "object",
        "description" : "Execution mode to incrementally process file-based DataObjects.\nIt takes all existing files in the input DataObject and removes (deletes) them after processing.\nInput partition values are applied when searching for files and also used as output partition values.",
        "additionalProperties" : false,
        "title" : "FileIncrementalMoveMode",
        "properties" : {
          "type" : {
            "const" : "FileIncrementalMoveMode"
          }
        }
      },
      "PartitionDiffMode" : {
        "type" : "object",
        "description" : "Partition difference execution mode lists partitions on mainInput & mainOutput DataObject and starts loading all missing partitions.\nPartition columns to be used for comparision need to be a common \\'init\\' of input and output partition columns.\nThis mode needs mainInput/Output DataObjects which CanHandlePartitions to list partitions.\nPartition values are passed to following actions for partition columns which they have in common.",
        "additionalProperties" : false,
        "title" : "PartitionDiffMode",
        "properties" : {
          "type" : {
            "const" : "PartitionDiffMode"
          },
          "partitionColNb" : {
            "type" : "integer",
            "description" : "optional number of partition columns to use as a common \\'init\\'."
          },
          "alternativeOutputId" : {
            "type" : "string",
            "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
          },
          "nbOfPartitionValuesPerRun" : {
            "type" : "integer",
            "description" : "optional restriction of the number of partition values per run."
          },
          "applyCondition" : {
            "type" : "string",
            "description" : "Condition to decide if execution mode should be applied or not. Define a spark sql expression working with attributes of[[DefaultExecutionModeExpressionData]] returning a boolean.\nDefault is to apply the execution mode if given partition values (partition values from command line or passed from previous action) are not empty."
          },
          "failCondition" : {
            "type" : "string",
            "description" : "If this execution mode should be run as asynchronous streaming process"
          },
          "failConditions" : {
            "type" : "array",
            "description" : "List of conditions to fail application of execution mode if true. Define as spark sql expressions working with attributes of[[PartitionDiffModeExpressionData]] returning a boolean.\nDefault is that the application of the PartitionDiffMode does not fail the action. If there is no data to process, the following actions are skipped.\nMultiple conditions are evaluated individually and every condition may fail the execution mode (or-logic)",
            "items" : {
              "type" : "object",
              "description" : "Definition of a Spark SQL condition with description.\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
              "additionalProperties" : false,
              "required" : [ "expression" ],
              "title" : "Condition",
              "properties" : {
                "expression" : {
                  "type" : "string",
                  "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
                },
                "description" : {
                  "type" : "string",
                  "description" : "A textual description of the condition to be shown in error messages."
                }
              }
            }
          },
          "selectExpression" : {
            "type" : "string",
            "description" : "optional expression to define or refine the list of selected output partitions. Define a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\nDefault is to return the originally selected output partitions found in attribute selectedOutputPartitionValues."
          },
          "applyPartitionValuesTransform" : {
            "type" : "string",
            "description" : "If true applies the partition values transform of custom transformations on input partition values before comparison with output partition values.\nIf enabled input and output partition columns can be different. Default is to disable the transformation of partition values."
          },
          "selectAdditionalInputExpression" : {
            "type" : "string",
            "description" : "optional expression to refine the list of selected input partitions. Note that primarily output partitions are selected by PartitionDiffMode.\nThe selected output partitions are then transformed back to the input partitions needed to create the selected output partitions. This is one-to-one except if applyPartitionValuesTransform=true.\nAnd sometimes there is a need for additional input data to create the output partitions, e.g. if you aggregate a window of 7 days for every day.\nYou can customize selected input partitions by defining a spark sql expression working with the attributes of[[PartitionDiffModeExpressionData]] returning a list<map<string,string>>.\nDefault is to return the originally selected input partitions found in attribute selectedInputPartitionValues."
          }
        }
      },
      "ProcessAllMode" : {
        "type" : "object",
        "description" : "An execution mode which forces processing all data from it\\'s inputs.",
        "additionalProperties" : false,
        "title" : "ProcessAllMode",
        "properties" : {
          "type" : {
            "const" : "ProcessAllMode"
          }
        }
      },
      "SparkIncrementalMode" : {
        "type" : "object",
        "description" : "Compares max entry in \\\"compare column\\\" between mainOutput and mainInput and incrementally loads the delta.\nThis mode works only with SparkSubFeeds. The filter is not propagated to following actions.",
        "additionalProperties" : false,
        "required" : [ "compareCol" ],
        "title" : "SparkIncrementalMode",
        "properties" : {
          "type" : {
            "const" : "SparkIncrementalMode"
          },
          "compareCol" : {
            "type" : "string",
            "description" : "a comparable column name existing in mainInput and mainOutput used to identify the delta. Column content should be bigger for newer records."
          },
          "alternativeOutputId" : {
            "type" : "string",
            "description" : "optional alternative outputId of DataObject later in the DAG. This replaces the mainOutputId.\nIt can be used to ensure processing all partitions over multiple actions in case of errors."
          },
          "applyCondition" : {
            "type" : "object",
            "description" : "Definition of a Spark SQL condition with description.\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
            "additionalProperties" : false,
            "required" : [ "expression" ],
            "title" : "Condition",
            "properties" : {
              "expression" : {
                "type" : "string",
                "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
              },
              "description" : {
                "type" : "string",
                "description" : "A textual description of the condition to be shown in error messages."
              }
            }
          }
        }
      },
      "SparkStreamingMode" : {
        "type" : "object",
        "description" : "Spark streaming execution mode uses Spark Structured Streaming to incrementally execute data loads and keep track of processed data.\nThis mode needs a DataObject implementing CanCreateStreamingDataFrame and works only with SparkSubFeeds.\nThis mode can be executed synchronously in the DAG by using triggerType=Once, or asynchronously as Streaming Query with triggerType = ProcessingTime or Continuous.",
        "additionalProperties" : false,
        "required" : [ "checkpointLocation" ],
        "title" : "SparkStreamingMode",
        "properties" : {
          "type" : {
            "const" : "SparkStreamingMode"
          },
          "checkpointLocation" : {
            "type" : "string",
            "description" : "location for checkpoints of streaming query to keep state"
          },
          "triggerType" : {
            "type" : "string",
            "description" : "define execution interval of Spark streaming query. Possible values are Once (default), ProcessingTime & Continuous. See[[Trigger]] for details.\n                      Note that this is only applied if SDL is executed in streaming mode. If SDL is executed in normal mode, TriggerType=Once is used always.\nIf triggerType=Once, the action is repeated with Trigger.Once in SDL streaming mode."
          },
          "triggerTime" : {
            "type" : "string",
            "description" : "Time as String in triggerType = ProcessingTime or Continuous. See[[Trigger]] for details."
          },
          "inputOptions" : {
            "type" : "object",
            "description" : "additional option to apply when reading streaming source. This overwrites options set by the DataObjects.",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "outputOptions" : {
            "type" : "object",
            "description" : "additional option to apply when writing to streaming sink. This overwrites options set by the DataObjects.",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "outputMode" : {
            "type" : "string",
            "enum" : [ "Append", "Complete", "Update" ],
            "description" : "If this execution mode should be run as asynchronous streaming process"
          }
        }
      }
    },
    "ParsableDfsTransformer" : {
      "DfTransformerWrapperDfsTransformer" : {
        "type" : "object",
        "description" : "A Transformer to use single DataFrame Transformers as multiple DataFrame Transformers.\nThis works by selecting the SubFeeds (DataFrames) the single DataFrame Transformer should be applied to.\nAll other SubFeeds will be passed through without transformation.",
        "additionalProperties" : false,
        "required" : [ "transformer", "subFeedsToApply" ],
        "title" : "DfTransformerWrapperDfsTransformer",
        "properties" : {
          "type" : {
            "const" : "DfTransformerWrapperDfsTransformer"
          },
          "transformer" : {
            "description" : "Configuration for a DfTransformer to be applied",
            "oneOf" : [ {
              "type" : "object",
              "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Java/Scala Class.\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\nDataFrame. The Java/Scala class has to implement interface[[CustomDfTransformer]] .",
              "additionalProperties" : false,
              "required" : [ "className", "type" ],
              "title" : "ScalaClassDfTransformer",
              "properties" : {
                "type" : {
                  "const" : "ScalaClassDfTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                },
                "className" : {
                  "type" : "string",
                  "description" : "class name implementing trait[[CustomDfTransformer]]"
                },
                "options" : {
                  "type" : "object",
                  "description" : "Options to pass to the transformation",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                },
                "runtimeOptions" : {
                  "type" : "object",
                  "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                }
              }
            }, {
              "type" : "object",
              "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\nDataFrame. The scala code has to implement a function of type[[fnTransformType]] .",
              "additionalProperties" : false,
              "required" : [ "type" ],
              "title" : "ScalaCodeDfTransformer",
              "properties" : {
                "type" : {
                  "const" : "ScalaCodeDfTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                },
                "code" : {
                  "type" : "string",
                  "description" : "Scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
                },
                "file" : {
                  "type" : "string",
                  "description" : "File where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
                },
                "options" : {
                  "type" : "object",
                  "description" : "Options to pass to the transformation",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                },
                "runtimeOptions" : {
                  "type" : "object",
                  "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                }
              }
            }, {
              "type" : "object",
              "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as SQL code.\nThe input data is available as temporary view in SQL. As name for the temporary view the input DataObjectId is used\n(special characters are replaces by underscores). A special token \\'%{inputViewName}\\' will be replaced with the name of\nthe temporary view at runtime.",
              "additionalProperties" : false,
              "required" : [ "code", "type" ],
              "title" : "SQLDfTransformer",
              "properties" : {
                "type" : {
                  "const" : "SQLDfTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                },
                "code" : {
                  "type" : "string",
                  "description" : "SQL code for transformation.\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\nExample: \\\"select * from test where run = %{runId}\\\"\nA special token %{inputViewName} can be used to insert the temporary view name."
                },
                "options" : {
                  "type" : "object",
                  "description" : "Options to pass to the transformation",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                },
                "runtimeOptions" : {
                  "type" : "object",
                  "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                }
              }
            }, {
              "type" : "object",
              "description" : "Add additional columns to the DataFrame by extracting information from the context.",
              "additionalProperties" : false,
              "required" : [ "additionalColumns", "type" ],
              "title" : "AdditionalColumnsTransformer",
              "properties" : {
                "type" : {
                  "const" : "AdditionalColumnsTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                },
                "additionalColumns" : {
                  "type" : "object",
                  "description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                }
              }
            }, {
              "type" : "object",
              "description" : "Apply a column blacklist to a DataFrame.",
              "additionalProperties" : false,
              "required" : [ "columnBlacklist", "type" ],
              "title" : "BlacklistTransformer",
              "properties" : {
                "type" : {
                  "const" : "BlacklistTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                },
                "columnBlacklist" : {
                  "type" : "array",
                  "description" : "List of columns to exclude from DataFrame",
                  "items" : {
                    "type" : "string"
                  }
                }
              }
            }, {
              "type" : "object",
              "description" : "Repartition DataFrame\nFor detailled description about repartitioning DataFrames see also[[SparkRepartitionDef]]",
              "additionalProperties" : false,
              "required" : [ "numberOfTasksPerPartition", "type" ],
              "title" : "RepartitionTransformer",
              "properties" : {
                "type" : {
                  "const" : "RepartitionTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                },
                "numberOfTasksPerPartition" : {
                  "type" : "integer",
                  "description" : "Number of Spark tasks to create per partition value by repartitioning the DataFrame."
                },
                "keyCols" : {
                  "type" : "array",
                  "description" : "Optional key columns to distribute records over Spark tasks inside a partition value.",
                  "items" : {
                    "type" : "string"
                  }
                }
              }
            }, {
              "type" : "object",
              "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\nThe code is loaded from a Notebook. It should define a transform function with a configurable name, which receives a DataObjectId, a DataFrame\nand a map of options and has to return a DataFrame, see also ([[fnTransformType]] ).\nNotebook-cells starting with \\\"//!IGNORE\\\" will be ignored.",
              "additionalProperties" : false,
              "required" : [ "url", "functionName", "type" ],
              "title" : "ScalaNotebookDfTransformer",
              "properties" : {
                "type" : {
                  "const" : "ScalaNotebookDfTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                },
                "url" : {
                  "type" : "string",
                  "description" : "Url to download notebook in IPYNB-format, which defines transformation."
                },
                "functionName" : {
                  "type" : "string",
                  "description" : "The notebook needs to contain a Scala-function with this name and type[[fnTransformType]] ."
                },
                "authMode" : {
                  "description" : "optional authentication information for webservice, e.g. BasicAuthMode for user/pw authentication",
                  "oneOf" : [ {
                    "$ref" : "#/definitions/AuthMode/AuthHeaderMode"
                  }, {
                    "$ref" : "#/definitions/AuthMode/BasicAuthMode"
                  }, {
                    "$ref" : "#/definitions/AuthMode/CustomHttpAuthMode"
                  }, {
                    "$ref" : "#/definitions/AuthMode/KeycloakClientSecretAuthMode"
                  }, {
                    "$ref" : "#/definitions/AuthMode/PublicKeyAuthMode"
                  }, {
                    "$ref" : "#/definitions/AuthMode/SASLSCRAMAuthMode"
                  }, {
                    "$ref" : "#/definitions/AuthMode/SSLCertsAuthMode"
                  }, {
                    "$ref" : "#/definitions/AuthMode/TokenAuthMode"
                  } ]
                },
                "options" : {
                  "type" : "object",
                  "description" : "Options to pass to the transformation",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                },
                "runtimeOptions" : {
                  "type" : "object",
                  "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                }
              }
            }, {
              "type" : "object",
              "description" : "Apply a filter condition to a DataFrame.",
              "additionalProperties" : false,
              "required" : [ "filterClause", "type" ],
              "title" : "FilterTransformer",
              "properties" : {
                "type" : {
                  "const" : "FilterTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                },
                "filterClause" : {
                  "type" : "string",
                  "description" : "Spark SQL expression to filter the DataFrame"
                }
              }
            }, {
              "type" : "object",
              "description" : "Standardize datatypes of a DataFrame.\nCurrent implementation converts all decimal datatypes to a corresponding integral or float datatype",
              "additionalProperties" : false,
              "required" : [ "type" ],
              "title" : "StandardizeDatatypesTransformer",
              "properties" : {
                "type" : {
                  "const" : "StandardizeDatatypesTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                }
              }
            }, {
              "type" : "object",
              "description" : "Interface to implement Spark-DataFrame transformers working with one input and one output (1:1).\nThis trait extends DfSparkTransformer to pass a map of options as parameter to the transform function. This is mainly\nused by custom transformers.",
              "additionalProperties" : false,
              "required" : [ "type" ],
              "title" : "OptionsDfTransformer",
              "properties" : {
                "type" : {
                  "const" : "OptionsDfTransformer"
                }
              }
            }, {
              "type" : "object",
              "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Python/PySpark code.\nNote that this transformer needs a Python and PySpark environment installed.\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\nOther variables available are\n-`inputDf`: Input DataFrame\n-`options`: Transformation options as Map[String,String]\n-`dataObjectId`: Id of input dataObject as String\nOutput DataFrame must be set with`setOutputDf(df)` .",
              "additionalProperties" : false,
              "required" : [ "type" ],
              "title" : "PythonCodeDfTransformer",
              "properties" : {
                "type" : {
                  "const" : "PythonCodeDfTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                },
                "code" : {
                  "type" : "string",
                  "description" : "Optional python code to user for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
                },
                "file" : {
                  "type" : "string",
                  "description" : "Optional file with python code to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
                },
                "options" : {
                  "type" : "object",
                  "description" : "Options to pass to the transformation",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                },
                "runtimeOptions" : {
                  "type" : "object",
                  "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                }
              }
            }, {
              "type" : "object",
              "description" : "Apply validation rules to a DataFrame and collect potential violation error messages in a new column.",
              "additionalProperties" : false,
              "required" : [ "rules", "type" ],
              "title" : "DataValidationTransformer",
              "properties" : {
                "type" : {
                  "const" : "DataValidationTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                },
                "rules" : {
                  "type" : "array",
                  "description" : "list of validation rules to apply to the DataFrame",
                  "items" : {
                    "$ref" : "#/definitions/ValidationRule/RowLevelValidationRule"
                  }
                },
                "errorsColumn" : {
                  "type" : "string",
                  "description" : "Optional column name for the list of error messages. Default is \\\"errors\\\"."
                }
              }
            }, {
              "type" : "object",
              "description" : "Apply a column whitelist to a DataFrame.",
              "additionalProperties" : false,
              "required" : [ "columnWhitelist", "type" ],
              "title" : "WhitelistTransformer",
              "properties" : {
                "type" : {
                  "const" : "WhitelistTransformer"
                },
                "name" : {
                  "type" : "string",
                  "description" : "name of the transformer"
                },
                "description" : {
                  "type" : "string",
                  "description" : "Optional description of the transformer"
                },
                "columnWhitelist" : {
                  "type" : "array",
                  "description" : "List of columns to keep from DataFrame",
                  "items" : {
                    "type" : "string"
                  }
                }
              }
            } ]
          },
          "subFeedsToApply" : {
            "type" : "array",
            "description" : "Names of SubFeeds the transformation should be applied to.",
            "items" : {
              "type" : "string"
            }
          }
        }
      },
      "SQLDfsTransformer" : {
        "type" : "object",
        "description" : "Configuration of a custom Spark-DataFrame transformation between many inputs and many outputs (n:m) as SQL code.\nThe input data is available as temporary views in SQL. As name for the temporary views the input DataObjectId is used\n(special characters are replaces by underscores).",
        "additionalProperties" : false,
        "required" : [ "code" ],
        "title" : "SQLDfsTransformer",
        "properties" : {
          "type" : {
            "const" : "SQLDfsTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "code" : {
            "type" : "object",
            "description" : "SQL code for transformation.\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\nExample: \\\"select * from test where run = %{runId}\\\"\nA special token %{inputViewName} can be used to insert the temporary view name.",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "options" : {
            "type" : "object",
            "description" : "Options to pass to the transformation",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "runtimeOptions" : {
            "type" : "object",
            "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      },
      "ScalaClassDfsTransformer" : {
        "type" : "object",
        "description" : "Configuration of a custom Spark-DataFrame transformation between many inputs and many outputs (n:m)\nDefine a transform function which receives a map of input DataObjectIds with DataFrames and a map of options and as\nto return a map of output DataObjectIds with DataFrames, see also trait[[CustomDfsTransformer]] .",
        "additionalProperties" : false,
        "required" : [ "className" ],
        "title" : "ScalaClassDfsTransformer",
        "properties" : {
          "type" : {
            "const" : "ScalaClassDfsTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "className" : {
            "type" : "string",
            "description" : "class name implementing trait[[CustomDfsTransformer]]"
          },
          "options" : {
            "type" : "object",
            "description" : "Options to pass to the transformation",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "runtimeOptions" : {
            "type" : "object",
            "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      },
      "ScalaCodeDfsTransformer" : {
        "type" : "object",
        "description" : "Configuration of a custom Spark-DataFrame transformation between many inputs and many outputs (n:m) as Scala code which is compiled at runtime.\nDefine a transform function which receives a map of input DataObjectIds with DataFrames and a map of options and has\nto return a map of output DataObjectIds with DataFrames. The scala code has to implement a function of type[[fnTransformType]] .",
        "additionalProperties" : false,
        "title" : "ScalaCodeDfsTransformer",
        "properties" : {
          "type" : {
            "const" : "ScalaCodeDfsTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "code" : {
            "type" : "string",
            "description" : "Scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
          },
          "file" : {
            "type" : "string",
            "description" : "File where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
          },
          "options" : {
            "type" : "object",
            "description" : "Options to pass to the transformation",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "runtimeOptions" : {
            "type" : "object",
            "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      }
    },
    "ValidationRule" : {
      "RowLevelValidationRule" : {
        "type" : "object",
        "description" : "Definition for a row level data validation rule.",
        "additionalProperties" : false,
        "required" : [ "condition" ],
        "title" : "RowLevelValidationRule",
        "properties" : {
          "type" : {
            "const" : "RowLevelValidationRule"
          },
          "condition" : {
            "type" : "string",
            "description" : "a Spark SQL expression defining the condition to be tested."
          },
          "errorMsg" : {
            "type" : "string",
            "description" : "Optional error msg to be create if the condition fails. Default is to use a text representation of the condition."
          }
        }
      }
    },
    "Connection" : {
      "DeltaLakeTableConnection" : {
        "type" : "object",
        "description" : "Connection information for DeltaLake tables",
        "additionalProperties" : false,
        "required" : [ "id", "db", "pathPrefix" ],
        "title" : "DeltaLakeTableConnection",
        "properties" : {
          "type" : {
            "const" : "DeltaLakeTableConnection"
          },
          "id" : {
            "type" : "string",
            "description" : "unique id of this connection"
          },
          "db" : {
            "type" : "string",
            "description" : "hive db"
          },
          "pathPrefix" : {
            "type" : "string",
            "description" : "schema, authority and base path for tables directory on hadoop"
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "checkDeltaLakeSparkOptions" : {
            "type" : "string",
            "description" : "A unique identifier for this instance."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ConnectionMetadata"
          }
        }
      },
      "HadoopFileConnection" : {
        "type" : "object",
        "description" : "Connection information for files on hadoop",
        "additionalProperties" : false,
        "required" : [ "id", "pathPrefix" ],
        "title" : "HadoopFileConnection",
        "properties" : {
          "type" : {
            "const" : "HadoopFileConnection"
          },
          "id" : {
            "type" : "string",
            "description" : "unique id of this connection"
          },
          "pathPrefix" : {
            "type" : "string",
            "description" : "schema, authority and base path for accessing files on hadoop"
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ConnectionMetadata"
          }
        }
      },
      "HiveTableConnection" : {
        "type" : "object",
        "description" : "Connection information for hive tables",
        "additionalProperties" : false,
        "required" : [ "id", "db", "pathPrefix" ],
        "title" : "HiveTableConnection",
        "properties" : {
          "type" : {
            "const" : "HiveTableConnection"
          },
          "id" : {
            "type" : "string",
            "description" : "unique id of this connection"
          },
          "db" : {
            "type" : "string",
            "description" : "hive db"
          },
          "pathPrefix" : {
            "type" : "string",
            "description" : "schema, authority and base path for tables directory on hadoop"
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ConnectionMetadata"
          }
        }
      },
      "JdbcTableConnection" : {
        "type" : "object",
        "description" : "Connection information for jdbc tables.\nIf authentication is needed, user and password must be provided.",
        "additionalProperties" : false,
        "required" : [ "id", "url", "driver" ],
        "title" : "JdbcTableConnection",
        "properties" : {
          "type" : {
            "const" : "JdbcTableConnection"
          },
          "id" : {
            "type" : "string",
            "description" : "unique id of this connection"
          },
          "url" : {
            "type" : "string",
            "description" : "jdbc connection url"
          },
          "driver" : {
            "type" : "string",
            "description" : "class name of jdbc driver"
          },
          "authMode" : {
            "description" : "optional authentication information: for now BasicAuthMode is supported.",
            "oneOf" : [ {
              "$ref" : "#/definitions/AuthMode/AuthHeaderMode"
            }, {
              "$ref" : "#/definitions/AuthMode/BasicAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/CustomHttpAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/KeycloakClientSecretAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/PublicKeyAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SASLSCRAMAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SSLCertsAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/TokenAuthMode"
            } ]
          },
          "db" : {
            "type" : "string",
            "description" : "jdbc database"
          },
          "maxParallelConnections" : {
            "type" : "integer",
            "description" : "number of parallel jdbc connections created by an instance of this connection\nNote that Spark manages JDBC Connections on its own. This setting only applies to JDBC connection\nused by SDL for validating metadata or pre/postSQL."
          },
          "connectionPoolMaxIdleTimeSec" : {
            "type" : "integer",
            "description" : "timeout to close unused connections in the pool"
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ConnectionMetadata"
          }
        }
      },
      "KafkaConnection" : {
        "type" : "object",
        "description" : "Connection information for kafka",
        "additionalProperties" : false,
        "required" : [ "id", "brokers" ],
        "title" : "KafkaConnection",
        "properties" : {
          "type" : {
            "const" : "KafkaConnection"
          },
          "id" : {
            "type" : "string",
            "description" : "unique id of this connection"
          },
          "brokers" : {
            "type" : "string",
            "description" : "comma separated list of kafka bootstrap server incl. port, e.g. \\\"host1:9092,host2:9092:"
          },
          "schemaRegistry" : {
            "type" : "string",
            "description" : "url of schema registry service, e.g. \\\"https://host2\\\""
          },
          "options" : {
            "type" : "object",
            "description" : "Options for the Kafka stream reader (see https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "authMode" : {
            "description" : "A unique identifier for this instance.",
            "oneOf" : [ {
              "$ref" : "#/definitions/AuthMode/AuthHeaderMode"
            }, {
              "$ref" : "#/definitions/AuthMode/BasicAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/CustomHttpAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/KeycloakClientSecretAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/PublicKeyAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SASLSCRAMAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SSLCertsAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/TokenAuthMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ConnectionMetadata"
          }
        }
      },
      "SftpFileRefConnection" : {
        "type" : "object",
        "description" : "SFTP Connection information",
        "additionalProperties" : false,
        "required" : [ "id", "host", "authMode" ],
        "title" : "SftpFileRefConnection",
        "properties" : {
          "type" : {
            "const" : "SftpFileRefConnection"
          },
          "id" : {
            "type" : "string",
            "description" : "unique id of this connection"
          },
          "host" : {
            "type" : "string",
            "description" : "sftp host"
          },
          "port" : {
            "type" : "integer",
            "description" : "port of sftp service, default is 22"
          },
          "authMode" : {
            "description" : "authentication information: for now BasicAuthMode and PublicKeyAuthMode are supported.",
            "oneOf" : [ {
              "$ref" : "#/definitions/AuthMode/AuthHeaderMode"
            }, {
              "$ref" : "#/definitions/AuthMode/BasicAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/CustomHttpAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/KeycloakClientSecretAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/PublicKeyAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SASLSCRAMAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SSLCertsAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/TokenAuthMode"
            } ]
          },
          "ignoreHostKeyVerification" : {
            "type" : "string",
            "description" : "do not validate host key if true, default is false"
          },
          "maxParallelConnections" : {
            "type" : "integer",
            "description" : "number of parallel sftp connections created by an instance of this connection"
          },
          "connectionPoolMaxIdleTimeSec" : {
            "type" : "integer",
            "description" : "timeout to close unused connections in the pool"
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ConnectionMetadata"
          }
        }
      },
      "SnowflakeConnection" : {
        "type" : "object",
        "description" : "Connection information for Snowflake databases.\nThe connection can be used for SnowflakeTableDataObjects\nIf multiple SnowflakeTableDataObjects share a connection, they share the same Snowpark session",
        "additionalProperties" : false,
        "required" : [ "id", "url", "warehouse", "database", "role", "authMode" ],
        "title" : "SnowflakeConnection",
        "properties" : {
          "type" : {
            "const" : "SnowflakeConnection"
          },
          "id" : {
            "type" : "string",
            "description" : "unique id of this connection"
          },
          "url" : {
            "type" : "string",
            "description" : "snowflake connection url"
          },
          "warehouse" : {
            "type" : "string",
            "description" : "Snowflake namespace"
          },
          "database" : {
            "type" : "string",
            "description" : "Snowflake database"
          },
          "role" : {
            "type" : "string",
            "description" : "Snowflake role"
          },
          "authMode" : {
            "description" : "optional authentication information: for now BasicAuthMode is supported.",
            "oneOf" : [ {
              "$ref" : "#/definitions/AuthMode/AuthHeaderMode"
            }, {
              "$ref" : "#/definitions/AuthMode/BasicAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/CustomHttpAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/KeycloakClientSecretAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/PublicKeyAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SASLSCRAMAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SSLCertsAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/TokenAuthMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ConnectionMetadata"
          }
        }
      },
      "SplunkConnection" : {
        "type" : "object",
        "description" : "Connection information for splunk queries",
        "additionalProperties" : false,
        "required" : [ "id", "host", "port", "authMode" ],
        "title" : "SplunkConnection",
        "properties" : {
          "type" : {
            "const" : "SplunkConnection"
          },
          "id" : {
            "type" : "string",
            "description" : "unique id of this connection"
          },
          "host" : {
            "type" : "string",
            "description" : ""
          },
          "port" : {
            "type" : "integer",
            "description" : ""
          },
          "authMode" : {
            "description" : "",
            "oneOf" : [ {
              "$ref" : "#/definitions/AuthMode/AuthHeaderMode"
            }, {
              "$ref" : "#/definitions/AuthMode/BasicAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/CustomHttpAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/KeycloakClientSecretAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/PublicKeyAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SASLSCRAMAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SSLCertsAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/TokenAuthMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ConnectionMetadata"
          }
        }
      }
    },
    "HousekeepingMode" : {
      "PartitionArchiveCompactionMode" : {
        "type" : "object",
        "description" : "Archive and compact old partitions:\nArchive partition reduces the number of partitions in the past by moving older partitions into special \\\"archive partitions\\\".\nCompact partition reduces the number of files in a partition by rewriting them with Spark.\nExample: archive and compact a table with partition layout run_id=<integer>\n- archive partitions after 1000 partitions into \\\"archive partition\\\" equal to floor(run_id/1000)\n- compact \\\"archive partition\\\" when full\n{{{\nhousekeepingMode = {\ntype = PartitionArchiveCompactionMode\narchivePartitionExpression = \\\"if( elements[\\'run_id\\'] < runId - 1000, map(\\'run_id\\', elements[\\'run_id\\'] div 1000), elements)\\\"\ncompactPartitionExpression = \\\"elements[\\'run_id\\'] % 1000 = 0 and elements[\\'run_id\\'] <= runId - 2000\\\"\n}\n}}}",
        "additionalProperties" : false,
        "title" : "PartitionArchiveCompactionMode",
        "properties" : {
          "type" : {
            "const" : "PartitionArchiveCompactionMode"
          },
          "archivePartitionExpression" : {
            "type" : "string",
            "description" : "Expression to define the archive partition for a given partition. Define a spark\nsql expression working with the attributes of[[PartitionExpressionData]] returning archive\npartition values as Map[String,String]. If return value is the same as input elements, partition is not touched,\notherwise all files of the partition are moved to the returned partition definition.\nBe aware that the value of the partition columns changes for these files/records."
          },
          "compactPartitionExpression" : {
            "type" : "string",
            "description" : "Expression to define partitions which should be compacted. Define a spark\nsql expression working with the attributes of[[PartitionExpressionData]] returning a\nboolean = true when this partition should be compacted.\nOnce a partition is compacted, it is marked as compacted and will not be compacted again.\nIt is therefore ok to return true for all partitions which should be compacted, regardless if they have been compacted already."
          },
          "description" : {
            "type" : "string"
          }
        }
      },
      "PartitionRetentionMode" : {
        "type" : "object",
        "description" : "Keep partitions while retention condition is fulfilled, delete other partitions.\nExample: cleanup partitions with partition layout dt=<yyyymmdd> after 90 days:\n{{{\nhousekeepingMode = {\ntype = PartitionRetentionMode\nretentionCondition = \\\"datediff(now(), to_date(elements[\\'dt\\'], \\'yyyyMMdd\\')) <= 90\\\"\n}\n}}}",
        "additionalProperties" : false,
        "required" : [ "retentionCondition" ],
        "title" : "PartitionRetentionMode",
        "properties" : {
          "type" : {
            "const" : "PartitionRetentionMode"
          },
          "retentionCondition" : {
            "type" : "string",
            "description" : "Condition to decide if a partition should be kept. Define a spark sql expression\nworking with the attributes of[[PartitionExpressionData]] returning a boolean with value true if the partition should be kept."
          },
          "description" : {
            "type" : "string"
          }
        }
      }
    },
    "DataObject" : {
      "AccessTableDataObject" : {
        "type" : "object",
        "description" : "[[DataObject]]of type JDBC / Access.\nProvides access to a Access DB to an Action. The functionality is handled seperately from[[JdbcTableDataObject]] \nto avoid problems with net.ucanaccess.jdbc.UcanaccessDriver",
        "additionalProperties" : false,
        "required" : [ "id", "path", "table" ],
        "title" : "AccessTableDataObject",
        "properties" : {
          "type" : {
            "const" : "AccessTableDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "path" : {
            "type" : "string",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "table" : {
            "$ref" : "#/definitions/Others/Table"
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "ActionsExporterDataObject" : {
        "type" : "object",
        "description" : "Exports a util[[DataFrame]]that contains properties and metadata extracted from all[[io.smartdatalake.workflow.action.Action]]s\nthat are registered in the current[[InstanceRegistry]].\nAlternatively, it can export the properties and metadata of all[[io.smartdatalake.workflow.action.Action]]s defined in config files. For this, the\nconfiguration \\\"config\\\" has to be set to the location of the config.\nExample:\n{{{\ndataObjects = {\n...\nactions-exporter {\ntype = ActionsExporterDataObject\nconfig = path/to/myconfiguration.conf\n}\n...\n}\n}}}\nThe config value can point to a configuration file or a directory containing configuration files.\nSEE: Refer to[[ConfigLoader.loadConfigFromFilesystem()]] for details about the configuration loading.",
        "additionalProperties" : false,
        "required" : [ "id" ],
        "title" : "ActionsExporterDataObject",
        "properties" : {
          "type" : {
            "const" : "ActionsExporterDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "A unique identifier for this instance."
          },
          "config" : {
            "type" : "string",
            "description" : "A unique identifier for this instance."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "AirbyteDataObject" : {
        "type" : "object",
        "description" : "Limitations: Connectors have only access to locally mounted directories",
        "additionalProperties" : false,
        "required" : [ "id", "config", "streamName", "cmd" ],
        "title" : "AirbyteDataObject",
        "properties" : {
          "type" : {
            "const" : "AirbyteDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "DataObject identifier"
          },
          "config" : {
            "type" : "string",
            "existingJavaType" : "com.typesafe.config.Config",
            "description" : "Configuration for the source"
          },
          "streamName" : {
            "type" : "string",
            "description" : "The stream name to read. Must match an entry of the catalog of the source."
          },
          "cmd" : {
            "description" : "command to launch airbyte connector. Normally this is of type[[DockerRunScript]] .",
            "oneOf" : [ {
              "$ref" : "#/definitions/ParsableScriptDef/CmdScript"
            }, {
              "$ref" : "#/definitions/ParsableScriptDef/DockerRunScript"
            } ]
          },
          "incrementalCursorFields" : {
            "type" : "array",
            "description" : "Some sources need a specification of the cursor field for incremental mode",
            "items" : {
              "type" : "string"
            }
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "AvroFileDataObject" : {
        "type" : "object",
        "description" : "A[[io.smartdatalake.workflow.dataobject.DataObject]]backed by an Avro data source.\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\nwork on Avro formatted files.\nReading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\nand[[org.apache.spark.sql.DataFrameWriter]]respectively. The reader and writer implementations are provided by\nthe[[https://github.com/databricks/spark-avro databricks spark-avro]] project.\nSEE: [[org.apache.spark.sql.DataFrameReader]]\nSEE: [[org.apache.spark.sql.DataFrameWriter]]",
        "additionalProperties" : false,
        "required" : [ "id", "path" ],
        "title" : "AvroFileDataObject",
        "properties" : {
          "type" : {
            "const" : "AvroFileDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "path" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "partitions" : {
            "type" : "array",
            "description" : "The Spark-Format provider to be used",
            "items" : {
              "type" : "string"
            }
          },
          "avroOptions" : {
            "type" : "object",
            "description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and\n[[org.apache.spark.sql.DataFrameWriter]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "schema" : {
            "type" : "string",
            "description" : "An optional schema for the spark data frame to be validated on read and write. Note: Existing Avro files\ncontain a source schema. Therefore, this schema is ignored when reading from existing Avro files.\nAs this corresponds to the schema on write, it must not include the optional filenameColumn on read."
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "The Spark-Format provider to be used"
          },
          "sparkRepartition" : {
            "type" : "object",
            "description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
            "additionalProperties" : false,
            "required" : [ "numberOfTasksPerPartition" ],
            "title" : "SparkRepartitionDef",
            "properties" : {
              "numberOfTasksPerPartition" : {
                "type" : "integer",
                "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\nThis controls how many files are created in each Hadoop partition."
              },
              "keyCols" : {
                "type" : "array",
                "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
                "items" : {
                  "type" : "string"
                }
              },
              "sortCols" : {
                "type" : "array",
                "description" : "Optional columns to sort records inside files created.",
                "items" : {
                  "type" : "string"
                }
              },
              "filename" : {
                "type" : "string",
                "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
              }
            }
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "filenameColumn" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "housekeepingMode" : {
            "description" : "Optional definition of a housekeeping mode applied after every write. E.g. it can be used to cleanup, archive and compact partitions.\nSee HousekeepingMode for available implementations. Default is None.\n*",
            "oneOf" : [ {
              "$ref" : "#/definitions/HousekeepingMode/PartitionArchiveCompactionMode"
            }, {
              "$ref" : "#/definitions/HousekeepingMode/PartitionRetentionMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "CsvFileDataObject" : {
        "type" : "object",
        "description" : "A[[DataObject]]backed by a comma-separated value (CSV) data source.\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\nwork on CSV formatted files.\nCSV reading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\nand[[org.apache.spark.sql.DataFrameWriter]]respectively.\nRead Schema specifications:\nIf a data object schema is not defined via the`schema`attribute (default) and`inferSchema`option is\ndisabled (default) in`csvOptions`, then all column types are set to String and the first row of the CSV file is read\nto determine the column names and the number of fields.\nIf the`header`option is disabled (default) in`csvOptions`, then the header is defined as \\\"_c#\\\" for each column\nwhere \\\"#\\\" is the column index.\nOtherwise the first row of the CSV file is not included in the DataFrame content and its entries\nare used as the column names for the schema.\nIf a data object schema is not defined via the`schema`attribute and`inferSchema`is enabled in`csvOptions`, then\nthe`samplingRatio`(default: 1.0) option in`csvOptions` is used to extract a sample from the CSV file in order to\ndetermine the input schema automatically.\nNOTE: This data object sets the following default values for`csvOptions`: delimiter = \\\"|\\\", quote = null, header = false, and inferSchema = false.\nAll other`csvOption` default to the values defined by Apache Spark.\nSEE: [[org.apache.spark.sql.DataFrameReader]]\nSEE: [[org.apache.spark.sql.DataFrameWriter]]",
        "additionalProperties" : false,
        "required" : [ "id", "path" ],
        "title" : "CsvFileDataObject",
        "properties" : {
          "type" : {
            "const" : "CsvFileDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "path" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "csvOptions" : {
            "type" : "object",
            "description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and[[org.apache.spark.sql.DataFrameWriter]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "partitions" : {
            "type" : "array",
            "description" : "The Spark-Format provider to be used",
            "items" : {
              "type" : "string"
            }
          },
          "schema" : {
            "type" : "string",
            "description" : "An optional data object schema. If defined, any automatic schema inference is avoided. As this corresponds to the schema on write, it must not include the optional filenameColumn on read."
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "dateColumnType" : {
            "type" : "string",
            "enum" : [ "Default ", "String ", "Date " ],
            "description" : "Specifies the string format used for writing date typed data."
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "The Spark-Format provider to be used"
          },
          "sparkRepartition" : {
            "type" : "object",
            "description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
            "additionalProperties" : false,
            "required" : [ "numberOfTasksPerPartition" ],
            "title" : "SparkRepartitionDef",
            "properties" : {
              "numberOfTasksPerPartition" : {
                "type" : "integer",
                "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\nThis controls how many files are created in each Hadoop partition."
              },
              "keyCols" : {
                "type" : "array",
                "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
                "items" : {
                  "type" : "string"
                }
              },
              "sortCols" : {
                "type" : "array",
                "description" : "Optional columns to sort records inside files created.",
                "items" : {
                  "type" : "string"
                }
              },
              "filename" : {
                "type" : "string",
                "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
              }
            }
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "filenameColumn" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "housekeepingMode" : {
            "description" : "Optional definition of a housekeeping mode applied after every write. E.g. it can be used to cleanup, archive and compact partitions.\nSee HousekeepingMode for available implementations. Default is None.",
            "oneOf" : [ {
              "$ref" : "#/definitions/HousekeepingMode/PartitionArchiveCompactionMode"
            }, {
              "$ref" : "#/definitions/HousekeepingMode/PartitionRetentionMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "CustomDfDataObject" : {
        "type" : "object",
        "description" : "Generic[[DataObject]] containing a config object.\nE.g. used to implement a CustomAction that reads a Webservice.",
        "additionalProperties" : false,
        "required" : [ "id", "creator" ],
        "title" : "CustomDfDataObject",
        "properties" : {
          "type" : {
            "const" : "CustomDfDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "creator" : {
            "type" : "object",
            "description" : "Configuration of a custom Spark-DataFrame creator as part of[[CustomDfDataObject]]\nDefine a exec function which receives a map of options and returns a DataFrame to be used as input.\nOptionally define a schema function to return a StructType used as schema in init-phase.\nSee also trait[[CustomDfCreator]] .\nNote that for now implementing CustomDfCreator.schema method is only possible with className configuration attribute.",
            "additionalProperties" : false,
            "title" : "CustomDfCreatorConfig",
            "properties" : {
              "className" : {
                "type" : "string",
                "description" : "Optional class name implementing trait[[CustomDfCreator]]"
              },
              "scalaFile" : {
                "type" : "string",
                "description" : "Optional file where scala code for creator is loaded from. The scala code in the file needs to be a function of type[[fnExecType]] ."
              },
              "scalaCode" : {
                "type" : "string",
                "description" : "Optional scala code for creator. The scala code needs to be a function of type[[fnExecType]] ."
              },
              "options" : {
                "type" : "object",
                "description" : "Options to pass to the creator",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "CustomFileDataObject" : {
        "type" : "object",
        "additionalProperties" : false,
        "required" : [ "id", "creator" ],
        "title" : "CustomFileDataObject",
        "properties" : {
          "type" : {
            "const" : "CustomFileDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout"
          },
          "creator" : {
            "type" : "object",
            "additionalProperties" : false,
            "title" : "CustomFileCreatorConfig",
            "properties" : {
              "className" : {
                "type" : "string"
              },
              "scalaFile" : {
                "type" : "string"
              },
              "scalaCode" : {
                "type" : "string"
              },
              "options" : {
                "type" : "object",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "DataObjectsExporterDataObject" : {
        "type" : "object",
        "description" : "Exports a util[[DataFrame]]that contains properties and metadata extracted from all[[DataObject]]s\nthat are registered in the current[[InstanceRegistry]].\nAlternatively, it can export the properties and metadata of all[[DataObject]]s defined in config files. For this, the\nconfiguration \\\"config\\\" has to be set to the location of the config.\nExample:\n{{{\n```dataObjects = {\n...\ndataobject-exporter {\ntype = DataObjectsExporterDataObject\nconfig = path/to/myconfiguration.conf\n}\n...\n}\n}}}\nThe config value can point to a configuration file or a directory containing configuration files.\nSEE: Refer to[[ConfigLoader.loadConfigFromFilesystem()]] for details about the configuration loading.",
        "additionalProperties" : false,
        "required" : [ "id" ],
        "title" : "DataObjectsExporterDataObject",
        "properties" : {
          "type" : {
            "const" : "DataObjectsExporterDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "A unique identifier for this instance."
          },
          "config" : {
            "type" : "string",
            "description" : "A unique identifier for this instance."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "DeltaLakeTableDataObject" : {
        "type" : "object",
        "description" : "[[DataObject]] of type DeltaLakeTableDataObject.\nProvides details to access Tables in delta format to an Action.\nDelta format maintains a transaction log in a separate _delta_log subfolder.\nThe schema is registered in Metastore by DeltaLakeTableDataObject.\nThe following anomalies might occur:\n- table is registered in metastore but path does not exist -> table is dropped from metastore\n- table is registered in metastore but path is empty -> error is thrown. Delete the path to clean up\n- table is registered and path contains parquet files, but _delta_log subfolder is missing -> path is converted to delta format\n- table is not registered but path contains parquet files and _delta_log subfolder -> Table is registered\n- table is not registered but path contains parquet files without _delta_log subfolder -> path is converted to delta format and table is registered\n- table is not registered and path does not exists -> table is created on write",
        "additionalProperties" : false,
        "required" : [ "id", "table" ],
        "title" : "DeltaLakeTableDataObject",
        "properties" : {
          "type" : {
            "const" : "DeltaLakeTableDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "unique name of this data object"
          },
          "path" : {
            "type" : "string",
            "description" : "hadoop directory for this table. If it doesn\\'t contain scheme and authority, the connections pathPrefix is applied.\nIf pathPrefix is not defined or doesn\\'t define scheme and authority, default schema and authority is applied."
          },
          "partitions" : {
            "type" : "array",
            "description" : "partition columns for this data object",
            "items" : {
              "type" : "string"
            }
          },
          "options" : {
            "type" : "object",
            "description" : "Options for Delta Lake tables see:[[https://docs.delta.io/latest/delta-batch.html]]and[[org.apache.spark.sql.delta.DeltaOptions]]",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "An optional, minimal schema that this DataObject must have to pass schema validation on reading and writing."
          },
          "table" : {
            "$ref" : "#/definitions/Others/Table"
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "[[SDLSaveMode]] to use when writing files, default is \\\"overwrite\\\". Overwrite, Append and Merge are supported for now."
          },
          "allowSchemaEvolution" : {
            "type" : "string",
            "description" : "If set to true schema evolution will automatically occur when writing to this DataObject with different schema, otherwise SDL will stop with error."
          },
          "retentionPeriod" : {
            "type" : "integer",
            "description" : "Optional delta lake retention threshold in hours. Files required by the table for reading versions younger than retentionPeriod will be preserved and the rest of them will be deleted."
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "optional id of[[io.smartdatalake.workflow.connection.HiveTableConnection]]"
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "housekeepingMode" : {
            "description" : "Optional definition of a housekeeping mode applied after every write. E.g. it can be used to cleanup, archive and compact partitions.\nSee HousekeepingMode for available implementations. Default is None.",
            "oneOf" : [ {
              "$ref" : "#/definitions/HousekeepingMode/PartitionArchiveCompactionMode"
            }, {
              "$ref" : "#/definitions/HousekeepingMode/PartitionRetentionMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "ExcelFileDataObject" : {
        "type" : "object",
        "description" : "A[[DataObject]]backed by an Microsoft Excel data source.\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\nwork on Microsoft Excel (.xslx) formatted files.\nReading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\nand[[org.apache.spark.sql.DataFrameWriter]]respectively. The reader and writer implementation is provided by the\n[[https://github.com/crealytics/spark-excel Crealytics spark-excel]]project.\nRead Schema:\nWhen`useHeader`is set to true (default), the reader will use the first row of the Excel sheet as column names for\nthe schema and not include the first row as data values. Otherwise the column names are taken from the schema.\nIf the schema is not provided or inferred, then each column name is defined as \\\"_c#\\\" where \\\"#\\\" is the column index.\nWhen a data object schema is provided, it is used as the schema for the DataFrame. Otherwise if`inferSchema`is\nenabled (default), then the data types of the columns are inferred based on the first`excerptSize`rows\n(excluding the first).\nWhen no schema is provided and`inferSchema` is disabled, all columns are assumed to be of string type.",
        "additionalProperties" : false,
        "required" : [ "id", "path" ],
        "title" : "ExcelFileDataObject",
        "properties" : {
          "type" : {
            "const" : "ExcelFileDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "path" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "excelOptions" : {
            "type" : "object",
            "description" : "Options passed to[[org.apache.spark.sql.DataFrameReader]]and[[org.apache.spark.sql.DataFrameWriter]] for\nreading and writing Microsoft Excel files. Excel support is provided by the spark-excel project (see link below).\nSEE: [[https://github.com/crealytics/spark-excel]]",
            "additionalProperties" : false,
            "title" : "ExcelOptions",
            "properties" : {
              "sheetName" : {
                "type" : "string",
                "description" : "Optional name of the Excel Sheet to read from/write to."
              },
              "numLinesToSkip" : {
                "type" : "integer",
                "description" : "Optional number of rows in the excel spreadsheet to skip before any data is read.\nThis option must not be set for writing."
              },
              "startColumn" : {
                "type" : "string",
                "description" : "Optional first column in the specified Excel Sheet to read from (as string, e.g B).\nThis option must not be set for writing."
              },
              "endColumn" : {
                "type" : "string",
                "description" : "Optional last column in the specified Excel Sheet to read from (as string, e.g. F)."
              },
              "rowLimit" : {
                "type" : "integer",
                "description" : "Optional limit of the number of rows being returned on read.\nThis is applied after`numLinesToSkip` ."
              },
              "useHeader" : {
                "type" : "string",
                "description" : "If`true` , the first row of the excel sheet specifies the column names (default: true)."
              },
              "treatEmptyValuesAsNulls" : {
                "type" : "string",
                "description" : "Empty cells are parsed as`null` values (default: true)."
              },
              "inferSchema" : {
                "type" : "string",
                "description" : "Infer the schema of the excel sheet automatically (default: true)."
              },
              "timestampFormat" : {
                "type" : "string",
                "description" : "A format string specifying the format to use when writing timestamps (default: dd-MM-yyyy HH:mm:ss)."
              },
              "dateFormat" : {
                "type" : "string",
                "description" : "A format string specifying the format to use when writing dates."
              },
              "maxRowsInMemory" : {
                "type" : "integer",
                "description" : "The number of rows that are stored in memory.\nIf set, a streaming reader is used which can help with big files."
              },
              "excerptSize" : {
                "type" : "integer",
                "description" : "Sample size for schema inference."
              }
            }
          },
          "partitions" : {
            "type" : "array",
            "description" : "The Spark-Format provider to be used",
            "items" : {
              "type" : "string"
            }
          },
          "schema" : {
            "type" : "string",
            "description" : "An optional data object schema. If defined, any automatic schema inference is avoided. As this corresponds to the schema on write, it must not include the optional filenameColumn on read."
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "The Spark-Format provider to be used"
          },
          "sparkRepartition" : {
            "type" : "object",
            "description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
            "additionalProperties" : false,
            "required" : [ "numberOfTasksPerPartition" ],
            "title" : "SparkRepartitionDef",
            "properties" : {
              "numberOfTasksPerPartition" : {
                "type" : "integer",
                "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\nThis controls how many files are created in each Hadoop partition."
              },
              "keyCols" : {
                "type" : "array",
                "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
                "items" : {
                  "type" : "string"
                }
              },
              "sortCols" : {
                "type" : "array",
                "description" : "Optional columns to sort records inside files created.",
                "items" : {
                  "type" : "string"
                }
              },
              "filename" : {
                "type" : "string",
                "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
              }
            }
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "filenameColumn" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "housekeepingMode" : {
            "description" : "Optional definition of a housekeeping mode applied after every write. E.g. it can be used to cleanup, archive and compact partitions.\nSee HousekeepingMode for available implementations. Default is None.",
            "oneOf" : [ {
              "$ref" : "#/definitions/HousekeepingMode/PartitionArchiveCompactionMode"
            }, {
              "$ref" : "#/definitions/HousekeepingMode/PartitionRetentionMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "HiveTableDataObject" : {
        "type" : "object",
        "description" : "[[DataObject]] of type Hive.\nProvides details to access Hive tables to an Action",
        "additionalProperties" : false,
        "required" : [ "id", "table" ],
        "title" : "HiveTableDataObject",
        "properties" : {
          "type" : {
            "const" : "HiveTableDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "unique name of this data object"
          },
          "path" : {
            "type" : "string",
            "description" : "hadoop directory for this table. If it doesn\\'t contain scheme and authority, the connections pathPrefix is applied.\nIf pathPrefix is not defined or doesn\\'t define scheme and authority, default schema and authority is applied.\nIf DataObject is only used for reading or if the HiveTable already exist, the path can be omitted.\nIf the HiveTable already exists but with a different path, a warning is issued"
          },
          "partitions" : {
            "type" : "array",
            "description" : "partition columns for this data object",
            "items" : {
              "type" : "string"
            }
          },
          "analyzeTableAfterWrite" : {
            "type" : "string",
            "description" : "enable compute statistics after writing data (default=false)"
          },
          "dateColumnType" : {
            "type" : "string",
            "enum" : [ "Default ", "String ", "Date " ],
            "description" : "type of date column"
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "An optional, minimal schema that this DataObject must have to pass schema validation on reading and writing."
          },
          "table" : {
            "$ref" : "#/definitions/Others/Table"
          },
          "numInitialHdfsPartitions" : {
            "type" : "integer",
            "description" : "number of files created when writing into an empty table (otherwise the number will be derived from the existing data)"
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "spark[[SaveMode]] to use when writing files, default is \\\"overwrite\\\""
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "optional id of[[io.smartdatalake.workflow.connection.HiveTableConnection]]"
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "housekeepingMode" : {
            "description" : "Optional definition of a housekeeping mode applied after every write. E.g. it can be used to cleanup, archive and compact partitions.\nSee HousekeepingMode for available implementations. Default is None.",
            "oneOf" : [ {
              "$ref" : "#/definitions/HousekeepingMode/PartitionArchiveCompactionMode"
            }, {
              "$ref" : "#/definitions/HousekeepingMode/PartitionRetentionMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "JdbcTableDataObject" : {
        "type" : "object",
        "description" : "[[DataObject]] of type JDBC.\nProvides details for an action to access tables in a database through JDBC.",
        "additionalProperties" : false,
        "required" : [ "id", "table", "connectionId" ],
        "title" : "JdbcTableDataObject",
        "properties" : {
          "type" : {
            "const" : "JdbcTableDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "unique name of this data object"
          },
          "createSql" : {
            "type" : "string",
            "description" : "DDL-statement to be executed in prepare phase, using output jdbc connection.\nNote that it is also possible to let Spark create the table in Init-phase. See jdbcOptions to customize column data types for auto-created DDL-statement."
          },
          "preReadSql" : {
            "type" : "string",
            "description" : "SQL-statement to be executed in exec phase before reading input table, using input jdbc connection.\nUse tokens with syntax %{<spark sql expression>} to substitute with values from[[DefaultExpressionData]] ."
          },
          "postReadSql" : {
            "type" : "string",
            "description" : "SQL-statement to be executed in exec phase after reading input table and before action is finished, using input jdbc connection\nUse tokens with syntax %{<spark sql expression>} to substitute with values from[[DefaultExpressionData]] ."
          },
          "preWriteSql" : {
            "type" : "string",
            "description" : "SQL-statement to be executed in exec phase before writing output table, using output jdbc connection\nUse tokens with syntax %{<spark sql expression>} to substitute with values from[[DefaultExpressionData]] ."
          },
          "postWriteSql" : {
            "type" : "string",
            "description" : "SQL-statement to be executed in exec phase after writing output table, using output jdbc connection\nUse tokens with syntax %{<spark sql expression>} to substitute with values from[[DefaultExpressionData]] ."
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "An optional, minimal schema that this DataObject must have to pass schema validation on reading and writing."
          },
          "table" : {
            "$ref" : "#/definitions/Others/Table"
          },
          "jdbcFetchSize" : {
            "type" : "integer",
            "description" : "Number of rows to be fetched together by the Jdbc driver"
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "[[SDLSaveMode]] to use when writing table, default is \\\"Overwrite\\\". Only \\\"Append\\\" and \\\"Overwrite\\\" supported."
          },
          "allowSchemaEvolution" : {
            "type" : "string",
            "description" : "If set to true schema evolution will automatically occur when writing to this DataObject with different schema, otherwise SDL will stop with error."
          },
          "connectionId" : {
            "type" : "string",
            "description" : "Id of JdbcConnection configuration"
          },
          "jdbcOptions" : {
            "type" : "object",
            "description" : "Any jdbc options according to[[https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html]] .\nNote that some options above set and override some of this options explicitly.\nUse \\\"createTableOptions\\\" and \\\"createTableColumnTypes\\\" to control automatic creating of database tables.",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "virtualPartitions" : {
            "type" : "array",
            "description" : "Virtual partition columns. Note that this doesn\\'t need to be the same as the database partition\ncolumns for this table. But it is important that there is an index on these columns to efficiently\nlist existing \\\"partitions\\\".",
            "items" : {
              "type" : "string"
            }
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "JmsDataObject" : {
        "type" : "object",
        "description" : "[[DataObject]] of type JMS queue.\nProvides details to an Action to access JMS queues.",
        "additionalProperties" : false,
        "required" : [ "id", "jndiContextFactory", "jndiProviderUrl", "authMode", "batchSize", "maxWaitSec", "maxBatchAgeSec", "txBatchSize", "connectionFactory", "queue" ],
        "title" : "JmsDataObject",
        "properties" : {
          "type" : {
            "const" : "JmsDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "jndiContextFactory" : {
            "type" : "string",
            "description" : "JNDI Context Factory"
          },
          "jndiProviderUrl" : {
            "type" : "string",
            "description" : "JNDI Provider URL"
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "authMode" : {
            "description" : "authentication information: for now BasicAuthMode is supported.",
            "oneOf" : [ {
              "$ref" : "#/definitions/AuthMode/AuthHeaderMode"
            }, {
              "$ref" : "#/definitions/AuthMode/BasicAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/CustomHttpAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/KeycloakClientSecretAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/PublicKeyAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SASLSCRAMAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SSLCertsAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/TokenAuthMode"
            } ]
          },
          "batchSize" : {
            "type" : "integer",
            "description" : "JMS batch size"
          },
          "maxWaitSec" : {
            "type" : "integer",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "maxBatchAgeSec" : {
            "type" : "integer",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "txBatchSize" : {
            "type" : "integer",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "connectionFactory" : {
            "type" : "string",
            "description" : "JMS Connection Factory"
          },
          "queue" : {
            "type" : "string",
            "description" : "Name of MQ Queue"
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "JsonFileDataObject" : {
        "type" : "object",
        "description" : "A[[io.smartdatalake.workflow.dataobject.DataObject]]backed by a JSON data source.\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\nwork on JSON formatted files.\nReading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\nand[[org.apache.spark.sql.DataFrameWriter]] respectively.\nNOTE: By default, the JSON option`multiline` is enabled.\nSEE: [[org.apache.spark.sql.DataFrameReader]]\nSEE: [[org.apache.spark.sql.DataFrameWriter]]",
        "additionalProperties" : false,
        "required" : [ "id", "path" ],
        "title" : "JsonFileDataObject",
        "properties" : {
          "type" : {
            "const" : "JsonFileDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "path" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "jsonOptions" : {
            "type" : "object",
            "description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and\n[[org.apache.spark.sql.DataFrameWriter]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "partitions" : {
            "type" : "array",
            "description" : "The Spark-Format provider to be used",
            "items" : {
              "type" : "string"
            }
          },
          "schema" : {
            "type" : "string",
            "description" : "An optional data object schema. If defined, any automatic schema inference is avoided. As this corresponds to the schema on write, it must not include the optional filenameColumn on read."
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "The Spark-Format provider to be used"
          },
          "sparkRepartition" : {
            "type" : "object",
            "description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
            "additionalProperties" : false,
            "required" : [ "numberOfTasksPerPartition" ],
            "title" : "SparkRepartitionDef",
            "properties" : {
              "numberOfTasksPerPartition" : {
                "type" : "integer",
                "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\nThis controls how many files are created in each Hadoop partition."
              },
              "keyCols" : {
                "type" : "array",
                "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
                "items" : {
                  "type" : "string"
                }
              },
              "sortCols" : {
                "type" : "array",
                "description" : "Optional columns to sort records inside files created.",
                "items" : {
                  "type" : "string"
                }
              },
              "filename" : {
                "type" : "string",
                "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
              }
            }
          },
          "stringify" : {
            "type" : "string",
            "description" : "Set the data type for all values to string."
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "filenameColumn" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "housekeepingMode" : {
            "description" : "Optional definition of a housekeeping mode applied after every write. E.g. it can be used to cleanup, archive and compact partitions.\nSee HousekeepingMode for available implementations. Default is None.\n*",
            "oneOf" : [ {
              "$ref" : "#/definitions/HousekeepingMode/PartitionArchiveCompactionMode"
            }, {
              "$ref" : "#/definitions/HousekeepingMode/PartitionRetentionMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "KafkaTopicDataObject" : {
        "type" : "object",
        "description" : "[[DataObject]]of type KafkaTopic.\nProvides details to an action to read from Kafka Topics using either\n[[org.apache.spark.sql.DataFrameReader]]or[[org.apache.spark.sql.streaming.DataStreamReader]]",
        "additionalProperties" : false,
        "required" : [ "id", "topicName", "connectionId" ],
        "title" : "KafkaTopicDataObject",
        "properties" : {
          "type" : {
            "const" : "KafkaTopicDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "topicName" : {
            "type" : "string",
            "description" : "The name of the topic to read"
          },
          "connectionId" : {
            "type" : "string",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "keyType" : {
            "type" : "string",
            "enum" : [ "String ", "Binary ", "AvroSchemaRegistry " ],
            "description" : "Optional type the key column should be converted to. If none is given it will remain a bytearray / binary."
          },
          "valueType" : {
            "type" : "string",
            "enum" : [ "String ", "Binary ", "AvroSchemaRegistry " ],
            "description" : "Optional type the value column should be converted to. If none is given it will remain a bytearray / binary."
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "An optional, minimal schema that this DataObject must have to pass schema validation on reading and writing."
          },
          "selectCols" : {
            "type" : "array",
            "description" : "Columns to be selected when reading the DataFrame. Available columns are key, value, topic,\npartition, offset, timestamp, timestampType. If key/valueType is AvroSchemaRegistry the key/value column are\nconvert to a complex type according to the avro schema. To expand it select \\\"value.*\\\".\nDefault is to select key and value.",
            "items" : {
              "type" : "string"
            }
          },
          "datePartitionCol" : {
            "type" : "object",
            "description" : "Definition of date partition column to extract formatted time into column.",
            "additionalProperties" : false,
            "required" : [ "colName" ],
            "title" : "DatePartitionColumnDef",
            "properties" : {
              "colName" : {
                "type" : "string",
                "description" : "date partition column name to extract time into column on batch read"
              },
              "timeFormat" : {
                "type" : "string",
                "description" : "time format for timestamp in date partition column, definition according to java DateTimeFormatter. Default is \\\"yyyyMMdd\\\"."
              },
              "timeUnit" : {
                "type" : "string",
                "description" : "time unit for timestamp in date partition column, definition according to java ChronoUnit. Default is \\\"days\\\"."
              },
              "timeZone" : {
                "type" : "string",
                "description" : "time zone used for date logic. If not specified, java system default is used."
              },
              "includeCurrentPartition" : {
                "type" : "string",
                "description" : "If the current partition should be included. Default is to list only completed partitions.\nAttention: including the current partition might result in data loss if there is more data arriving.\nBut it might be useful to export all data before a scheduled maintenance."
              }
            }
          },
          "batchReadConsecutivePartitionsAsRanges" : {
            "type" : "string",
            "description" : "Set to true if consecutive partitions should be combined as one range of offsets when batch reading from topic. This results in less tasks but can be a performance problem when reading many partitions. (default=false)"
          },
          "batchReadMaxOffsetsPerTask" : {
            "type" : "integer",
            "description" : "Set number of offsets per Spark task when batch reading from topic."
          },
          "options" : {
            "type" : "object",
            "description" : "Options for the Kafka stream reader (see https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html).\nThese options override connection.options.",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "PKViolatorsDataObject" : {
        "type" : "object",
        "description" : "Checks for Primary Key violations for all[[DataObject]]s with Primary Keys defined that are registered in the current[[InstanceRegistry]].\nReturns the list of Primary Key violations as a[[DataFrame]].\nAlternatively, it can check for Primary Key violations of all[[DataObject]]s defined in config files. For this, the\nconfiguration \\\"config\\\" has to be set to the location of the config.\nExample:\n{{{\n```dataObjects = {\n...\nprimarykey-violations {\ntype = PKViolatorsDataObject\nconfig = path/to/myconfiguration.conf\n}\n...\n}\n}}}\nSEE: Refer to[[ConfigLoader.loadConfigFromFilesystem()]] for details about the configuration loading.",
        "additionalProperties" : false,
        "required" : [ "id" ],
        "title" : "PKViolatorsDataObject",
        "properties" : {
          "type" : {
            "const" : "PKViolatorsDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "A unique identifier for this instance."
          },
          "config" : {
            "type" : "string",
            "description" : ": The config value can point to a configuration file or a directory containing configuration files."
          },
          "flattenOutput" : {
            "type" : "string",
            "description" : ": if true, key and data column are converted from type map<k,v> to string (default).\n*"
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "ParquetFileDataObject" : {
        "type" : "object",
        "description" : "A[[io.smartdatalake.workflow.dataobject.DataObject]]backed by an Apache Hive data source.\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\nwork on Parquet formatted files.\nReading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\nand[[org.apache.spark.sql.DataFrameWriter]] respectively.\nSEE: [[org.apache.spark.sql.DataFrameReader]]\nSEE: [[org.apache.spark.sql.DataFrameWriter]]",
        "additionalProperties" : false,
        "required" : [ "id", "path" ],
        "title" : "ParquetFileDataObject",
        "properties" : {
          "type" : {
            "const" : "ParquetFileDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "unique name of this data object"
          },
          "path" : {
            "type" : "string",
            "description" : "Hadoop directory where this data object reads/writes it\\'s files.\nIf it doesn\\'t contain scheme and authority, the connections pathPrefix is applied. If pathPrefix is not\ndefined or doesn\\'t define scheme and authority, default schema and authority is applied.\nOptionally defined partitions are appended with hadoop standard partition layout to this path.\nOnly files ending with *.parquet* are considered as data for this DataObject."
          },
          "partitions" : {
            "type" : "array",
            "description" : "partition columns for this data object",
            "items" : {
              "type" : "string"
            }
          },
          "parquetOptions" : {
            "type" : "object",
            "description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and\n[[org.apache.spark.sql.DataFrameWriter]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "schema" : {
            "type" : "string",
            "description" : "An optional schema for the spark data frame to be validated on read and write. Note: Existing Parquet files\ncontain a source schema. Therefore, this schema is ignored when reading from existing Parquet files.\nAs this corresponds to the schema on write, it must not include the optional filenameColumn on read."
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "spark[[SaveMode]] to use when writing files, default is \\\"overwrite\\\""
          },
          "sparkRepartition" : {
            "type" : "object",
            "description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
            "additionalProperties" : false,
            "required" : [ "numberOfTasksPerPartition" ],
            "title" : "SparkRepartitionDef",
            "properties" : {
              "numberOfTasksPerPartition" : {
                "type" : "integer",
                "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\nThis controls how many files are created in each Hadoop partition."
              },
              "keyCols" : {
                "type" : "array",
                "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
                "items" : {
                  "type" : "string"
                }
              },
              "sortCols" : {
                "type" : "array",
                "description" : "Optional columns to sort records inside files created.",
                "items" : {
                  "type" : "string"
                }
              },
              "filename" : {
                "type" : "string",
                "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
              }
            }
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "optional id of[[io.smartdatalake.workflow.connection.HadoopFileConnection]]"
          },
          "filenameColumn" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "housekeepingMode" : {
            "description" : "Optional definition of a housekeeping mode applied after every write. E.g. it can be used to cleanup, archive and compact partitions.\nSee HousekeepingMode for available implementations. Default is None.",
            "oneOf" : [ {
              "$ref" : "#/definitions/HousekeepingMode/PartitionArchiveCompactionMode"
            }, {
              "$ref" : "#/definitions/HousekeepingMode/PartitionRetentionMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "RawFileDataObject" : {
        "type" : "object",
        "description" : "DataObject of type raw for files with unknown content.\nProvides details to an Action to access raw files.\nBy specifying format you can custom Spark data formats",
        "additionalProperties" : false,
        "required" : [ "id", "path" ],
        "title" : "RawFileDataObject",
        "properties" : {
          "type" : {
            "const" : "RawFileDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "path" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "customFormat" : {
            "type" : "string",
            "description" : "Custom Spark data source format, e.g. binaryFile or text. Only needed if you want to read/write this DataObject with Spark."
          },
          "options" : {
            "type" : "object",
            "description" : "Options for custom Spark data source format. Only of use if you want to read/write this DataObject with Spark.",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "fileName" : {
            "type" : "string",
            "description" : "Definition of fileName. This is concatenated with path and partition layout to search for files. Default is an asterix to match everything."
          },
          "partitions" : {
            "type" : "array",
            "description" : "The Spark-Format provider to be used",
            "items" : {
              "type" : "string"
            }
          },
          "schema" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "Overwrite or Append new data."
          },
          "sparkRepartition" : {
            "type" : "object",
            "description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
            "additionalProperties" : false,
            "required" : [ "numberOfTasksPerPartition" ],
            "title" : "SparkRepartitionDef",
            "properties" : {
              "numberOfTasksPerPartition" : {
                "type" : "integer",
                "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\nThis controls how many files are created in each Hadoop partition."
              },
              "keyCols" : {
                "type" : "array",
                "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
                "items" : {
                  "type" : "string"
                }
              },
              "sortCols" : {
                "type" : "array",
                "description" : "Optional columns to sort records inside files created.",
                "items" : {
                  "type" : "string"
                }
              },
              "filename" : {
                "type" : "string",
                "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
              }
            }
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "filenameColumn" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "housekeepingMode" : {
            "description" : "Optional definition of a housekeeping mode applied after every write. E.g. it can be used to cleanup, archive and compact partitions.\nSee HousekeepingMode for available implementations. Default is None.",
            "oneOf" : [ {
              "$ref" : "#/definitions/HousekeepingMode/PartitionArchiveCompactionMode"
            }, {
              "$ref" : "#/definitions/HousekeepingMode/PartitionRetentionMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "RelaxedCsvFileDataObject" : {
        "type" : "object",
        "description" : "A[[DataObject]] which allows for more flexible CSV parsing.\nThe standard CsvFileDataObject doesnt support reading multiple CSV-Files with different column order, missing columns\nor additional columns.\nRelaxCsvFileDataObject works more like reading JSON-Files. You need to define a schema, then it tries to read every file\nwith that schema independently of the column order, adding missing columns and removing superfluous ones.\nCSV files are read by Spark as whole text files and then parsed manually with Sparks CSV parser class. You can therefore use the\nnormal CSV options of spark, but some properties are fixed, e.g. header=true, inferSchema=false, enforceSchema (ignored).\nNOTE: This data object sets the following default values for`csvOptions`: delimiter = \\\",\\\", quote = null\nAll other`csvOption` default to the values defined by Apache Spark.\nSEE: [[org.apache.spark.sql.DataFrameReader]]\nSEE: [[org.apache.spark.sql.DataFrameWriter]] \nIf mode is permissive you can retrieve the corrupt input record by adding <options.columnNameOfCorruptRecord> as field to the schema.\nRelaxCsvFileDataObject also supports getting an error msg by adding \\\"<options.columnNameOfCorruptRecord>_msg\\\" as field to the schema.",
        "additionalProperties" : false,
        "required" : [ "id", "path" ],
        "title" : "RelaxedCsvFileDataObject",
        "properties" : {
          "type" : {
            "const" : "RelaxedCsvFileDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "path" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "csvOptions" : {
            "type" : "object",
            "description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and[[org.apache.spark.sql.DataFrameWriter]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "partitions" : {
            "type" : "array",
            "description" : "The Spark-Format provider to be used",
            "items" : {
              "type" : "string"
            }
          },
          "schema" : {
            "type" : "string",
            "description" : "The data object schema."
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "dateColumnType" : {
            "type" : "string",
            "enum" : [ "Default ", "String ", "Date " ],
            "description" : "Specifies the string format used for writing date typed data."
          },
          "treatMissingColumnsAsCorrupt" : {
            "type" : "string",
            "description" : "If set to true records from files with missing columns in its header are treated as corrupt (default=false).\nCorrupt records are handled according to options.mode (default=permissive)."
          },
          "treatSuperfluousColumnsAsCorrupt" : {
            "type" : "string",
            "description" : "If set to true records from files with superfluous columns in its header are treated as corrupt (default=false).\nCorrupt records are handled according to options.mode (default=permissive)."
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "The Spark-Format provider to be used"
          },
          "sparkRepartition" : {
            "type" : "object",
            "description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
            "additionalProperties" : false,
            "required" : [ "numberOfTasksPerPartition" ],
            "title" : "SparkRepartitionDef",
            "properties" : {
              "numberOfTasksPerPartition" : {
                "type" : "integer",
                "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\nThis controls how many files are created in each Hadoop partition."
              },
              "keyCols" : {
                "type" : "array",
                "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
                "items" : {
                  "type" : "string"
                }
              },
              "sortCols" : {
                "type" : "array",
                "description" : "Optional columns to sort records inside files created.",
                "items" : {
                  "type" : "string"
                }
              },
              "filename" : {
                "type" : "string",
                "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
              }
            }
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "filenameColumn" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "SFtpFileRefDataObject" : {
        "type" : "object",
        "description" : "Connects to SFtp files\nNeeds java library \\\"com.hieronymus % sshj % 0.21.1\\\"\nThe following authentication mechanisms are supported\n-> public/private-key: private key must be saved in ~/.ssh, public key must be registered on server.\n-> user/pwd authentication: user and password is taken from two variables set as parameters.\nThese variables could come from clear text (CLEAR), a file (FILE) or an environment variable (ENV)",
        "additionalProperties" : false,
        "required" : [ "id", "path", "connectionId" ],
        "title" : "SFtpFileRefDataObject",
        "properties" : {
          "type" : {
            "const" : "SFtpFileRefDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout"
          },
          "path" : {
            "type" : "string",
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout"
          },
          "connectionId" : {
            "type" : "string",
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout"
          },
          "partitions" : {
            "type" : "array",
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout",
            "items" : {
              "type" : "string"
            }
          },
          "partitionLayout" : {
            "type" : "string",
            "description" : "partition layout defines how partition values can be extracted from the path.\nUse \\\"%<colname>%\\\" as token to extract the value for a partition column.\nWith \\\"%<colname:regex>%\\\" a regex can be given to limit search. This is especially useful\nif there is no char to delimit the last token from the rest of the path or also between\ntwo tokens."
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "Overwrite or Append new data."
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "SnowflakeTableDataObject" : {
        "type" : "object",
        "description" : "[[DataObject]] of type SnowflakeTableDataObject.\nProvides details to access Snowflake tables via an action\nCan be used both for interacting with Snowflake through Spark with JDBC,\nas well as for actions written in the Snowpark API that run directly on Snowflake",
        "additionalProperties" : false,
        "required" : [ "id", "table", "connectionId" ],
        "title" : "SnowflakeTableDataObject",
        "properties" : {
          "type" : {
            "const" : "SnowflakeTableDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "unique name of this data object"
          },
          "table" : {
            "$ref" : "#/definitions/Others/Table"
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "An optional, minimal schema that a[[DataObject]]schema must have to pass schema validation.\nThe schema validation semantics are:\n- Schema A is valid in respect to a minimal schema B when B is a subset of A. This means: the whole column set of B is contained in the column set of A.\n- A column of B is contained in A when A contains a column with equal name and data type.\n- Column order is ignored.\n- Column nullability is ignored.\n- Duplicate columns in terms of name and data type are eliminated (set semantics).\nNote: This is mainly used by the functionality defined in[[CanCreateDataFrame]]and[[CanWriteDataFrame]], that is,\nwhen reading or writing Spark data frames from/to the underlying data container.\n[[io.smartdatalake.workflow.action.Action]]s that work with files ignore the`schemaMin` attribute\nif it is defined.\nAdditionally schemaMin can be used to define the schema used if there is no data or table doesn\\'t yet exist."
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "spark[[SDLSaveMode]] to use when writing files, default is \\\"overwrite\\\""
          },
          "connectionId" : {
            "type" : "string",
            "description" : "The SnowflakeTableConnection to use for the table"
          },
          "comment" : {
            "type" : "string",
            "description" : "An optional comment to add to the table after writing a DataFrame to it"
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "SplunkDataObject" : {
        "type" : "object",
        "description" : "[[DataObject]] of type Splunk.\nProvides details to an action to access Splunk logs.",
        "additionalProperties" : false,
        "required" : [ "id", "params", "connectionId" ],
        "title" : "SplunkDataObject",
        "properties" : {
          "type" : {
            "const" : "SplunkDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "A unique identifier for this instance."
          },
          "params" : {
            "type" : "object",
            "additionalProperties" : false,
            "required" : [ "query", "queryFrom", "queryTo" ],
            "title" : "SplunkParams",
            "properties" : {
              "query" : {
                "type" : "string"
              },
              "queryFrom" : {
                "type" : "string",
                "existingJavaType" : "java.time.LocalDateTime"
              },
              "queryTo" : {
                "type" : "string",
                "existingJavaType" : "java.time.LocalDateTime"
              },
              "queryTimeInterval" : {
                "type" : "string",
                "existingJavaType" : "java.time.Duration"
              },
              "columnNames" : {
                "type" : "array",
                "items" : {
                  "type" : "string"
                }
              },
              "parallelRequests" : {
                "type" : "integer"
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "A unique identifier for this instance."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "TickTockHiveTableDataObject" : {
        "type" : "object",
        "additionalProperties" : false,
        "required" : [ "id", "table" ],
        "title" : "TickTockHiveTableDataObject",
        "properties" : {
          "type" : {
            "const" : "TickTockHiveTableDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "Definition of partition columns"
          },
          "path" : {
            "type" : "string",
            "description" : "Definition of partition columns"
          },
          "partitions" : {
            "type" : "array",
            "description" : "Definition of partition columns",
            "items" : {
              "type" : "string"
            }
          },
          "analyzeTableAfterWrite" : {
            "type" : "string",
            "description" : "Definition of partition columns"
          },
          "dateColumnType" : {
            "type" : "string",
            "enum" : [ "Default ", "String ", "Date " ],
            "description" : "Definition of partition columns"
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "Definition of partition columns"
          },
          "table" : {
            "$ref" : "#/definitions/Others/Table"
          },
          "numInitialHdfsPartitions" : {
            "type" : "integer",
            "description" : "Definition of partition columns"
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "Definition of partition columns"
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "Definition of partition columns"
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Definition of partition columns"
          },
          "housekeepingMode" : {
            "description" : "Definition of partition columns",
            "oneOf" : [ {
              "$ref" : "#/definitions/HousekeepingMode/PartitionArchiveCompactionMode"
            }, {
              "$ref" : "#/definitions/HousekeepingMode/PartitionRetentionMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "WebserviceFileDataObject" : {
        "type" : "object",
        "description" : "[[DataObject]] to call webservice and return response as InputStream\nThis is implemented as FileRefDataObject because the response is treated as some file content.\nFileRefDataObjects support partitioned data. For a WebserviceFileDataObject partitions are mapped as query parameters to create query string.\nAll possible query parameter values must be given in configuration.",
        "additionalProperties" : false,
        "required" : [ "id", "url" ],
        "title" : "WebserviceFileDataObject",
        "properties" : {
          "type" : {
            "const" : "WebserviceFileDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout"
          },
          "url" : {
            "type" : "string",
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout"
          },
          "additionalHeaders" : {
            "type" : "object",
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "timeouts" : {
            "type" : "object",
            "additionalProperties" : false,
            "required" : [ "connectionTimeoutMs", "readTimeoutMs" ],
            "title" : "HttpTimeoutConfig",
            "properties" : {
              "connectionTimeoutMs" : {
                "type" : "integer"
              },
              "readTimeoutMs" : {
                "type" : "integer"
              }
            }
          },
          "readTimeoutMs" : {
            "type" : "integer",
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout"
          },
          "authMode" : {
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout",
            "oneOf" : [ {
              "$ref" : "#/definitions/AuthMode/AuthHeaderMode"
            }, {
              "$ref" : "#/definitions/AuthMode/BasicAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/CustomHttpAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/KeycloakClientSecretAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/PublicKeyAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SASLSCRAMAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SSLCertsAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/TokenAuthMode"
            } ]
          },
          "mimeType" : {
            "type" : "string",
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout"
          },
          "writeMethod" : {
            "type" : "string",
            "enum" : [ "Delete ", "Put ", "Post ", "Get " ],
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout"
          },
          "proxy" : {
            "type" : "object",
            "additionalProperties" : false,
            "required" : [ "host", "port" ],
            "title" : "HttpProxyConfig",
            "properties" : {
              "host" : {
                "type" : "string"
              },
              "port" : {
                "type" : "integer"
              }
            }
          },
          "followRedirects" : {
            "type" : "string",
            "description" : "Definition of partition layout\nuse %<partitionColName>% as placeholder and * for globs in layout\nNote: if you have globs in partition layout, it\\'s not possible to write files to this DataObject\nNote: if this is a directory, you must add a final backslash to the partition layout"
          },
          "partitionDefs" : {
            "type" : "array",
            "description" : "list of partitions with list of possible values for every entry",
            "items" : {
              "type" : "object",
              "additionalProperties" : false,
              "required" : [ "name", "values" ],
              "title" : "WebservicePartitionDefinition",
              "properties" : {
                "name" : {
                  "type" : "string"
                },
                "values" : {
                  "type" : "array",
                  "items" : {
                    "type" : "string"
                  }
                }
              }
            }
          },
          "partitionLayout" : {
            "type" : "string",
            "description" : "definition of partitions in query string. Use %<partitionColName>% as placeholder for partition column value in layout."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      },
      "XmlFileDataObject" : {
        "type" : "object",
        "description" : "A[[io.smartdatalake.workflow.dataobject.DataObject]]backed by an XML data source.\nIt manages read and write access and configurations required for[[io.smartdatalake.workflow.action.Action]]s to\nwork on XML formatted files.\nReading and writing details are delegated to Apache Spark[[org.apache.spark.sql.DataFrameReader]]\nand[[org.apache.spark.sql.DataFrameWriter]]respectively. The reader and writer implementations are provided by\nthe[[https://github.com/databricks/spark-xml databricks spark-xml]] project.\nNote that writing XML-file partitioned is not supported by spark-xml.\nSEE: [[org.apache.spark.sql.DataFrameReader]]\nSEE: [[org.apache.spark.sql.DataFrameWriter]]",
        "additionalProperties" : false,
        "required" : [ "id", "path" ],
        "title" : "XmlFileDataObject",
        "properties" : {
          "type" : {
            "const" : "XmlFileDataObject"
          },
          "id" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "path" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "rowTag" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "xmlOptions" : {
            "type" : "object",
            "description" : "Settings for the underlying[[org.apache.spark.sql.DataFrameReader]]and[[org.apache.spark.sql.DataFrameWriter]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "partitions" : {
            "type" : "array",
            "description" : "The Spark-Format provider to be used",
            "items" : {
              "type" : "string"
            }
          },
          "schema" : {
            "type" : "string",
            "description" : "An optional data object schema. If defined, any automatic schema inference is avoided. As this corresponds to the schema on write, it must not include the optional filenameColumn on read."
          },
          "schemaMin" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ],
            "description" : "The Spark-Format provider to be used"
          },
          "sparkRepartition" : {
            "type" : "object",
            "description" : "This controls repartitioning of the DataFrame before writing with Spark to Hadoop.\nWhen writing multiple partitions of a partitioned DataObject, the number of spark tasks created is equal to numberOfTasksPerPartition\nmultiplied with the number of partitions to write. To spread the records of a partition only over numberOfTasksPerPartition spark tasks,\nkeyCols must be given which are used to derive a task number inside the partition (hashvalue(keyCols) modulo numberOfTasksPerPartition).\nWhen writing to an unpartitioned DataObject or only one partition of a partitioned DataObject, the number of spark tasks created is equal\nto numberOfTasksPerPartition. Optional keyCols can be used to keep corresponding records together in the same task/file.",
            "additionalProperties" : false,
            "required" : [ "numberOfTasksPerPartition" ],
            "title" : "SparkRepartitionDef",
            "properties" : {
              "numberOfTasksPerPartition" : {
                "type" : "integer",
                "description" : "Number of Spark tasks to create per partition before writing to DataObject by repartitioning the DataFrame.\nThis controls how many files are created in each Hadoop partition."
              },
              "keyCols" : {
                "type" : "array",
                "description" : "Optional key columns to distribute records over Spark tasks inside a Hadoop partition.\nIf DataObject has Hadoop partitions defined, keyCols must be defined.",
                "items" : {
                  "type" : "string"
                }
              },
              "sortCols" : {
                "type" : "array",
                "description" : "Optional columns to sort records inside files created.",
                "items" : {
                  "type" : "string"
                }
              },
              "filename" : {
                "type" : "string",
                "description" : "Option filename to rename target file(s). If numberOfTasksPerPartition is greater than 1,\nmultiple files can exist in a directory and a number is inserted into the filename after the first \\'.\\'.\nExample: filename=data.csv -> files created are data.1.csv, data.2.csv, ..."
              }
            }
          },
          "flatten" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "acl" : {
            "type" : "object",
            "description" : "Describes a complete ACL Specification (basic owner/group/other permissions AND extended ACLS)\nto be applied to a Data Object on writing",
            "additionalProperties" : false,
            "required" : [ "permission", "acls" ],
            "title" : "AclDef",
            "properties" : {
              "permission" : {
                "type" : "string",
                "description" : ": File system permission string in symbolic notation form (e.g. rwxr-xr-x)"
              },
              "acls" : {
                "type" : "array",
                "description" : ": a sequence of[[AclElement]] s",
                "items" : {
                  "type" : "object",
                  "description" : "Describes a single extended ACL to be applied to a Data Object\nin addition to the basic file system permissions",
                  "additionalProperties" : false,
                  "required" : [ "aclType", "name", "permission" ],
                  "title" : "AclElement",
                  "properties" : {
                    "aclType" : {
                      "type" : "string",
                      "description" : ": type of ACL to be added \\\"group\\\", \\\"user\\\""
                    },
                    "name" : {
                      "type" : "string",
                      "description" : ": the name of the user/group for which an ACL definition is being added"
                    },
                    "permission" : {
                      "type" : "string",
                      "description" : ": the permission (rwx syntax) to be granted"
                    }
                  }
                }
              }
            }
          },
          "connectionId" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "filenameColumn" : {
            "type" : "string",
            "description" : "The Spark-Format provider to be used"
          },
          "expectedPartitionsCondition" : {
            "type" : "string",
            "description" : "Optional definition of partitions expected to exist.\nDefine a Spark SQL expression that is evaluated against a[[PartitionValues]] instance and returns true or false\nDefault is to expect all partitions to exist."
          },
          "housekeepingMode" : {
            "description" : "Optional definition of a housekeeping mode applied after every write. E.g. it can be used to cleanup, archive and compact partitions.\nSee HousekeepingMode for available implementations. Default is None.\n*",
            "oneOf" : [ {
              "$ref" : "#/definitions/HousekeepingMode/PartitionArchiveCompactionMode"
            }, {
              "$ref" : "#/definitions/HousekeepingMode/PartitionRetentionMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/DataObjectMetadata"
          }
        }
      }
    },
    "SaveModeOptions" : {
      "SaveModeGenericOptions" : {
        "type" : "object",
        "description" : "This class can be used to override save mode without further special parameters.",
        "additionalProperties" : false,
        "required" : [ "saveMode" ],
        "title" : "SaveModeGenericOptions",
        "properties" : {
          "type" : {
            "const" : "SaveModeGenericOptions"
          },
          "saveMode" : {
            "type" : "string",
            "enum" : [ "Merge ", "OverwriteOptimized ", "OverwritePreserveDirectories ", "Ignore ", "ErrorIfExists ", "Append ", "Overwrite " ]
          }
        }
      },
      "SaveModeMergeOptions" : {
        "type" : "object",
        "description" : "Options to control detailed behaviour of SaveMode.Merge.\nIn Spark expressions use table alias \\'existing\\' to reference columns of the existing table data, and table alias \\'new\\' to reference columns of new data set.",
        "additionalProperties" : false,
        "title" : "SaveModeMergeOptions",
        "properties" : {
          "type" : {
            "const" : "SaveModeMergeOptions"
          },
          "deleteCondition" : {
            "type" : "string",
            "description" : "A condition to control if matched records are deleted. If no condition is given, *no* records are delete."
          },
          "updateCondition" : {
            "type" : "string",
            "description" : "A condition to control if matched records are updated. If no condition is given all matched records are updated (default).\nNote that delete is applied before update. Records selected for deletion are automatically excluded from the updates."
          },
          "updateColumns" : {
            "type" : "array",
            "description" : "List of column names to update in update clause. If empty all columns (except primary keys) are updated (default)",
            "items" : {
              "type" : "string"
            }
          },
          "insertCondition" : {
            "type" : "string",
            "description" : "A condition to control if unmatched records are inserted. If no condition is given all unmatched records are inserted (default)."
          },
          "insertColumnsToIgnore" : {
            "type" : "array",
            "description" : "List of column names to ignore in insert clause. If empty all columns are inserted (default).",
            "items" : {
              "type" : "string"
            }
          },
          "additionalMergePredicate" : {
            "type" : "string",
            "description" : "To optimize performance for SDLSaveMode.Merge it might be interesting to limit the records read from the existing table data, e.g. merge operation might use only the last 7 days."
          }
        }
      }
    },
    "ParsableScriptDef" : {
      "CmdScript" : {
        "type" : "object",
        "description" : "Execute a command on the command line and get its std output\nCommand can be different for windows and linux operating systems, but it must be defined for at least one of them.\nIf return value is not zero an exception is thrown.\nNote about internal implementation: on execution value of parameter map entries where key starts with\n- \\'param\\' will be added as parameter after the docker run command, sorted by key.\nThis allows to customize execution behaviour through Actions or DataObjects using CmdScript.",
        "additionalProperties" : false,
        "title" : "CmdScript",
        "properties" : {
          "type" : {
            "const" : "CmdScript"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "winCmd" : {
            "type" : "string",
            "description" : "Cmd to execute on windows operating systems - note that it is executed with \\\"cmd /C\\\" prefixed"
          },
          "linuxCmd" : {
            "type" : "string",
            "description" : "Cmd to execute on linux operating systems - note that it is executed with \\\"sh -c\\\" prefixed."
          }
        }
      },
      "DockerRunScript" : {
        "type" : "object",
        "description" : "Run a docker image and get its std output.\nIf return value is not zero an exception is thrown.\nNote about internal implementation: on execution value of parameter map entries where key starts with\n- \\'runParam\\' will be added as parameter after the docker run command, sorted by their key.\n- \\'dockerParam\\' will be added as parameter for the docker command, e.g. before the image name in the docker run command, sorted by their key.\nThis allows to customize execution behaviour through Actions or DataObjects using CmdScript.",
        "additionalProperties" : false,
        "required" : [ "image" ],
        "title" : "DockerRunScript",
        "properties" : {
          "type" : {
            "const" : "DockerRunScript"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "image" : {
            "type" : "string",
            "description" : "Docker image to run"
          },
          "winDockerCmd" : {
            "type" : "string",
            "description" : "Cmd to execute docker on windows operating systems. Default is \\'docker\\'."
          },
          "linuxDockerCmd" : {
            "type" : "string",
            "description" : "Cmd to execute docker on linux operating systems. Default is \\'docker\\'."
          },
          "localDataDirToMount" : {
            "type" : "string",
            "description" : "Optional directory that will be mounted as /mnt/data in the container. This is needed if your container wants to access files available in your local filesystem."
          }
        }
      }
    },
    "ParsableDfTransformer" : {
      "AdditionalColumnsTransformer" : {
        "type" : "object",
        "description" : "Add additional columns to the DataFrame by extracting information from the context.",
        "additionalProperties" : false,
        "required" : [ "additionalColumns" ],
        "title" : "AdditionalColumnsTransformer",
        "properties" : {
          "type" : {
            "const" : "AdditionalColumnsTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "additionalColumns" : {
            "type" : "object",
            "description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      },
      "BlacklistTransformer" : {
        "type" : "object",
        "description" : "Apply a column blacklist to a DataFrame.",
        "additionalProperties" : false,
        "required" : [ "columnBlacklist" ],
        "title" : "BlacklistTransformer",
        "properties" : {
          "type" : {
            "const" : "BlacklistTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "columnBlacklist" : {
            "type" : "array",
            "description" : "List of columns to exclude from DataFrame",
            "items" : {
              "type" : "string"
            }
          }
        }
      },
      "DataValidationTransformer" : {
        "type" : "object",
        "description" : "Apply validation rules to a DataFrame and collect potential violation error messages in a new column.",
        "additionalProperties" : false,
        "required" : [ "rules" ],
        "title" : "DataValidationTransformer",
        "properties" : {
          "type" : {
            "const" : "DataValidationTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "rules" : {
            "type" : "array",
            "description" : "list of validation rules to apply to the DataFrame",
            "items" : {
              "$ref" : "#/definitions/ValidationRule/RowLevelValidationRule"
            }
          },
          "errorsColumn" : {
            "type" : "string",
            "description" : "Optional column name for the list of error messages. Default is \\\"errors\\\"."
          }
        }
      },
      "FilterTransformer" : {
        "type" : "object",
        "description" : "Apply a filter condition to a DataFrame.",
        "additionalProperties" : false,
        "required" : [ "filterClause" ],
        "title" : "FilterTransformer",
        "properties" : {
          "type" : {
            "const" : "FilterTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "filterClause" : {
            "type" : "string",
            "description" : "Spark SQL expression to filter the DataFrame"
          }
        }
      },
      "PythonCodeDfTransformer" : {
        "type" : "object",
        "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Python/PySpark code.\nNote that this transformer needs a Python and PySpark environment installed.\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\nOther variables available are\n-`inputDf`: Input DataFrame\n-`options`: Transformation options as Map[String,String]\n-`dataObjectId`: Id of input dataObject as String\nOutput DataFrame must be set with`setOutputDf(df)` .",
        "additionalProperties" : false,
        "title" : "PythonCodeDfTransformer",
        "properties" : {
          "type" : {
            "const" : "PythonCodeDfTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "code" : {
            "type" : "string",
            "description" : "Optional python code to user for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
          },
          "file" : {
            "type" : "string",
            "description" : "Optional file with python code to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
          },
          "options" : {
            "type" : "object",
            "description" : "Options to pass to the transformation",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "runtimeOptions" : {
            "type" : "object",
            "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      },
      "RepartitionTransformer" : {
        "type" : "object",
        "description" : "Repartition DataFrame\nFor detailled description about repartitioning DataFrames see also[[SparkRepartitionDef]]",
        "additionalProperties" : false,
        "required" : [ "numberOfTasksPerPartition" ],
        "title" : "RepartitionTransformer",
        "properties" : {
          "type" : {
            "const" : "RepartitionTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "numberOfTasksPerPartition" : {
            "type" : "integer",
            "description" : "Number of Spark tasks to create per partition value by repartitioning the DataFrame."
          },
          "keyCols" : {
            "type" : "array",
            "description" : "Optional key columns to distribute records over Spark tasks inside a partition value.",
            "items" : {
              "type" : "string"
            }
          }
        }
      },
      "SQLDfTransformer" : {
        "type" : "object",
        "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as SQL code.\nThe input data is available as temporary view in SQL. As name for the temporary view the input DataObjectId is used\n(special characters are replaces by underscores). A special token \\'%{inputViewName}\\' will be replaced with the name of\nthe temporary view at runtime.",
        "additionalProperties" : false,
        "required" : [ "code" ],
        "title" : "SQLDfTransformer",
        "properties" : {
          "type" : {
            "const" : "SQLDfTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "code" : {
            "type" : "string",
            "description" : "SQL code for transformation.\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\nExample: \\\"select * from test where run = %{runId}\\\"\nA special token %{inputViewName} can be used to insert the temporary view name."
          },
          "options" : {
            "type" : "object",
            "description" : "Options to pass to the transformation",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "runtimeOptions" : {
            "type" : "object",
            "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      },
      "ScalaClassDfTransformer" : {
        "type" : "object",
        "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Java/Scala Class.\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\nDataFrame. The Java/Scala class has to implement interface[[CustomDfTransformer]] .",
        "additionalProperties" : false,
        "required" : [ "className" ],
        "title" : "ScalaClassDfTransformer",
        "properties" : {
          "type" : {
            "const" : "ScalaClassDfTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "className" : {
            "type" : "string",
            "description" : "class name implementing trait[[CustomDfTransformer]]"
          },
          "options" : {
            "type" : "object",
            "description" : "Options to pass to the transformation",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "runtimeOptions" : {
            "type" : "object",
            "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      },
      "ScalaCodeDfTransformer" : {
        "type" : "object",
        "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\nDefine a transform function which receives a DataObjectId, a DataFrame and a map of options and has to return a\nDataFrame. The scala code has to implement a function of type[[fnTransformType]] .",
        "additionalProperties" : false,
        "title" : "ScalaCodeDfTransformer",
        "properties" : {
          "type" : {
            "const" : "ScalaCodeDfTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "code" : {
            "type" : "string",
            "description" : "Scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
          },
          "file" : {
            "type" : "string",
            "description" : "File where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
          },
          "options" : {
            "type" : "object",
            "description" : "Options to pass to the transformation",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "runtimeOptions" : {
            "type" : "object",
            "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      },
      "ScalaNotebookDfTransformer" : {
        "type" : "object",
        "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1) as Scala code which is compiled at runtime.\nThe code is loaded from a Notebook. It should define a transform function with a configurable name, which receives a DataObjectId, a DataFrame\nand a map of options and has to return a DataFrame, see also ([[fnTransformType]] ).\nNotebook-cells starting with \\\"//!IGNORE\\\" will be ignored.",
        "additionalProperties" : false,
        "required" : [ "url", "functionName" ],
        "title" : "ScalaNotebookDfTransformer",
        "properties" : {
          "type" : {
            "const" : "ScalaNotebookDfTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "url" : {
            "type" : "string",
            "description" : "Url to download notebook in IPYNB-format, which defines transformation."
          },
          "functionName" : {
            "type" : "string",
            "description" : "The notebook needs to contain a Scala-function with this name and type[[fnTransformType]] ."
          },
          "authMode" : {
            "description" : "optional authentication information for webservice, e.g. BasicAuthMode for user/pw authentication",
            "oneOf" : [ {
              "$ref" : "#/definitions/AuthMode/AuthHeaderMode"
            }, {
              "$ref" : "#/definitions/AuthMode/BasicAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/CustomHttpAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/KeycloakClientSecretAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/PublicKeyAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SASLSCRAMAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/SSLCertsAuthMode"
            }, {
              "$ref" : "#/definitions/AuthMode/TokenAuthMode"
            } ]
          },
          "options" : {
            "type" : "object",
            "description" : "Options to pass to the transformation",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "runtimeOptions" : {
            "type" : "object",
            "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      },
      "StandardizeDatatypesTransformer" : {
        "type" : "object",
        "description" : "Standardize datatypes of a DataFrame.\nCurrent implementation converts all decimal datatypes to a corresponding integral or float datatype",
        "additionalProperties" : false,
        "title" : "StandardizeDatatypesTransformer",
        "properties" : {
          "type" : {
            "const" : "StandardizeDatatypesTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          }
        }
      },
      "WhitelistTransformer" : {
        "type" : "object",
        "description" : "Apply a column whitelist to a DataFrame.",
        "additionalProperties" : false,
        "required" : [ "columnWhitelist" ],
        "title" : "WhitelistTransformer",
        "properties" : {
          "type" : {
            "const" : "WhitelistTransformer"
          },
          "name" : {
            "type" : "string",
            "description" : "name of the transformer"
          },
          "description" : {
            "type" : "string",
            "description" : "Optional description of the transformer"
          },
          "columnWhitelist" : {
            "type" : "array",
            "description" : "List of columns to keep from DataFrame",
            "items" : {
              "type" : "string"
            }
          }
        }
      }
    },
    "Others" : {
      "ActionMetadata" : {
        "type" : "object",
        "description" : "Additional metadata for an Action",
        "additionalProperties" : false,
        "title" : "ActionMetadata",
        "properties" : {
          "name" : {
            "type" : "string",
            "description" : "Readable name of the Action"
          },
          "description" : {
            "type" : "string",
            "description" : "Description of the content of the Action"
          },
          "feed" : {
            "type" : "string",
            "description" : "Name of the feed this Action belongs to"
          },
          "tags" : {
            "type" : "array",
            "description" : "Optional custom tags for this object",
            "items" : {
              "type" : "string"
            }
          }
        }
      },
      "ConnectionMetadata" : {
        "type" : "object",
        "description" : "Additional metadata for a Connection",
        "additionalProperties" : false,
        "title" : "ConnectionMetadata",
        "properties" : {
          "name" : {
            "type" : "string",
            "description" : "Readable name of the Connection"
          },
          "description" : {
            "type" : "string",
            "description" : "Description of the content of the Connection"
          },
          "layer" : {
            "type" : "string",
            "description" : "Name of the layer this Connection belongs to"
          },
          "subjectArea" : {
            "type" : "string",
            "description" : "Name of the subject area this Connection belongs to"
          },
          "tags" : {
            "type" : "array",
            "description" : "Optional custom tags for this object",
            "items" : {
              "type" : "string"
            }
          }
        }
      },
      "DataObjectMetadata" : {
        "type" : "object",
        "description" : "Additional metadata for a DataObject",
        "additionalProperties" : false,
        "title" : "DataObjectMetadata",
        "properties" : {
          "name" : {
            "type" : "string",
            "description" : "Readable name of the DataObject"
          },
          "description" : {
            "type" : "string",
            "description" : "Description of the content of the DataObject"
          },
          "layer" : {
            "type" : "string",
            "description" : "Name of the layer this DataObject belongs to"
          },
          "subjectArea" : {
            "type" : "string",
            "description" : "Name of the subject area this DataObject belongs to"
          },
          "tags" : {
            "type" : "array",
            "description" : "Optional custom tags for this object",
            "items" : {
              "type" : "string"
            }
          }
        }
      },
      "Table" : {
        "type" : "object",
        "description" : "Table attributes",
        "additionalProperties" : false,
        "required" : [ "name" ],
        "title" : "Table",
        "properties" : {
          "db" : {
            "type" : "string",
            "description" : "optional override of db defined by connection"
          },
          "name" : {
            "type" : "string",
            "description" : "table name"
          },
          "query" : {
            "type" : "string",
            "description" : "optional select query"
          },
          "primaryKey" : {
            "type" : "array",
            "description" : "optional sequence of primary key columns",
            "items" : {
              "type" : "string"
            }
          },
          "foreignKeys" : {
            "type" : "array",
            "description" : "optional sequence of foreign key definitions.\nThis is used as metadata for a data catalog.",
            "items" : {
              "type" : "object",
              "description" : "Foreign key definition",
              "additionalProperties" : false,
              "required" : [ "table", "columns" ],
              "title" : "ForeignKey",
              "properties" : {
                "db" : {
                  "type" : "string",
                  "description" : "target database, if not defined it is assumed to be the same as the table owning the foreign key"
                },
                "table" : {
                  "type" : "string",
                  "description" : "referenced target table name"
                },
                "columns" : {
                  "type" : "object",
                  "description" : "mapping of source column(s) to referenced target table column(s)",
                  "additionalProperties" : {
                    "type" : "string"
                  }
                },
                "name" : {
                  "type" : "string",
                  "description" : "optional name for foreign key, e.g to depict it\\'s role"
                }
              }
            }
          },
          "options" : {
            "type" : "object",
            "description" : "",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      }
    },
    "AuthMode" : {
      "AuthHeaderMode" : {
        "type" : "object",
        "description" : "Connect by custom authorization header",
        "additionalProperties" : false,
        "required" : [ "secretVariable" ],
        "title" : "AuthHeaderMode",
        "properties" : {
          "type" : {
            "const" : "AuthHeaderMode"
          },
          "headerName" : {
            "type" : "string",
            "description" : "Return additional headers to add/overwrite in http request."
          },
          "secretVariable" : {
            "type" : "string",
            "description" : "Return additional headers to add/overwrite in http request."
          }
        }
      },
      "BasicAuthMode" : {
        "type" : "object",
        "description" : "Connect by basic authentication",
        "additionalProperties" : false,
        "required" : [ "userVariable", "passwordVariable" ],
        "title" : "BasicAuthMode",
        "properties" : {
          "type" : {
            "const" : "BasicAuthMode"
          },
          "userVariable" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          },
          "passwordVariable" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          }
        }
      },
      "CustomHttpAuthMode" : {
        "type" : "object",
        "description" : "Connect with custom HTTP authentication",
        "additionalProperties" : false,
        "required" : [ "className", "options" ],
        "title" : "CustomHttpAuthMode",
        "properties" : {
          "type" : {
            "const" : "CustomHttpAuthMode"
          },
          "className" : {
            "type" : "string",
            "description" : "class name implementing trait[[CustomHttpAuthModeLogic]]"
          },
          "options" : {
            "type" : "object",
            "description" : "Options to pass to the custom auth mode logc in prepare function",
            "additionalProperties" : {
              "type" : "string"
            }
          }
        }
      },
      "KeycloakClientSecretAuthMode" : {
        "type" : "object",
        "description" : "Connect by using Keycloak to manage token and token refresh giving clientId/secret as information.\nFor HTTP Connection this is used as Bearer token in Authorization header.",
        "additionalProperties" : false,
        "required" : [ "ssoServer", "ssoRealm", "ssoGrantType", "clientIdVariable", "clientSecretVariable" ],
        "title" : "KeycloakClientSecretAuthMode",
        "properties" : {
          "type" : {
            "const" : "KeycloakClientSecretAuthMode"
          },
          "ssoServer" : {
            "type" : "string",
            "description" : "Return additional headers to add/overwrite in http request."
          },
          "ssoRealm" : {
            "type" : "string",
            "description" : "Return additional headers to add/overwrite in http request."
          },
          "ssoGrantType" : {
            "type" : "string",
            "description" : "Return additional headers to add/overwrite in http request."
          },
          "clientIdVariable" : {
            "type" : "string",
            "description" : "Return additional headers to add/overwrite in http request."
          },
          "clientSecretVariable" : {
            "type" : "string",
            "description" : "Return additional headers to add/overwrite in http request."
          }
        }
      },
      "PublicKeyAuthMode" : {
        "type" : "object",
        "description" : "Validate by user and private/public key\nPrivate key is read from .ssh",
        "additionalProperties" : false,
        "required" : [ "userVariable" ],
        "title" : "PublicKeyAuthMode",
        "properties" : {
          "type" : {
            "const" : "PublicKeyAuthMode"
          },
          "userVariable" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          }
        }
      },
      "SASLSCRAMAuthMode" : {
        "type" : "object",
        "description" : "Validate by SASL_SSL Authentication : user / password and truststore",
        "additionalProperties" : false,
        "required" : [ "username", "passwordVariable", "sslMechanism", "truststorePath", "truststorePassVariable" ],
        "title" : "SASLSCRAMAuthMode",
        "properties" : {
          "type" : {
            "const" : "SASLSCRAMAuthMode"
          },
          "username" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          },
          "passwordVariable" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          },
          "sslMechanism" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          },
          "truststorePath" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          },
          "truststoreType" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          },
          "truststorePassVariable" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          }
        }
      },
      "SSLCertsAuthMode" : {
        "type" : "object",
        "description" : "Validate by SSL Certificates : Only location an credentials. Additional attributes should be\nsupplied via options map",
        "additionalProperties" : false,
        "required" : [ "keystorePath", "keystorePassVariable", "truststorePath", "truststorePassVariable" ],
        "title" : "SSLCertsAuthMode",
        "properties" : {
          "type" : {
            "const" : "SSLCertsAuthMode"
          },
          "keystorePath" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          },
          "keystoreType" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          },
          "keystorePassVariable" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          },
          "truststorePath" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          },
          "truststoreType" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          },
          "truststorePassVariable" : {
            "type" : "string",
            "description" : "This method is called in prepare phase through the data object.\nIt allows the check configuration and setup variables."
          }
        }
      },
      "TokenAuthMode" : {
        "type" : "object",
        "description" : "Connect by token\nFor HTTP Connection this is used as Bearer token in Authorization header.",
        "additionalProperties" : false,
        "required" : [ "tokenVariable" ],
        "title" : "TokenAuthMode",
        "properties" : {
          "type" : {
            "const" : "TokenAuthMode"
          },
          "tokenVariable" : {
            "type" : "string",
            "description" : "Return additional headers to add/overwrite in http request."
          }
        }
      }
    },
    "Action" : {
      "CopyAction" : {
        "type" : "object",
        "description" : "[[Action]] to copy files (i.e. from stage to integration)",
        "additionalProperties" : false,
        "required" : [ "id", "inputId", "outputId" ],
        "title" : "CopyAction",
        "properties" : {
          "type" : {
            "const" : "CopyAction"
          },
          "id" : {
            "type" : "string",
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "inputId" : {
            "type" : "string",
            "description" : "inputs DataObject"
          },
          "outputId" : {
            "type" : "string",
            "description" : "output DataObject"
          },
          "deleteDataAfterRead" : {
            "type" : "string",
            "description" : "a flag to enable deletion of input partitions after copying."
          },
          "transformer" : {
            "type" : "object",
            "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1)\nDefine a transform function which receives a DataObjectIds, a DataFrames and a map of options and has to return a\nDataFrame, see also[[CustomDfTransformer]].\nNote about Python transformation: Environment with Python and PySpark needed.\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\nOther variables available are\n-`inputDf`: Input DataFrame\n-`options`: Transformation options as Map[String,String]\n-`dataObjectId`: Id of input dataObject as String\nOutput DataFrame must be set with`setOutputDf(df)` .",
            "additionalProperties" : false,
            "title" : "CustomDfTransformerConfig",
            "properties" : {
              "className" : {
                "type" : "string",
                "description" : "Optional class name implementing trait[[CustomDfTransformer]]"
              },
              "scalaFile" : {
                "type" : "string",
                "description" : "Optional file where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
              },
              "scalaCode" : {
                "type" : "string",
                "description" : "Optional scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
              },
              "sqlCode" : {
                "type" : "string",
                "description" : "Optional SQL code for transformation.\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\nExample: \\\"select * from test where run = %{runId}\\\""
              },
              "pythonFile" : {
                "type" : "string",
                "description" : "Optional pythonFile to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
              },
              "pythonCode" : {
                "type" : "string",
                "description" : "Optional pythonCode to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
              },
              "options" : {
                "type" : "object",
                "description" : "Options to pass to the transformation",
                "additionalProperties" : {
                  "type" : "string"
                }
              },
              "runtimeOptions" : {
                "type" : "object",
                "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          },
          "transformers" : {
            "type" : "array",
            "description" : "optional list of transformations to apply. See[[sparktransformer]] for a list of included Transformers.\nThe transformations are applied according to the lists ordering.",
            "items" : {
              "oneOf" : [ {
                "$ref" : "#/definitions/ParsableDfTransformer/AdditionalColumnsTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/BlacklistTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/DataValidationTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/FilterTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/PythonCodeDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/RepartitionTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/SQLDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/ScalaClassDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/ScalaCodeDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/ScalaNotebookDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/StandardizeDatatypesTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/WhitelistTransformer"
              } ]
            }
          },
          "columnBlacklist" : {
            "type" : "array",
            "deprecated" : true,
            "description" : "Remove all columns on blacklist from dataframe",
            "items" : {
              "type" : "string"
            }
          },
          "columnWhitelist" : {
            "type" : "array",
            "deprecated" : true,
            "description" : "Keep only columns on whitelist in dataframe",
            "items" : {
              "type" : "string"
            }
          },
          "additionalColumns" : {
            "type" : "object",
            "deprecated" : true,
            "description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "filterClause" : {
            "type" : "string",
            "deprecated" : true,
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "standardizeDatatypes" : {
            "type" : "string",
            "deprecated" : true,
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "breakDataFrameLineage" : {
            "type" : "string",
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "persist" : {
            "type" : "string",
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "executionMode" : {
            "description" : "optional execution mode for this Action",
            "oneOf" : [ {
              "$ref" : "#/definitions/ExecutionMode/CustomPartitionMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/DataObjectStateIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FailIfNoPartitionValuesMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FileIncrementalMoveMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/PartitionDiffMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/ProcessAllMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkStreamingMode"
            } ]
          },
          "executionCondition" : {
            "type" : "object",
            "description" : "Definition of a Spark SQL condition with description.\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
            "additionalProperties" : false,
            "required" : [ "expression" ],
            "title" : "Condition",
            "properties" : {
              "expression" : {
                "type" : "string",
                "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
              },
              "description" : {
                "type" : "string",
                "description" : "A textual description of the condition to be shown in error messages."
              }
            }
          },
          "metricsFailCondition" : {
            "type" : "string",
            "description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
          },
          "saveModeOptions" : {
            "description" : "override and parametrize saveMode set in output DataObject configurations when writing to DataObjects.",
            "oneOf" : [ {
              "$ref" : "#/definitions/SaveModeOptions/SaveModeGenericOptions"
            }, {
              "$ref" : "#/definitions/SaveModeOptions/SaveModeMergeOptions"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ActionMetadata"
          }
        }
      },
      "CustomFileAction" : {
        "type" : "object",
        "description" : "[[Action]] to transform files between two Hadoop Data Objects.\nThe transformation is executed in distributed mode on the Spark executors.\nA custom file transformer must be given, which reads a file from Hadoop and writes it back to Hadoop.",
        "additionalProperties" : false,
        "required" : [ "id", "inputId", "outputId", "transformer" ],
        "title" : "CustomFileAction",
        "properties" : {
          "type" : {
            "const" : "CustomFileAction"
          },
          "id" : {
            "type" : "string",
            "description" : "Input[[FileRefDataObject]] which can CanCreateInputStream"
          },
          "inputId" : {
            "type" : "string",
            "description" : "inputs DataObject"
          },
          "outputId" : {
            "type" : "string",
            "description" : "output DataObject"
          },
          "transformer" : {
            "type" : "object",
            "description" : "Configuration of custom file transformation between one input and one output (1:1)",
            "additionalProperties" : false,
            "title" : "CustomFileTransformerConfig",
            "properties" : {
              "className" : {
                "type" : "string",
                "description" : "Optional class name to load transformer code from"
              },
              "scalaFile" : {
                "type" : "string",
                "description" : "Optional file where scala code for transformation is loaded from"
              },
              "scalaCode" : {
                "type" : "string",
                "description" : "Optional scala code for transformation"
              },
              "options" : {
                "type" : "object",
                "description" : "Options to pass to the transformation",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          },
          "filesPerPartition" : {
            "type" : "integer",
            "description" : "number of files per Spark partition"
          },
          "breakFileRefLineage" : {
            "type" : "string",
            "description" : "Input[[FileRefDataObject]] which can CanCreateInputStream"
          },
          "executionMode" : {
            "description" : "optional execution mode for this Action",
            "oneOf" : [ {
              "$ref" : "#/definitions/ExecutionMode/CustomPartitionMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/DataObjectStateIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FailIfNoPartitionValuesMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FileIncrementalMoveMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/PartitionDiffMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/ProcessAllMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkStreamingMode"
            } ]
          },
          "executionCondition" : {
            "type" : "object",
            "description" : "Definition of a Spark SQL condition with description.\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
            "additionalProperties" : false,
            "required" : [ "expression" ],
            "title" : "Condition",
            "properties" : {
              "expression" : {
                "type" : "string",
                "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
              },
              "description" : {
                "type" : "string",
                "description" : "A textual description of the condition to be shown in error messages."
              }
            }
          },
          "metricsFailCondition" : {
            "type" : "string",
            "description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ActionMetadata"
          }
        }
      },
      "CustomScriptAction" : {
        "type" : "object",
        "description" : "[[Action]] execute script after multiple input DataObjects are ready, notifying multiple output DataObjects when script succeeded.",
        "additionalProperties" : false,
        "required" : [ "id", "inputIds", "outputIds" ],
        "title" : "CustomScriptAction",
        "properties" : {
          "type" : {
            "const" : "CustomScriptAction"
          },
          "id" : {
            "type" : "string",
            "description" : "put configuration validation checks here"
          },
          "inputIds" : {
            "type" : "array",
            "description" : "input DataObject\\'s",
            "items" : {
              "type" : "string"
            }
          },
          "outputIds" : {
            "type" : "array",
            "description" : "output DataObject\\'s",
            "items" : {
              "type" : "string"
            }
          },
          "scripts" : {
            "type" : "array",
            "description" : "definition of scripts to execute",
            "items" : {
              "oneOf" : [ {
                "$ref" : "#/definitions/ParsableScriptDef/CmdScript"
              }, {
                "$ref" : "#/definitions/ParsableScriptDef/DockerRunScript"
              } ]
            }
          },
          "executionCondition" : {
            "type" : "object",
            "description" : "Definition of a Spark SQL condition with description.\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
            "additionalProperties" : false,
            "required" : [ "expression" ],
            "title" : "Condition",
            "properties" : {
              "expression" : {
                "type" : "string",
                "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
              },
              "description" : {
                "type" : "string",
                "description" : "A textual description of the condition to be shown in error messages."
              }
            }
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ActionMetadata"
          }
        }
      },
      "CustomSnowparkAction" : {
        "type" : "object",
        "additionalProperties" : false,
        "required" : [ "id", "inputIds", "outputIds", "transformer" ],
        "title" : "CustomSnowparkAction",
        "properties" : {
          "type" : {
            "const" : "CustomSnowparkAction"
          },
          "id" : {
            "type" : "string",
            "description" : "put configuration validation checks here"
          },
          "inputIds" : {
            "type" : "array",
            "description" : "put configuration validation checks here",
            "items" : {
              "type" : "string"
            }
          },
          "outputIds" : {
            "type" : "array",
            "description" : "put configuration validation checks here",
            "items" : {
              "type" : "string"
            }
          },
          "recursiveInputIds" : {
            "type" : "array",
            "description" : "put configuration validation checks here",
            "items" : {
              "type" : "string"
            }
          },
          "mainInputId" : {
            "type" : "string",
            "description" : "put configuration validation checks here"
          },
          "mainOutputId" : {
            "type" : "string",
            "description" : "put configuration validation checks here"
          },
          "executionCondition" : {
            "type" : "object",
            "description" : "Definition of a Spark SQL condition with description.\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
            "additionalProperties" : false,
            "required" : [ "expression" ],
            "title" : "Condition",
            "properties" : {
              "expression" : {
                "type" : "string",
                "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
              },
              "description" : {
                "type" : "string",
                "description" : "A textual description of the condition to be shown in error messages."
              }
            }
          },
          "executionMode" : {
            "description" : "put configuration validation checks here",
            "oneOf" : [ {
              "$ref" : "#/definitions/ExecutionMode/CustomPartitionMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/DataObjectStateIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FailIfNoPartitionValuesMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FileIncrementalMoveMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/PartitionDiffMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/ProcessAllMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkStreamingMode"
            } ]
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ActionMetadata"
          },
          "transformer" : {
            "type" : "object",
            "additionalProperties" : false,
            "required" : [ "className" ],
            "title" : "SnowparkDfsTransformer",
            "properties" : {
              "name" : {
                "type" : "string",
                "description" : "Returns the factory that can parse this type (that is, type`CO`).\nTypically, implementations of this method should return the companion object of the implementing class.\nThe companion object in turn should implement[[FromConfigFactory]] .\nOTHERTAG: the factory (object) for this class."
              },
              "description" : {
                "type" : "string",
                "description" : "Returns the factory that can parse this type (that is, type`CO`).\nTypically, implementations of this method should return the companion object of the implementing class.\nThe companion object in turn should implement[[FromConfigFactory]] .\nOTHERTAG: the factory (object) for this class."
              },
              "className" : {
                "type" : "string",
                "description" : "Returns the factory that can parse this type (that is, type`CO`).\nTypically, implementations of this method should return the companion object of the implementing class.\nThe companion object in turn should implement[[FromConfigFactory]] .\nOTHERTAG: the factory (object) for this class."
              },
              "options" : {
                "type" : "object",
                "description" : "Returns the factory that can parse this type (that is, type`CO`).\nTypically, implementations of this method should return the companion object of the implementing class.\nThe companion object in turn should implement[[FromConfigFactory]] .\nOTHERTAG: the factory (object) for this class.",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          }
        }
      },
      "CustomSparkAction" : {
        "type" : "object",
        "description" : "[[Action]] to transform data according to a custom transformer.\nAllows to transform multiple input and output dataframes.",
        "additionalProperties" : false,
        "required" : [ "id", "inputIds", "outputIds" ],
        "title" : "CustomSparkAction",
        "properties" : {
          "type" : {
            "const" : "CustomSparkAction"
          },
          "id" : {
            "type" : "string",
            "description" : "Stop propagating input DataFrame through action and instead get a new DataFrame from DataObject.\nThis can help to save memory and performance if the input DataFrame includes many transformations from previous Actions.\nThe new DataFrame will be initialized according to the SubFeed\\'s partitionValues."
          },
          "inputIds" : {
            "type" : "array",
            "description" : "input DataObject\\'s",
            "items" : {
              "type" : "string"
            }
          },
          "outputIds" : {
            "type" : "array",
            "description" : "output DataObject\\'s",
            "items" : {
              "type" : "string"
            }
          },
          "transformer" : {
            "type" : "object",
            "description" : "Configuration of a custom Spark-DataFrame transformation between many inputs and many outputs (n:m).\nDefine a transform function which receives a map of input DataObjectIds with DataFrames and a map of options and has\nto return a map of output DataObjectIds with DataFrames, see also trait[[CustomDfsTransformer]] .",
            "additionalProperties" : false,
            "title" : "CustomDfsTransformerConfig",
            "properties" : {
              "className" : {
                "type" : "string",
                "description" : "Optional class name implementing trait[[CustomDfsTransformer]]"
              },
              "scalaFile" : {
                "type" : "string",
                "description" : "Optional file where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
              },
              "scalaCode" : {
                "type" : "string",
                "description" : "Optional scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
              },
              "sqlCode" : {
                "type" : "object",
                "description" : "Optional map of DataObjectId and corresponding SQL Code.\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\nExample: \\\"select * from test where run = %{runId}\\\"",
                "additionalProperties" : {
                  "type" : "string"
                }
              },
              "options" : {
                "type" : "object",
                "description" : "Options to pass to the transformation",
                "additionalProperties" : {
                  "type" : "string"
                }
              },
              "runtimeOptions" : {
                "type" : "object",
                "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          },
          "transformers" : {
            "type" : "array",
            "description" : "Stop propagating input DataFrame through action and instead get a new DataFrame from DataObject.\nThis can help to save memory and performance if the input DataFrame includes many transformations from previous Actions.\nThe new DataFrame will be initialized according to the SubFeed\\'s partitionValues.",
            "items" : {
              "oneOf" : [ {
                "$ref" : "#/definitions/ParsableDfsTransformer/DfTransformerWrapperDfsTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfsTransformer/SQLDfsTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfsTransformer/ScalaClassDfsTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfsTransformer/ScalaCodeDfsTransformer"
              } ]
            }
          },
          "breakDataFrameLineage" : {
            "type" : "string",
            "description" : "Stop propagating input DataFrame through action and instead get a new DataFrame from DataObject.\nThis can help to save memory and performance if the input DataFrame includes many transformations from previous Actions.\nThe new DataFrame will be initialized according to the SubFeed\\'s partitionValues."
          },
          "persist" : {
            "type" : "string",
            "description" : "Stop propagating input DataFrame through action and instead get a new DataFrame from DataObject.\nThis can help to save memory and performance if the input DataFrame includes many transformations from previous Actions.\nThe new DataFrame will be initialized according to the SubFeed\\'s partitionValues."
          },
          "mainInputId" : {
            "type" : "string",
            "description" : "optional selection of main inputId used for execution mode and partition values propagation. Only needed if there are multiple input DataObject\\'s."
          },
          "mainOutputId" : {
            "type" : "string",
            "description" : "optional selection of main outputId used for execution mode and partition values propagation. Only needed if there are multiple output DataObject\\'s."
          },
          "executionMode" : {
            "description" : "optional execution mode for this Action",
            "oneOf" : [ {
              "$ref" : "#/definitions/ExecutionMode/CustomPartitionMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/DataObjectStateIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FailIfNoPartitionValuesMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FileIncrementalMoveMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/PartitionDiffMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/ProcessAllMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkStreamingMode"
            } ]
          },
          "executionCondition" : {
            "type" : "object",
            "description" : "Definition of a Spark SQL condition with description.\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
            "additionalProperties" : false,
            "required" : [ "expression" ],
            "title" : "Condition",
            "properties" : {
              "expression" : {
                "type" : "string",
                "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
              },
              "description" : {
                "type" : "string",
                "description" : "A textual description of the condition to be shown in error messages."
              }
            }
          },
          "metricsFailCondition" : {
            "type" : "string",
            "description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ActionMetadata"
          },
          "recursiveInputIds" : {
            "type" : "array",
            "description" : "output of action that are used as input in the same action",
            "items" : {
              "type" : "string"
            }
          },
          "inputIdsToIgnoreFilter" : {
            "type" : "array",
            "description" : "optional list of input ids to ignore filter (partition values & filter clause)",
            "items" : {
              "type" : "string"
            }
          }
        }
      },
      "DeduplicateAction" : {
        "type" : "object",
        "description" : "[[Action]]to deduplicate a subfeed.\nDeduplication keeps the last record for every key, also after it has been deleted in the source.\nDeduplicateAction adds an additional Column[[TechnicalTableColumn.captured]]. It contains the timestamp of the last occurrence of the record in the source.\nThis creates lots of updates. Especially when using saveMode.Merge it is better to set[[TechnicalTableColumn.captured]]to the last change of the record in the source. Use updateCapturedColumnOnlyWhenChanged = true to enable this optimization.\nDeduplicateAction needs a transactional table (e.g.[[TransactionalSparkTableDataObject]]) as output with defined primary keys.\nIf output implements[[CanMergeDataFrame]] , saveMode.Merge can be enabled by setting mergeModeEnable = true. This allows for much better performance.",
        "additionalProperties" : false,
        "required" : [ "id", "inputId", "outputId" ],
        "title" : "DeduplicateAction",
        "properties" : {
          "type" : {
            "const" : "DeduplicateAction"
          },
          "id" : {
            "type" : "string",
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "inputId" : {
            "type" : "string",
            "description" : "inputs DataObject"
          },
          "outputId" : {
            "type" : "string",
            "description" : "output DataObject"
          },
          "transformer" : {
            "type" : "object",
            "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1)\nDefine a transform function which receives a DataObjectIds, a DataFrames and a map of options and has to return a\nDataFrame, see also[[CustomDfTransformer]].\nNote about Python transformation: Environment with Python and PySpark needed.\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\nOther variables available are\n-`inputDf`: Input DataFrame\n-`options`: Transformation options as Map[String,String]\n-`dataObjectId`: Id of input dataObject as String\nOutput DataFrame must be set with`setOutputDf(df)` .",
            "additionalProperties" : false,
            "title" : "CustomDfTransformerConfig",
            "properties" : {
              "className" : {
                "type" : "string",
                "description" : "Optional class name implementing trait[[CustomDfTransformer]]"
              },
              "scalaFile" : {
                "type" : "string",
                "description" : "Optional file where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
              },
              "scalaCode" : {
                "type" : "string",
                "description" : "Optional scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
              },
              "sqlCode" : {
                "type" : "string",
                "description" : "Optional SQL code for transformation.\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\nExample: \\\"select * from test where run = %{runId}\\\""
              },
              "pythonFile" : {
                "type" : "string",
                "description" : "Optional pythonFile to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
              },
              "pythonCode" : {
                "type" : "string",
                "description" : "Optional pythonCode to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
              },
              "options" : {
                "type" : "object",
                "description" : "Options to pass to the transformation",
                "additionalProperties" : {
                  "type" : "string"
                }
              },
              "runtimeOptions" : {
                "type" : "object",
                "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          },
          "transformers" : {
            "type" : "array",
            "description" : "optional list of transformations to apply before deduplication. See[[sparktransformer]] for a list of included Transformers.\nThe transformations are applied according to the lists ordering.",
            "items" : {
              "oneOf" : [ {
                "$ref" : "#/definitions/ParsableDfTransformer/AdditionalColumnsTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/BlacklistTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/DataValidationTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/FilterTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/PythonCodeDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/RepartitionTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/SQLDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/ScalaClassDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/ScalaCodeDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/ScalaNotebookDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/StandardizeDatatypesTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/WhitelistTransformer"
              } ]
            }
          },
          "columnBlacklist" : {
            "type" : "array",
            "deprecated" : true,
            "description" : "Remove all columns on blacklist from dataframe",
            "items" : {
              "type" : "string"
            }
          },
          "columnWhitelist" : {
            "type" : "array",
            "deprecated" : true,
            "description" : "Keep only columns on whitelist in dataframe",
            "items" : {
              "type" : "string"
            }
          },
          "additionalColumns" : {
            "type" : "object",
            "deprecated" : true,
            "description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\nThe spark sql expressions are evaluated against an instance of[[io.smartdatalake.util.misc.DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "filterClause" : {
            "type" : "string",
            "deprecated" : true,
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "standardizeDatatypes" : {
            "type" : "string",
            "deprecated" : true,
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "ignoreOldDeletedColumns" : {
            "type" : "string",
            "description" : "if true, remove no longer existing columns in Schema Evolution"
          },
          "ignoreOldDeletedNestedColumns" : {
            "type" : "string",
            "description" : "if true, remove no longer existing columns from nested data types in Schema Evolution.\nKeeping deleted columns in complex data types has performance impact as all new data\nin the future has to be converted by a complex function."
          },
          "updateCapturedColumnOnlyWhenChanged" : {
            "type" : "string",
            "description" : "Set to true to enable update Column[[TechnicalTableColumn.captured]] only if Record has changed in the source, instead of updating it with every execution (default=false).\nThis results in much less records updated with saveMode.Merge."
          },
          "mergeModeEnable" : {
            "type" : "string",
            "description" : "Set to true to use saveMode.Merge for much better performance. Output DataObject must implement[[CanMergeDataFrame]] if enabled (default = false)."
          },
          "mergeModeAdditionalJoinPredicate" : {
            "type" : "string",
            "description" : "To optimize performance it might be interesting to limit the records read from the existing table data, e.g. it might be sufficient to use only the last 7 days.\nSpecify a condition to select existing data to be used in transformation as Spark SQL expression.\nUse table alias \\'existing\\' to reference columns of the existing table data."
          },
          "breakDataFrameLineage" : {
            "type" : "string",
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "persist" : {
            "type" : "string",
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "executionMode" : {
            "description" : "optional execution mode for this Action",
            "oneOf" : [ {
              "$ref" : "#/definitions/ExecutionMode/CustomPartitionMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/DataObjectStateIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FailIfNoPartitionValuesMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FileIncrementalMoveMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/PartitionDiffMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/ProcessAllMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkStreamingMode"
            } ]
          },
          "executionCondition" : {
            "type" : "object",
            "description" : "Definition of a Spark SQL condition with description.\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
            "additionalProperties" : false,
            "required" : [ "expression" ],
            "title" : "Condition",
            "properties" : {
              "expression" : {
                "type" : "string",
                "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
              },
              "description" : {
                "type" : "string",
                "description" : "A textual description of the condition to be shown in error messages."
              }
            }
          },
          "metricsFailCondition" : {
            "type" : "string",
            "description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ActionMetadata"
          }
        }
      },
      "FileTransferAction" : {
        "type" : "object",
        "description" : "[[Action]] to transfer files between SFtp, Hadoop and local Fs.",
        "additionalProperties" : false,
        "required" : [ "id", "inputId", "outputId" ],
        "title" : "FileTransferAction",
        "properties" : {
          "type" : {
            "const" : "FileTransferAction"
          },
          "id" : {
            "type" : "string",
            "description" : "Input[[FileRefDataObject]] which can CanCreateInputStream"
          },
          "inputId" : {
            "type" : "string",
            "description" : "inputs DataObject"
          },
          "outputId" : {
            "type" : "string",
            "description" : "output DataObject"
          },
          "overwrite" : {
            "type" : "string",
            "description" : "Input[[FileRefDataObject]] which can CanCreateInputStream"
          },
          "breakFileRefLineage" : {
            "type" : "string",
            "description" : "If set to true, file references passed on from previous action are ignored by this action.\nThe action will detect on its own what files it is going to process."
          },
          "executionMode" : {
            "description" : "optional execution mode for this Action",
            "oneOf" : [ {
              "$ref" : "#/definitions/ExecutionMode/CustomPartitionMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/DataObjectStateIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FailIfNoPartitionValuesMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FileIncrementalMoveMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/PartitionDiffMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/ProcessAllMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkStreamingMode"
            } ]
          },
          "executionCondition" : {
            "type" : "object",
            "description" : "Definition of a Spark SQL condition with description.\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
            "additionalProperties" : false,
            "required" : [ "expression" ],
            "title" : "Condition",
            "properties" : {
              "expression" : {
                "type" : "string",
                "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
              },
              "description" : {
                "type" : "string",
                "description" : "A textual description of the condition to be shown in error messages."
              }
            }
          },
          "metricsFailCondition" : {
            "type" : "string",
            "description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ActionMetadata"
          }
        }
      },
      "HistorizeAction" : {
        "type" : "object",
        "description" : "[[Action]] to historize a subfeed.\nHistorization creates a technical history of data by creating valid-from/to columns.\nIt needs a transactional table as output with defined primary keys.",
        "additionalProperties" : false,
        "required" : [ "id", "inputId", "outputId" ],
        "title" : "HistorizeAction",
        "properties" : {
          "type" : {
            "const" : "HistorizeAction"
          },
          "id" : {
            "type" : "string",
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "inputId" : {
            "type" : "string",
            "description" : "inputs DataObject"
          },
          "outputId" : {
            "type" : "string",
            "description" : "output DataObject"
          },
          "transformer" : {
            "type" : "object",
            "description" : "Configuration of a custom Spark-DataFrame transformation between one input and one output (1:1)\nDefine a transform function which receives a DataObjectIds, a DataFrames and a map of options and has to return a\nDataFrame, see also[[CustomDfTransformer]].\nNote about Python transformation: Environment with Python and PySpark needed.\nPySpark session is initialize and available under variables`sc`,`session`,`sqlContext`.\nOther variables available are\n-`inputDf`: Input DataFrame\n-`options`: Transformation options as Map[String,String]\n-`dataObjectId`: Id of input dataObject as String\nOutput DataFrame must be set with`setOutputDf(df)` .",
            "additionalProperties" : false,
            "title" : "CustomDfTransformerConfig",
            "properties" : {
              "className" : {
                "type" : "string",
                "description" : "Optional class name implementing trait[[CustomDfTransformer]]"
              },
              "scalaFile" : {
                "type" : "string",
                "description" : "Optional file where scala code for transformation is loaded from. The scala code in the file needs to be a function of type[[fnTransformType]] ."
              },
              "scalaCode" : {
                "type" : "string",
                "description" : "Optional scala code for transformation. The scala code needs to be a function of type[[fnTransformType]] ."
              },
              "sqlCode" : {
                "type" : "string",
                "description" : "Optional SQL code for transformation.\nUse tokens %{<key>} to replace with runtimeOptions in SQL code.\nExample: \\\"select * from test where run = %{runId}\\\""
              },
              "pythonFile" : {
                "type" : "string",
                "description" : "Optional pythonFile to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
              },
              "pythonCode" : {
                "type" : "string",
                "description" : "Optional pythonCode to use for python transformation. The python code can use variables inputDf, dataObjectId and options. The transformed DataFrame has to be set with setOutputDf."
              },
              "options" : {
                "type" : "object",
                "description" : "Options to pass to the transformation",
                "additionalProperties" : {
                  "type" : "string"
                }
              },
              "runtimeOptions" : {
                "type" : "object",
                "description" : "optional tuples of [key, spark sql expression] to be added as additional options when executing transformation.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
                "additionalProperties" : {
                  "type" : "string"
                }
              }
            }
          },
          "transformers" : {
            "type" : "array",
            "description" : "optional list of transformations to apply before historization. See[[sparktransformer]] for a list of included Transformers.\nThe transformations are applied according to the lists ordering.",
            "items" : {
              "oneOf" : [ {
                "$ref" : "#/definitions/ParsableDfTransformer/AdditionalColumnsTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/BlacklistTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/DataValidationTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/FilterTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/PythonCodeDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/RepartitionTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/SQLDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/ScalaClassDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/ScalaCodeDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/ScalaNotebookDfTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/StandardizeDatatypesTransformer"
              }, {
                "$ref" : "#/definitions/ParsableDfTransformer/WhitelistTransformer"
              } ]
            }
          },
          "columnBlacklist" : {
            "type" : "array",
            "deprecated" : true,
            "description" : "Remove all columns on blacklist from dataframe",
            "items" : {
              "type" : "string"
            }
          },
          "columnWhitelist" : {
            "type" : "array",
            "deprecated" : true,
            "description" : "Keep only columns on whitelist in dataframe",
            "items" : {
              "type" : "string"
            }
          },
          "additionalColumns" : {
            "type" : "object",
            "deprecated" : true,
            "description" : "optional tuples of [column name, spark sql expression] to be added as additional columns to the dataframe.\nThe spark sql expressions are evaluated against an instance of[[DefaultExpressionData]] .",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "standardizeDatatypes" : {
            "type" : "string",
            "deprecated" : true,
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "filterClause" : {
            "type" : "string",
            "deprecated" : true,
            "description" : "Filter of data to be processed by historization. It can be used to exclude historical data not needed to create new history, for performance reasons.\nNote that filterClause is only applied if mergeModeEnable=false. Use mergeModeAdditionalJoinPredicate if mergeModeEnable=true to achieve a similar performance tuning."
          },
          "historizeBlacklist" : {
            "type" : "array",
            "deprecated" : true,
            "description" : "optional list of columns to ignore when comparing two records in historization. Can not be used together with[[historizeWhitelist]] .",
            "items" : {
              "type" : "string"
            }
          },
          "historizeWhitelist" : {
            "type" : "array",
            "deprecated" : true,
            "description" : "optional final list of columns to use when comparing two records in historization. Can not be used together with[[historizeBlacklist]] .",
            "items" : {
              "type" : "string"
            }
          },
          "ignoreOldDeletedColumns" : {
            "type" : "string",
            "description" : "if true, remove no longer existing columns in Schema Evolution"
          },
          "ignoreOldDeletedNestedColumns" : {
            "type" : "string",
            "description" : "if true, remove no longer existing columns from nested data types in Schema Evolution.\nKeeping deleted columns in complex data types has performance impact as all new data\nin the future has to be converted by a complex function."
          },
          "mergeModeEnable" : {
            "type" : "string",
            "description" : "Set to true to use saveMode.Merge for much better performance. Output DataObject must implement[[CanMergeDataFrame]] if enabled (default = false)."
          },
          "mergeModeAdditionalJoinPredicate" : {
            "type" : "string",
            "description" : "To optimize performance it might be interesting to limit the records read from the existing table data, e.g. it might be sufficient to use only the last 7 days.\nSpecify a condition to select existing data to be used in transformation as Spark SQL expression.\nUse table alias \\'existing\\' to reference columns of the existing table data."
          },
          "breakDataFrameLineage" : {
            "type" : "string",
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "persist" : {
            "type" : "string",
            "description" : "Input[[DataObject]] which can CanCreateDataFrame"
          },
          "executionMode" : {
            "description" : "optional execution mode for this Action",
            "oneOf" : [ {
              "$ref" : "#/definitions/ExecutionMode/CustomPartitionMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/DataObjectStateIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FailIfNoPartitionValuesMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/FileIncrementalMoveMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/PartitionDiffMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/ProcessAllMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkIncrementalMode"
            }, {
              "$ref" : "#/definitions/ExecutionMode/SparkStreamingMode"
            } ]
          },
          "executionCondition" : {
            "type" : "object",
            "description" : "Definition of a Spark SQL condition with description.\nThis is used for example to define failConditions of[[PartitionDiffMode]] .",
            "additionalProperties" : false,
            "required" : [ "expression" ],
            "title" : "Condition",
            "properties" : {
              "expression" : {
                "type" : "string",
                "description" : "Condition formulated as Spark SQL. The attributes available are dependent on the context."
              },
              "description" : {
                "type" : "string",
                "description" : "A textual description of the condition to be shown in error messages."
              }
            }
          },
          "metricsFailCondition" : {
            "type" : "string",
            "description" : "optional spark sql expression evaluated as where-clause against dataframe of metrics. Available columns are dataObjectId, key, value.\nIf there are any rows passing the where clause, a MetricCheckFailed exception is thrown."
          },
          "metadata" : {
            "$ref" : "#/definitions/Others/ActionMetadata"
          }
        }
      }
    }
  },
  "id" : "sdl-schema.json#",
  "version" : "2.2.2-SNAPSHOT",
  "$schema" : "http://json-schema.org/draft-07/schema#"
}
