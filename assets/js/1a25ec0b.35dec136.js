"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[8672],{2363:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var a=i(4848),t=i(8453);const r={id:"features",title:"Features"},l=void 0,s={id:"features",title:"Features",description:"Smart Data Lake Builder is still under heavy development so new features are added all the time.",source:"@site/docs/features.md",sourceDirName:".",slug:"/features",permalink:"/docs/features",draft:!1,unlisted:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/features.md",tags:[],version:"current",frontMatter:{id:"features",title:"Features"},sidebar:"tutorialSidebar",previous:{title:"Introduction",permalink:"/docs/"},next:{title:"Architecture",permalink:"/docs/architecture"}},o={},c=[{value:"Declarative approach, file based metadata",id:"declarative-approach-file-based-metadata",level:2},{value:"Support for complex workflows &amp; streaming",id:"support-for-complex-workflows--streaming",level:2},{value:"Multi-Engine",id:"multi-engine",level:2},{value:"Connectivity",id:"connectivity",level:2},{value:"Generic Transformations",id:"generic-transformations",level:2},{value:"Customizable Transformations",id:"customizable-transformations",level:2},{value:"Early Validation",id:"early-validation",level:2},{value:"Execution Modes",id:"execution-modes",level:2},{value:"Schema Evolution",id:"schema-evolution",level:2},{value:"Metrics",id:"metrics",level:2},{value:"Data Catalog",id:"data-catalog",level:2},{value:"Lineage",id:"lineage",level:2},{value:"Data Quality",id:"data-quality",level:2},{value:"Testing",id:"testing",level:2},{value:"Spark Performance",id:"spark-performance",level:2},{value:"Housekeeping",id:"housekeeping",level:2},{value:"User Interface",id:"user-interface",level:2}];function d(e){const n={a:"a",h2:"h2",li:"li",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"Smart Data Lake Builder is still under heavy development so new features are added all the time.\nThe following list will give you a rough overview of current and planned features.\nMore details on the roadmap will follow shortly."}),"\n",(0,a.jsx)(n.h2,{id:"declarative-approach-file-based-metadata",children:"Declarative approach, file based metadata"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Easy to version with a VCS for DevOps"}),"\n",(0,a.jsx)(n.li,{children:"Flexible structure by splitting over multiple files and subdirectories"}),"\n",(0,a.jsx)(n.li,{children:"Easy to generate from third party metadata (e.g. source system table catalog) to automate transformation of large number of DataObjects"}),"\n",(0,a.jsx)(n.li,{children:"Support to handle multiple environments"}),"\n"]}),"\n",(0,a.jsxs)(n.h2,{id:"support-for-complex-workflows--streaming",children:["Support for ",(0,a.jsx)(n.a,{href:"reference/dag",children:"complex workflows"})," & ",(0,a.jsx)(n.a,{href:"reference/streaming",children:"streaming"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Fork, join, parallel execution, multiple start- & end-nodes possible"}),"\n",(0,a.jsx)(n.li,{children:"Recovery of failed runs"}),"\n",(0,a.jsx)(n.li,{children:"Switch a workflow between batch or streaming execution by using just a command line switch"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"multi-engine",children:"Multi-Engine"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Spark (DataFrames)"}),"\n",(0,a.jsx)(n.li,{children:"Snowflake (DataFrames)"}),"\n",(0,a.jsx)(n.li,{children:"File (Input&OutputStream)"}),"\n",(0,a.jsx)(n.li,{children:"Future: SQL, Kafka Streams, Flink, \u2026"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"connectivity",children:"Connectivity"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Spark: diverse connectors (HadoopFS, Hive, DeltaLake, JDBC, Kafka, Splunk, Webservice, JMS) and formats (CSV, JSON, XML, Avro, Parquet, Excel, Access \u2026)"}),"\n",(0,a.jsx)(n.li,{children:"File: SFTP, Local, Webservice"}),"\n",(0,a.jsx)(n.li,{children:"Easy to extend by implementing predefined scala traits"}),"\n",(0,a.jsx)(n.li,{children:"Support for getting secrets from different secret providers"}),"\n",(0,a.jsx)(n.li,{children:"Support for SQL update & merge (Jdbc, DeltaLake)"}),"\n",(0,a.jsxs)(n.li,{children:["Support for integration of ",(0,a.jsx)(n.a,{href:"https://docs.airbyte.com/category/sources",children:"Airbyte sources"})]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"generic-transformations",children:"Generic Transformations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Spark based: Copy, Historization, Deduplication (incl. incremental update/merge mode for streaming)"}),"\n",(0,a.jsx)(n.li,{children:"File based: FileTransfer"}),"\n",(0,a.jsx)(n.li,{children:"Easy to extend by implementing predefined scala traits"}),"\n",(0,a.jsx)(n.li,{children:"Future: applying MLFlow machine learning models"}),"\n"]}),"\n",(0,a.jsxs)(n.h2,{id:"customizable-transformations",children:["Customizable ",(0,a.jsx)(n.a,{href:"reference/transformations",children:"Transformations"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Spark Transformations:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Chain predefined standard transformations (e.g. filter, row level data validation and more) and custom transformations within the same action"}),"\n",(0,a.jsx)(n.li,{children:"Custom Transformation Languages: SQL, Scala (Class, compile from config), Python"}),"\n",(0,a.jsx)(n.li,{children:"Many input DataFrames to many outputs DataFrames (but only one output recommended normally, in order to define dependencies as detailed as possible for the lineage)"}),"\n",(0,a.jsx)(n.li,{children:"Add metadata to each transformation to explain your data pipeline."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["File Transformations:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Language: Scala"}),"\n",(0,a.jsx)(n.li,{children:"Only one to one (one InputStream to one OutputStream)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"early-validation",children:"Early Validation"}),"\n",(0,a.jsx)(n.p,{children:"Execution in 3 phases before execution"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Load Config: validate configuration"}),"\n",(0,a.jsx)(n.li,{children:"Prepare: validate connections"}),"\n",(0,a.jsx)(n.li,{children:"Init: validate Spark DataFrame Lineage (missing columns in transformations of later actions will stop the execution)"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["see ",(0,a.jsx)(n.a,{href:"reference/executionPhases",children:"execution phases"})," for details"]}),"\n",(0,a.jsx)(n.h2,{id:"execution-modes",children:(0,a.jsx)(n.a,{href:"reference/executionModes",children:"Execution Modes"})}),"\n",(0,a.jsx)(n.p,{children:"Select data to process, e.g."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Process all data"}),"\n",(0,a.jsx)(n.li,{children:"Partition parameters: give partition values to process for start nodes as parameter"}),"\n",(0,a.jsx)(n.li,{children:"Partition Diff: search missing partitions and use as parameter"}),"\n",(0,a.jsx)(n.li,{children:"Incremental: use stateful input DataObject, or compare sortable column between source and target and load the difference"}),"\n",(0,a.jsx)(n.li,{children:"Spark Streaming: asynchronous incremental processing by using Spark Structured Streaming"}),"\n",(0,a.jsx)(n.li,{children:"Spark Streaming Once: synchronous incremental processing by using Spark Structured Streaming with Trigger=Once mode"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"schema-evolution",children:(0,a.jsx)(n.a,{href:"reference/schema#schema-evolution",children:"Schema Evolution"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Automatic evolution of data schemas (new column, removed column, changed datatype)"}),"\n",(0,a.jsx)(n.li,{children:"Support for changes in complex datatypes (e.g. new column in array of struct)"}),"\n",(0,a.jsx)(n.li,{children:"Automatic adaption of DataObjects with fixed schema (Jdbc, DeltaLake)"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"metrics",children:"Metrics"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Number of rows read/written per DataObject"}),"\n",(0,a.jsx)(n.li,{children:"Execution duration per Action"}),"\n",(0,a.jsx)(n.li,{children:"Arbitrary custom metrics defined by aggregation expressions"}),"\n",(0,a.jsx)(n.li,{children:"Predefined metric for transfer rate, completness and ensuring unique constraints."}),"\n",(0,a.jsx)(n.li,{children:"StateListener interface to get notified about progress & metrics"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"data-catalog",children:"Data Catalog"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Report all DataObjects attributes (incl. foreign keys if defined) for visualisation of data catalog in BI tool"}),"\n",(0,a.jsx)(n.li,{children:"Metadata support for categorizing Actions and DataObjects"}),"\n",(0,a.jsx)(n.li,{children:"Custom metadata attributes"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"lineage",children:"Lineage"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Browse lineage of DataObjects and Actions in the UI"}),"\n",(0,a.jsx)(n.li,{}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"data-quality",children:(0,a.jsx)(n.a,{href:"reference/dataQuality",children:"Data Quality"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Metadata support for primary & foreign keys"}),"\n",(0,a.jsx)(n.li,{children:"Check & report primary key violations by executing primary key checker action"}),"\n",(0,a.jsx)(n.li,{children:"Define and validate row-level Constraints before writing DataObject"}),"\n",(0,a.jsx)(n.li,{children:"Define and evaluate Expectations when writing DataObject, trigger warning or error, collect result as custom metric"}),"\n",(0,a.jsx)(n.li,{children:"Future: Report data quality (foreign key matching & expectations) by executing data quality reporter action"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"testing",children:(0,a.jsx)(n.a,{href:"reference/testing",children:"Testing"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Support for CI","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Config validation"}),"\n",(0,a.jsx)(n.li,{children:"Custom transformation unit tests"}),"\n",(0,a.jsx)(n.li,{children:"Spark data pipeline simulation (acceptance tests)"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Support for Deployment","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Dry-run"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"spark-performance",children:"Spark Performance"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Execute multiple Spark jobs in parallel within the same Spark Session to save resources"}),"\n",(0,a.jsx)(n.li,{children:"Automatically cache and release intermediate results (DataFrames)"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"housekeeping",children:(0,a.jsx)(n.a,{href:"/blog/sdl-housekeeping",children:"Housekeeping"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Delete, or archive & compact partitions according to configurable expressions"}),"\n",(0,a.jsx)(n.li,{children:"Extend with custom housekeeping logic"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"user-interface",children:(0,a.jsx)(n.a,{href:"/blog/sdl-uidemo",children:"User Interface"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Configuration viewer with catalog and lineage view"}),"\n",(0,a.jsx)(n.li,{children:"Comprehensive workflow visualization"}),"\n",(0,a.jsx)(n.li,{children:"Documentation from metadata and code approach - all configuration elements can be described in the metadata, and are enriched with documentation from code where possible."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["see also ",(0,a.jsx)(n.a,{href:"https://ui-demo.smartdatalake.ch/",children:"UI Demo"})," visualizing ",(0,a.jsx)(n.a,{href:"getting-started/setup",children:"Getting Started"})," data pipeline."]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>s});var a=i(6540);const t={},r=a.createContext(t);function l(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);