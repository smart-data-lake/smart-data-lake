"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[3868],{8033:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>l,default:()=>p,frontMatter:()=>i,metadata:()=>d,toc:()=>u});var n=a(5893),r=a(1151),o=a(4866),s=a(5162);const i={title:"Delta Lake - a better data format"},l=void 0,d={id:"getting-started/part-2/delta-lake-format",title:"Delta Lake - a better data format",description:"Goal",source:"@site/docs/getting-started/part-2/delta-lake-format.md",sourceDirName:"getting-started/part-2",slug:"/getting-started/part-2/delta-lake-format",permalink:"/docs/getting-started/part-2/delta-lake-format",draft:!1,unlisted:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/getting-started/part-2/delta-lake-format.md",tags:[],version:"current",frontMatter:{title:"Delta Lake - a better data format"},sidebar:"tutorialSidebar",previous:{title:"Industrializing our data pipeline",permalink:"/docs/getting-started/part-2/industrializing"},next:{title:"Keeping historical data",permalink:"/docs/getting-started/part-2/historical-data"}},c={},u=[{value:"Goal",id:"goal",level:2},{value:"File formats",id:"file-formats",level:2},{value:"Catalog",id:"catalog",level:2},{value:"Transactions",id:"transactions",level:2},{value:"DeltaLakeTableDataObject",id:"deltalaketabledataobject",level:2},{value:"Reading Delta Lake Format with Spark",id:"reading-delta-lake-format-with-spark",level:2}];function h(e){const t={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h2,{id:"goal",children:"Goal"}),"\n",(0,n.jsx)(t.p,{children:"Up to now we have used CSV with CsvFileDataObject as file format. We will switch to a more modern data format in this step which supports a catalog, compression and even transactions."}),"\n",(0,n.jsx)(t.h2,{id:"file-formats",children:"File formats"}),"\n",(0,n.jsx)(t.p,{children:"Smart Data Lake Builder has built in support for many data formats and technologies.\nAn important one is storing files on a Hadoop filesystem, supporting standard file formats such as CSV, Json, Avro or Parquet.\nIn Part 1 we have used CSV through the CsvFileDataObject. CSV files can be easily checked in an editor or Excel, but the format also has many problems, e.g. support of multi-line strings or lack of data type definition.\nOften Parquet format is used, as it includes a schema definition and is very space efficient through its columnar compression."}),"\n",(0,n.jsx)(t.h2,{id:"catalog",children:"Catalog"}),"\n",(0,n.jsx)(t.p,{children:"Just storing files on Hadoop filesystem makes it difficult to use them in a SQL engine such as Spark SQL. You need a metadata layer on top which stores table definitions. This is also called a metastore or catalog.\nIf you start a Spark session, a configuration to connect to an external catalog can be set, or otherwise Spark creates an internal catalog for the session.\nWe could register our CSV files in this catalog by creating a table via a DDL-statement, including the definition of all columns, a path and the format of our data.\nBut you could also directly create and write into a table by using Spark Hive tables.\nSmart Data Lake Builder supports this by the HiveTableDataObject. It always uses Parquet file format in the background as a best practice, although Hive tables could also be created on top of CSV files."}),"\n",(0,n.jsx)(t.admonition,{type:"info",children:(0,n.jsx)(t.p,{children:"Hive is a Metadata layer and SQL engine on top of a Hadoop filesystem. Spark uses the metadata layer of Hive, but implements its own SQL engine."})}),"\n",(0,n.jsx)(t.h2,{id:"transactions",children:"Transactions"}),"\n",(0,n.jsx)(t.p,{children:"Hive tables with Parquet format are lacking transactions. This means for example that writing and reading the table at the same time could result in failure or empty results.\nIn consequence"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"consecutive jobs need to by synchronized"}),"\n",(0,n.jsx)(t.li,{children:"it's not recommended having end-user accessing the table while data processing jobs are running"}),"\n",(0,n.jsx)(t.li,{children:"update and deletes are not supported"}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:["There are other options like classical databases which always had a metadata layer, offer transactions but don't integrate easily with Hive metastore and cheap, scalable Hadoop file storage.\nNevertheless, Smart Data Lake Builder supports classical databases through the JdbcTableDataObject.\nFortunately there is a new technology called Delta Lake, see also ",(0,n.jsx)(t.a,{href:"https://delta.io/",children:"delta.io"}),". It integrates into a Hive metastore, supports transactions and stores Parquet files and a transaction log on hadoop filesystems.\nSmart Data Lake Builder supports this by the DeltaLakeTableDataObject, and this is what we are going to use for our airport and departure data now."]}),"\n",(0,n.jsx)(t.h2,{id:"deltalaketabledataobject",children:"DeltaLakeTableDataObject"}),"\n",(0,n.jsxs)(t.p,{children:["Switching to Delta Lake format is easy with Smart Data Lake Builder, just replace ",(0,n.jsx)(t.code,{children:"CsvFileDataObject"})," with ",(0,n.jsx)(t.code,{children:"DeltaLakeTableDataObject"})," and define the table's db and name.\nLet's start by changing the existing definitions for ",(0,n.jsx)(t.code,{children:"int-airports"}),", ",(0,n.jsx)(t.code,{children:"btl-departures-arrivals-airports"})," and ",(0,n.jsx)(t.code,{children:"btl-distances"}),":"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'    int-airports {\n        type = DeltaLakeTableDataObject\n        path = "~{id}"\n        table = {\n            db = "default"\n            name = "int_airports"\n        }\n    }\n\n    btl-departures-arrivals-airports {\n        type = DeltaLakeTableDataObject\n        path = "~{id}"\n        table {\n            db = "default"\n            name = "btl_departures_arrivals_airports"\n        }\n    }\n    \n    btl-distances {\n        type = DeltaLakeTableDataObject\n        path = "~{id}"\n        table {\n            db = "default"\n            name = "btl_distances"\n        }\n    }\n'})}),"\n",(0,n.jsxs)(t.p,{children:["Then create a new, similar data object ",(0,n.jsx)(t.code,{children:"int-departures"}),":"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'    int-departures {\n        type = DeltaLakeTableDataObject\n        path = "~{id}"\n        table = {\n            db = default\n            name = int_departures\n        }\n    }\n'})}),"\n",(0,n.jsxs)(t.p,{children:["Next, create a new action ",(0,n.jsx)(t.code,{children:"prepare-departures"})," in the ",(0,n.jsx)(t.code,{children:"actions"})," section to fill the new table with the data:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"    prepare-departures {\n        type = CopyAction\n        inputId = stg-departures\n        outputId = int-departures\n        metadata {\n            feed = compute\n        }\n    }\n"})}),"\n",(0,n.jsxs)(t.p,{children:["Finally, adapt the action definition for ",(0,n.jsx)(t.code,{children:"join-departures-airports"}),":"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["change ",(0,n.jsx)(t.code,{children:"stg-departures"})," to ",(0,n.jsx)(t.code,{children:"int-departures"})," in inputIds"]}),"\n",(0,n.jsxs)(t.li,{children:["change ",(0,n.jsx)(t.code,{children:"stg_departures"})," to ",(0,n.jsx)(t.code,{children:"int_departures"})," in the first SQLDfsTransformer (watch out, you need to replace the string 4 times)"]}),"\n"]}),"\n",(0,n.jsx)(t.admonition,{title:"Explanation",type:"info",children:(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["We changed ",(0,n.jsx)(t.code,{children:"int-airports"}),", ",(0,n.jsx)(t.code,{children:"btl-departures-arrivals-airports"})," and ",(0,n.jsx)(t.code,{children:"btl-distances"})," from CSV to Delta Lake format"]}),"\n",(0,n.jsxs)(t.li,{children:["Created an additional table ",(0,n.jsx)(t.code,{children:"int-departures"})]}),"\n",(0,n.jsxs)(t.li,{children:["Created an action ",(0,n.jsx)(t.code,{children:"prepare-departures"}),"  to fill the new integration layer table ",(0,n.jsx)(t.code,{children:"int-departures"})]}),"\n",(0,n.jsxs)(t.li,{children:["Adapted the existing action ",(0,n.jsx)(t.code,{children:"join-departures-airports"})," to use the new table ",(0,n.jsx)(t.code,{children:"int-departures"})]}),"\n"]})}),"\n",(0,n.jsxs)(t.p,{children:["To run our data pipeline, first make sure data directory is empty - otherwise DeltaLakeTableDataObject will fail because of existing files in different format.\nThen you can execute the usual ",(0,n.jsx)(t.em,{children:"docker run"})," command for all feeds:"]}),"\n",(0,n.jsxs)(o.Z,{groupId:"docker-podman-switch",defaultValue:"docker",values:[{label:"Docker",value:"docker"},{label:"Podman",value:"podman"}],children:[(0,n.jsx)(s.Z,{value:"docker",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-jsx",children:"mkdir data\ndocker run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest -c /mnt/config --feed-sel 'download*'\ndocker run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest -c /mnt/config --feed-sel '^(?!download).*'\n"})})}),(0,n.jsx)(s.Z,{value:"podman",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-jsx",children:"mkdir data\npodman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest -c /mnt/config --feed-sel 'download*'\npodman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest -c /mnt/config --feed-sel '^(?!download).*'\n"})})})]}),"\n",(0,n.jsxs)(t.admonition,{type:"info",children:[(0,n.jsxs)(t.p,{children:["Why two separate commands?",(0,n.jsx)(t.br,{}),"\n","Because you deleted all data first."]}),(0,n.jsxs)(t.p,{children:["Remember from part 1 that we either need to define a schema for our downloaded files or we need to execute the download steps separately on the first run.\nThe first command only executes the download steps, the second command executes everything but the download steps (regex with negative lookahead).\nSee ",(0,n.jsx)(t.a,{href:"/docs/getting-started/troubleshooting/common-problems",children:"Common Problems"})," for more Info."]})]}),"\n",(0,n.jsxs)(t.p,{children:["Getting an error like ",(0,n.jsx)(t.code,{children:"io.smartdatalake.util.webservice.WebserviceException: Read timed out"}),"? Check the list of ",(0,n.jsx)(t.a,{href:"../troubleshooting/common-problems",children:"Common Problems"})," for a workaround."]}),"\n",(0,n.jsx)(t.h2,{id:"reading-delta-lake-format-with-spark",children:"Reading Delta Lake Format with Spark"}),"\n",(0,n.jsxs)(t.p,{children:["Checking our results gets more complicated now - we can't just open delta lake format in a text editor like we used to do for CSV files.\nWe could now use SQL to query our results, that would be even better.\nOne option is to use a Spark session, i.e. by starting a spark-shell.\nBut state-of-the-art is to use notebooks like Jupyter for this.\nOne of the most advanced notebooks for Scala code we found is Polynote, see ",(0,n.jsx)(t.a,{href:"https://polynote.org/",children:"polynote.org"}),"."]}),"\n",(0,n.jsxs)(t.p,{children:["We will now start Polynote in a docker container, and an external Metastore (Derby database) in another container to share the catalog between our experiments and the notebook.\nTo do so, we will use the additional files in the subfolder ",(0,n.jsx)(t.code,{children:"part2"}),".\nExecute these commands:"]}),"\n",(0,n.jsxs)(o.Z,{groupId:"docker-podman-switch",defaultValue:"docker",values:[{label:"Docker",value:"docker"},{label:"Podman",value:"podman"}],children:[(0,n.jsx)(s.Z,{value:"docker",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-jsx",children:"docker-compose -f part2/docker-compose.yml -p getting-started build\nmkdir -p data/_metastore\ndocker-compose -f part2/docker-compose.yml -p getting-started up\n"})})}),(0,n.jsx)(s.Z,{value:"podman",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-jsx",children:"mkdir -p data/_metastore\n./part2/podman-compose.sh \n"})})})]}),"\n",(0,n.jsxs)(t.p,{children:["This might take multiple minutes.\nYou should now be able to access Polynote at ",(0,n.jsx)(t.code,{children:"localhost:8192"}),"."]}),"\n",(0,n.jsx)(t.admonition,{title:"Docker on Windows",type:"info",children:(0,n.jsxs)(t.p,{children:["If you use Windows, please read our note on ",(0,n.jsx)(t.a,{href:"../troubleshooting/docker-on-windows",children:"Docker for Windows"}),".\nYou might notice that the commands for docker and podman differ at this point.\nThe latest version of podman-compose changed the behavior of creating pods,\nwhich is why we have implemented a script ",(0,n.jsx)(t.code,{children:"podman-compose.sh"})," to emulate podman-compose."]})}),"\n",(0,n.jsxs)(t.p,{children:['But when you walk through the prepared notebook "SelectingData", you won\'t see any tables and data yet.\nCan you guess why?',(0,n.jsx)(t.br,{}),"\n","This is because your last pipeline run used an internal metastore, and not the external metastore we started with docker-compose yet.\nTo configure Spark to use our external metastore, add the following spark properties to the application.conf under global.spark-options.\nYou probably don't have a global section in your application.conf yet, so here is the full block you need to add at the top of the file:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'    global {\n      spark-options {\n        "spark.hadoop.javax.jdo.option.ConnectionURL" = "jdbc:derby://metastore:1527/db;create=true"\n        "spark.hadoop.javax.jdo.option.ConnectionDriverName" = "org.apache.derby.jdbc.ClientDriver"\n        "spark.hadoop.javax.jdo.option.ConnectionUserName" = "sa"\n        "spark.hadoop.javax.jdo.option.ConnectionPassword" = "1234"\n      }\n    }\n'})}),"\n",(0,n.jsxs)(t.p,{children:["This instructs Spark to use the external metastore you started with docker-compose.\nYour Smart Data Lake container doesn't have access to the other containers just yet.\nSo when you run your data pipeline again, you need to add a parameter ",(0,n.jsx)(t.code,{children:"--network"}),"/",(0,n.jsx)(t.code,{children:"--pod"})," to join the virtual network where the metastore is located:"]}),"\n",(0,n.jsxs)(o.Z,{groupId:"docker-podman-switch",defaultValue:"docker",values:[{label:"Docker",value:"docker"},{label:"Podman",value:"podman"}],children:[(0,n.jsx)(s.Z,{value:"docker",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-jsx",children:"docker run --hostname localhost --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config --network getting-started_default sdl-spark:latest -c /mnt/config --feed-sel '.*'\n"})})}),(0,n.jsx)(s.Z,{value:"podman",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-jsx",children:"podman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config --pod getting-started sdl-spark:latest -c /mnt/config --feed-sel '.*'\n"})})})]}),"\n",(0,n.jsxs)(t.p,{children:["After you run your data pipeline again, you should now be able to see our DataObjects data in Polynote.\nNo need to restart Polynote, just open it again and run all cells.\n",(0,n.jsx)(t.a,{target:"_blank",href:a(4565).Z+"",children:"This"})," is how the final configuration file should look like. Feel free to play around."]}),"\n",(0,n.jsxs)(t.admonition,{title:"Delta Lake tuning",type:"tip",children:[(0,n.jsxs)(t.p,{children:["You might have seen that our data pipeline with DeltaTableDataObject runs a Spark stage with 50 tasks several times.\nThis is delta lake reading it's transaction log with Spark. For our data volume, 50 tasks are way too much.\nYou can reduce the number of snapshot partitions to speed up the execution by setting the following Spark property in your ",(0,n.jsx)(t.code,{children:"application.conf"})," under ",(0,n.jsx)(t.code,{children:"global.spark-options"}),":"]}),(0,n.jsx)(t.p,{children:'"spark.databricks.delta.snapshotPartitions" = 2'})]}),"\n",(0,n.jsx)(t.admonition,{title:"Spark UI from Polynote",type:"tip",children:(0,n.jsxs)(t.p,{children:["On the right side of Polynote you find a link to the Spark UI for the current notebooks Spark session.\nIf it doesn't work, try to replace 127.0.0.1 with localhost. If it still doesn't work, replace with IP address of WSL (",(0,n.jsx)(t.code,{children:"wsl hostname -I"}),")."]})}),"\n",(0,n.jsx)(t.p,{children:"In the next step, we are going to take a look at keeping historical data..."})]})}function p(e={}){const{wrapper:t}={...(0,r.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(h,{...e})}):h(e)}},5162:(e,t,a)=>{a.d(t,{Z:()=>s});a(7294);var n=a(6010);const r={tabItem:"tabItem_Ymn6"};var o=a(5893);function s(e){let{children:t,hidden:a,className:s}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,n.Z)(r.tabItem,s),hidden:a,children:t})}},4866:(e,t,a)=>{a.d(t,{Z:()=>w});var n=a(7294),r=a(6010),o=a(2466),s=a(6550),i=a(469),l=a(1980),d=a(7392),c=a(12);function u(e){return n.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function h(e){const{values:t,children:a}=e;return(0,n.useMemo)((()=>{const e=t??function(e){return u(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:r}}=e;return{value:t,label:a,attributes:n,default:r}}))}(a);return function(e){const t=(0,d.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function p(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function m(e){let{queryString:t=!1,groupId:a}=e;const r=(0,s.k6)(),o=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,l._X)(o),(0,n.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(r.location.search);t.set(o,e),r.replace({...r.location,search:t.toString()})}),[o,r])]}function f(e){const{defaultValue:t,queryString:a=!1,groupId:r}=e,o=h(e),[s,l]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!p({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:o}))),[d,u]=m({queryString:a,groupId:r}),[f,b]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,o]=(0,c.Nk)(a);return[r,(0,n.useCallback)((e=>{a&&o.set(e)}),[a,o])]}({groupId:r}),g=(()=>{const e=d??f;return p({value:e,tabValues:o})?e:null})();(0,i.Z)((()=>{g&&l(g)}),[g]);return{selectedValue:s,selectValue:(0,n.useCallback)((e=>{if(!p({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),b(e)}),[u,b,o]),tabValues:o}}var b=a(2389);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var j=a(5893);function x(e){let{className:t,block:a,selectedValue:n,selectValue:s,tabValues:i}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,o.o5)(),c=e=>{const t=e.currentTarget,a=l.indexOf(t),r=i[a].value;r!==n&&(d(t),s(r))},u=e=>{let t=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const a=l.indexOf(e.currentTarget)+1;t=l[a]??l[0];break}case"ArrowLeft":{const a=l.indexOf(e.currentTarget)-1;t=l[a]??l[l.length-1];break}}t?.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":a},t),children:i.map((e=>{let{value:t,label:a,attributes:o}=e;return(0,j.jsx)("li",{role:"tab",tabIndex:n===t?0:-1,"aria-selected":n===t,ref:e=>l.push(e),onKeyDown:u,onClick:c,...o,className:(0,r.Z)("tabs__item",g.tabItem,o?.className,{"tabs__item--active":n===t}),children:a??t},t)}))})}function k(e){let{lazy:t,children:a,selectedValue:r}=e;const o=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=o.find((e=>e.props.value===r));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:o.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==r})))})}function v(e){const t=f(e);return(0,j.jsxs)("div",{className:(0,r.Z)("tabs-container",g.tabList),children:[(0,j.jsx)(x,{...e,...t}),(0,j.jsx)(k,{...e,...t})]})}function w(e){const t=(0,b.Z)();return(0,j.jsx)(v,{...e,children:u(e.children)},String(t))}},4565:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/files/application-part2-deltalake-0b93e26707372373b4afca4796d70cb3.conf"},1151:(e,t,a)=>{a.d(t,{Z:()=>i,a:()=>s});var n=a(7294);const r={},o=n.createContext(r);function s(e){const t=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),n.createElement(o.Provider,{value:t},e.children)}}}]);