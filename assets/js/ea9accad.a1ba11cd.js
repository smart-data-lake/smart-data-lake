"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[6234],{6473:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var n=a(4848),i=a(8453);const s={id:"dataQuality",title:"Data Quality"},o=void 0,r={id:"reference/dataQuality",title:"Data Quality",description:"Data quality is an important topic in data governance. To monitoring and improvement data quality, building data pipelines with implementing data quality measures is an important measure.",source:"@site/docs/reference/dataQuality.md",sourceDirName:"reference",slug:"/reference/dataQuality",permalink:"/docs/reference/dataQuality",draft:!1,unlisted:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/reference/dataQuality.md",tags:[],version:"current",frontMatter:{id:"dataQuality",title:"Data Quality"},sidebar:"tutorialSidebar",previous:{title:"Schema Viewer Usage",permalink:"/docs/reference/schema-viewer-navigation"},next:{title:"Execution Phases",permalink:"/docs/reference/executionPhases"}},c={},l=[{value:"Metrics",id:"metrics",level:2},{value:"Constraints",id:"constraints",level:2},{value:"Expectations",id:"expectations",level:2}];function d(e){const t={a:"a",code:"code",em:"em",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.p,{children:"Data quality is an important topic in data governance. To monitoring and improvement data quality, building data pipelines with implementing data quality measures is an important measure."}),"\n",(0,n.jsx)(t.p,{children:"SDLB provides the following features to improve data quality:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["Runtime ",(0,n.jsx)(t.strong,{children:"metrics"})," to monitor data pipeline output and track it over time"]}),"\n",(0,n.jsxs)(t.li,{children:["Row-level ",(0,n.jsx)(t.strong,{children:"constraints"})," to stop before writing wrong data to an output"]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Expectations"})," on dataset level to stop or warn on unplausible data"]}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"metrics",children:"Metrics"}),"\n",(0,n.jsx)(t.p,{children:"Every SDLB job collects metrics for each Action and output-DataObject. They are logged with the following log statements:"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.code,{children:"2020-07-21 11:36:34 INFO  CopyAction:105 - (Action~a) finished writing to DataObject~tgt1: job_duration=PT0.906S records_written=1 bytes_written=1142 num_tasks=1 stage=save"})}),"\n",(0,n.jsxs)(t.p,{children:["Metrics are also stored in the state file, and if you want to sync them to monitoring system in real-time, the StateListener can be implemented. It gets notified about action new events and metrics as soon as they are available. To configure state listeners set config attribute ",(0,n.jsx)(t.code,{children:"global.stateListeners = [{className = ...}]"}),"."]}),"\n",(0,n.jsx)(t.h2,{id:"constraints",children:"Constraints"}),"\n",(0,n.jsx)(t.p,{children:"Constraints can be defined on DataObjects to validate data on row-level. They work similar as database constraints and are validated by an Action when writing data into a DataObject. If a constraints validation fails, an Exception is thrown and the Action stops. No further data is written to the DataObject, and if the DataObject implements transactional write (Hive, DeltaLake, Jdbc, ...), no data at all is stored in the output DataObject."}),"\n",(0,n.jsx)(t.p,{children:"To define a constraint an arbitrary SQL expression is evaluated for each row, if it returns false the constraint validation fails. To return a meaningful error message you should configure a useful name and the columns that should be included in the text. See the following example:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'dataObjects {\n  testDataObject {\n    ...\n    constraints = [{\n      name = A should be smaller than B\n      description = "If A is bigger than B we have a problem because of ..."\n      expression = "a < b"\n      errorMsgCols = [id, a, b]\n    }]\n}\n'})}),"\n",(0,n.jsx)(t.h2,{id:"expectations",children:"Expectations"}),"\n",(0,n.jsx)(t.p,{children:"Expectations can be defined on DataObjects to monitor and validate a dataset after it has been written to the DataObject. An expectation collects a custom metric and compares its result against a given expectation condition. If the condition fails a warning can be logged or an error created which stops further processing."}),"\n",(0,n.jsxs)(t.p,{children:["Using the ",(0,n.jsx)(t.code,{children:"type = SQLExpectation"}),", a simple aggregation SQL expression is evaluated over the dataset. Further, an arbitrary SQL expression can be configured as expectation condition, which is compared against the metric value. If no expectation condition is given, the custom metric value is just logged in the ",(0,n.jsx)(t.code,{children:"finished writing to DataObject~xyz:..."})," log message, see example in ",(0,n.jsx)(t.a,{href:"#metrics",children:"Metrics"})," section."]}),"\n",(0,n.jsxs)(t.p,{children:["SDLB supports other expectation types, see ",(0,n.jsx)(t.a,{href:"http://smartdatalake.ch/json-schema-viewer/index.html",children:"Schema Viewer"})," for a list."]}),"\n",(0,n.jsxs)(t.p,{children:["By default, the expectation is evaluated against the currently processed dataset (scope=Job), which may consist of multiple partition values. Using ",(0,n.jsx)(t.code,{children:"scope=Job"})," results in one metric for the processed dataset. Using the option ",(0,n.jsx)(t.code,{children:"scope=JobPartition"}),", the scope can be changed to evalute against ",(0,n.jsx)(t.em,{children:"each"})," partition value in the dataset processed by the job. This results in one metric per processed partition. The option ",(0,n.jsx)(t.code,{children:"scope=All"})," would take all data in the output DataObject into account, and create one metric for it. Note that expectations with scope!=Job need reading the data from the output again after it has been written, while expectations with scope=Job can be calculated on the fly when using Spark as execution engine."]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'dataObjects {\n  testDataObject {\n    ...\n    expectations = [{\n      type = SQLExpectation\n      name = NoDataPct\n      description = "percentage of records having no data should be less than 0.1"\n      aggExpression = "count(data) / count(*)"\n      expectation = "< 0.1"\n    }]\n}\n'})})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,t,a)=>{a.d(t,{R:()=>o,x:()=>r});var n=a(6540);const i={},s=n.createContext(i);function o(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);