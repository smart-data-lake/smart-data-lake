"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[8779],{8554:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var i=o(4848),t=o(8453);const a={id:"troubleshooting",title:"Troubleshooting"},s=void 0,r={id:"reference/troubleshooting",title:"Troubleshooting",description:"If you have problems with the getting started guide, note that there's a separate troubleshooting section for that.",source:"@site/docs/reference/troubleshooting.md",sourceDirName:"reference",slug:"/reference/troubleshooting",permalink:"/docs/reference/troubleshooting",draft:!1,unlisted:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/reference/troubleshooting.md",tags:[],version:"current",frontMatter:{id:"troubleshooting",title:"Troubleshooting"},sidebar:"tutorialSidebar",previous:{title:"Testing",permalink:"/docs/reference/testing"}},l={},c=[{value:"Windows: missing winutils",id:"windows-missing-winutils",level:2},{value:"Windows: <code>/tmp/hive</code> is not writable",id:"windows-tmphive-is-not-writable",level:2},{value:"Windows: winutils.exe is not working correctly",id:"windows-winutilsexe-is-not-working-correctly",level:2},{value:"Java IllegalAccessError / InaccessibleObjectException (Java 17)",id:"java-illegalaccesserror--inaccessibleobjectexception-java-17",level:2},{value:"Java InvalidObjectException: ReflectiveOperationException during deserialization (Java 17)",id:"java-invalidobjectexception-reflectiveoperationexception-during-deserialization-java-17",level:2},{value:"Resources not copied",id:"resources-not-copied",level:2},{value:"Maven compile error: tools.jar",id:"maven-compile-error-toolsjar",level:2},{value:"How can I test Hadoop / HDFS locally ?",id:"how-can-i-test-hadoop--hdfs-locally-",level:2}];function d(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["If you have problems with the getting started guide, note that there's a separate ",(0,i.jsx)(n.a,{href:"/docs/getting-started/troubleshooting/common-problems",children:"troubleshooting section"})," for that."]})}),"\n",(0,i.jsx)(n.h2,{id:"windows-missing-winutils",children:"Windows: missing winutils"}),"\n",(0,i.jsxs)(n.p,{children:["Error:",(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.code,{children:"java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries"})]}),"\n",(0,i.jsxs)(n.p,{children:["Cause:",(0,i.jsx)(n.br,{}),"\n","The ",(0,i.jsx)(n.code,{children:"winutils.exe"})," executable can not be found."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Download hadoop winutils binaries (e.g ",(0,i.jsx)(n.a,{href:"https://github.com/cdarlint/winutils/archive/refs/heads/master.zip",children:"https://github.com/cdarlint/winutils/archive/refs/heads/master.zip"}),")"]}),"\n",(0,i.jsx)(n.li,{children:"Extract binaries for desired hadoop version into folder (e.g. hadoop-3.2.2\\bin)"}),"\n",(0,i.jsx)(n.li,{children:"Set HADOOP_HOME evironment variable (e.g. HADOOP_HOME=...\\hadoop-3.2.2).\nNote that the binary files need to be located at %HADOOP_HOME%\\bin!"}),"\n",(0,i.jsx)(n.li,{children:"Add %HADOOP_HOME%\\bin to PATH variable."}),"\n"]}),"\n",(0,i.jsxs)(n.h2,{id:"windows-tmphive-is-not-writable",children:["Windows: ",(0,i.jsx)(n.code,{children:"/tmp/hive"})," is not writable"]}),"\n",(0,i.jsxs)(n.p,{children:["Error:",(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.code,{children:"RuntimeException: Error while running command to get file permissions"}),(0,i.jsx)(n.br,{}),"\n","Solution:",(0,i.jsx)(n.br,{}),"\n","Change to ",(0,i.jsx)(n.code,{children:"%HADOOP_HOME%\\bin"})," and execute ",(0,i.jsx)(n.code,{children:"winutils chmod 777 /tmp/hive"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"windows-winutilsexe-is-not-working-correctly",children:"Windows: winutils.exe is not working correctly"}),"\n",(0,i.jsxs)(n.p,{children:["Error:",(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.code,{children:"winutils.exe - System Error The code execution cannot proceed because MSVCR100.dll was not found. Reinstalling the program may fix this problem."})]}),"\n",(0,i.jsx)(n.p,{children:"Other errors are also possible:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Similar error message when double clicking on winutils.exe (Popup)"}),"\n",(0,i.jsx)(n.li,{children:"Errors when providing a path to the configuration instead of a single configuration file"}),"\n",(0,i.jsx)(n.li,{children:"ExitCodeException exitCode=-1073741515 when executing SDL even though everything ran without errors"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Solution:",(0,i.jsx)(n.br,{}),"\n","Install VC++ Redistributable Package from Microsoft:",(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.a,{href:"http://www.microsoft.com/en-us/download/details.aspx?id=5555",children:"http://www.microsoft.com/en-us/download/details.aspx?id=5555"})," (x86)",(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.a,{href:"http://www.microsoft.com/en-us/download/details.aspx?id=14632",children:"http://www.microsoft.com/en-us/download/details.aspx?id=14632"})," (x64)"]}),"\n",(0,i.jsx)(n.h2,{id:"java-illegalaccesserror--inaccessibleobjectexception-java-17",children:"Java IllegalAccessError / InaccessibleObjectException (Java 17)"}),"\n",(0,i.jsx)(n.p,{children:"Symptom:\nStarting an SDLB pipeline fails with the following exception:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x343570b7) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x343570b7\n        at org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213)\n        ...\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Solution:\nJava 17 is more restrictive regarding usage of module exports. Unfortunately Spark uses classes from unexported packages. Packages can be exported manually. To fix the above exception add ",(0,i.jsx)(n.code,{children:"--add-exports java.base/sun.nio.ch=ALL-UNNAMED"})," to the java command line / IntelliJ VM Options, see also ",(0,i.jsx)(n.a,{href:"https://stackoverflow.com/questions/72230174/java-17-solution-for-spark-java-lang-noclassdeffounderror-could-not-initializ",children:"Stackoverflow"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"There might be additional InaccessibleObjectException erros depending on the function of Spark used:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'java.lang.reflect.InaccessibleObjectException: Unable to make field private final sun.nio.cs.StreamDecoder java.io.InputStreamReader.sd accessible: module java.base does not "opens java.io" to unnamed module @62e7f11d\n'})}),"\n",(0,i.jsxs)(n.p,{children:["To fix them add additional ",(0,i.jsx)(n.code,{children:"--add-opens"})," parameters to the command line / IntelliJ VM Options according to the list in ",(0,i.jsx)(n.a,{href:"https://github.com/apache/spark/blob/aa1ff3789e492545b07d84ac095fc4c39f7446c6/pom.xml#L312",children:"https://github.com/apache/spark/blob/aa1ff3789e492545b07d84ac095fc4c39f7446c6/pom.xml#L312"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"java-invalidobjectexception-reflectiveoperationexception-during-deserialization-java-17",children:"Java InvalidObjectException: ReflectiveOperationException during deserialization (Java 17)"}),"\n",(0,i.jsx)(n.p,{children:"Symptom:\nStarting an SDLB pipeline fails with the following exception:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"java.io.InvalidObjectException: ReflectiveOperationException during deserialization\n\tat java.base/java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:280)\n\t...\nCaused by: java.lang.reflect.InvocationTargetException\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t...\nCaused by: java.lang.IllegalArgumentException: too many arguments\n\tat java.base/java.lang.invoke.LambdaMetafactory.altMetafactory(LambdaMetafactory.java:511)\n    ...\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Solution:\nThis is a bug in Scala 2.12 together with Java 17. It is solved in Scala 2.12.17+, see also ",(0,i.jsx)(n.a,{href:"https://github.com/scala/bug/issues/12419",children:"https://github.com/scala/bug/issues/12419"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["To fix update Scala 2.12 version in pom.xml to latest release (e.g. ",(0,i.jsx)(n.code,{children:"<scala.version>2.12.18</scala.version>"}),")"]}),"\n",(0,i.jsx)(n.h2,{id:"resources-not-copied",children:"Resources not copied"}),"\n",(0,i.jsxs)(n.p,{children:["Symptom:",(0,i.jsx)(n.br,{}),"\n","Tests fail due to missing or outdated resources or the execution starts but can not find the feeds specified.\nIntelliJ might not copy the resource files to the target directory."]}),"\n",(0,i.jsxs)(n.p,{children:["Solution:",(0,i.jsx)(n.br,{}),"\n","Execute the maven goal ",(0,i.jsx)(n.code,{children:"resources:resources"})," (",(0,i.jsx)(n.code,{children:"mvn resources:resources"}),") manually after you changed any resource file."]}),"\n",(0,i.jsx)(n.h2,{id:"maven-compile-error-toolsjar",children:"Maven compile error: tools.jar"}),"\n",(0,i.jsxs)(n.p,{children:["Error:",(0,i.jsx)(n.br,{}),"\n",(0,i.jsx)(n.code,{children:"Could not find artifact jdk.tools:jdk.tools:jar:1.7 at specified path ..."})]}),"\n",(0,i.jsxs)(n.p,{children:["Context:",(0,i.jsx)(n.br,{}),"\n","Hadoop/Spark has a dependency on the tools.jar file which is installed as part of the JDK installation."]}),"\n",(0,i.jsx)(n.p,{children:"Possible Reasons:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Your system does not have a JDK installed (only a JRE).","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Fix: Make sure a JDK is installed and your PATH and JAVA_HOME environment variables are pointing to the JDK installation."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["You are using a Java 9 JDK or higher. The tools.jar has been removed in JDK 9. See: ",(0,i.jsx)(n.a,{href:"https://openjdk.java.net/jeps/220",children:"https://openjdk.java.net/jeps/220"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Fix: Downgrade your JDK to Java 8."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"how-can-i-test-hadoop--hdfs-locally-",children:"How can I test Hadoop / HDFS locally ?"}),"\n",(0,i.jsxs)(n.p,{children:["When using ",(0,i.jsx)(n.code,{children:"local://"})," URIs, file permissions on Windows, or certain actions, local Hadoop binaries are required."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Download your desired Apache Hadoop binary release from ",(0,i.jsx)(n.a,{href:"https://hadoop.apache.org/releases.html",children:"https://hadoop.apache.org/releases.html"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Extract the contents of the Hadoop distribution archive to a location of your choice, e.g., ",(0,i.jsx)(n.code,{children:"/path/to/hadoop"})," (Unix) or ",(0,i.jsx)(n.code,{children:"C:\\path\\to\\hadoop"})," (Windows)."]}),"\n",(0,i.jsxs)(n.li,{children:["Set the environment variable ",(0,i.jsx)(n.code,{children:"HADOOP_HOME=/path/to/hadoop"})," (Unix) or ",(0,i.jsx)(n.code,{children:"HADOOP_HOME=C:\\path\\to\\hadoop"})," (Windows)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Windows only"}),": Download a Hadoop winutils distribution corresponding to your Hadoop version from ",(0,i.jsx)(n.a,{href:"https://github.com/steveloughran/winutils",children:"https://github.com/steveloughran/winutils"})," (for newer Hadoop releases at: ",(0,i.jsx)(n.a,{href:"https://github.com/cdarlint/winutils",children:"https://github.com/cdarlint/winutils"}),") and extract the contents to ",(0,i.jsx)(n.code,{children:"%HADOOP_HOME%\\bin"}),"."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>r});var i=o(6540);const t={},a=i.createContext(t);function s(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);