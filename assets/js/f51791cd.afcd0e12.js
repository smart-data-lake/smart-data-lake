"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[1107],{1679:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>d});var a=t(4848),s=t(8453);const i={title:"Deployment on Databricks",description:"A brief example of deploying SDL on Databricks",slug:"sdl-databricks",authors:[{name:"Mandes Sch\xf6nherr",title:"Dr.sc.nat.",url:"https://github.com/mand35"}],tags:["Databricks","Cloud"],hide_table_of_contents:!1},r=void 0,o={permalink:"/blog/sdl-databricks",editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/blog/2022-04-07-SDL_databricks/2022-04-07-Databricks.md",source:"@site/blog/2022-04-07-SDL_databricks/2022-04-07-Databricks.md",title:"Deployment on Databricks",description:"A brief example of deploying SDL on Databricks",date:"2022-04-07T00:00:00.000Z",formattedDate:"April 7, 2022",tags:[{label:"Databricks",permalink:"/blog/tags/databricks"},{label:"Cloud",permalink:"/blog/tags/cloud"}],readingTime:5.9,hasTruncateMarker:!0,authors:[{name:"Mandes Sch\xf6nherr",title:"Dr.sc.nat.",url:"https://github.com/mand35"}],frontMatter:{title:"Deployment on Databricks",description:"A brief example of deploying SDL on Databricks",slug:"sdl-databricks",authors:[{name:"Mandes Sch\xf6nherr",title:"Dr.sc.nat.",url:"https://github.com/mand35"}],tags:["Databricks","Cloud"],hide_table_of_contents:!1},unlisted:!1,prevItem:{title:"Incremental historization using CDC and Airbyte MSSQL connector",permalink:"/blog/sdl-hist"},nextItem:{title:"Combine Spark and Snowpark to ingest and transform data in one pipeline",permalink:"/blog/sdl-snowpark"}},c={authorsImageUrls:[void 0]},d=[{value:"Lessons Learned",id:"lessons-learned",level:2}];function l(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.p,{children:["Many analytics applications are ported to the cloud, Data Lakes and Lakehouses in the cloud becoming more and more popular.\nThe ",(0,a.jsx)(n.a,{href:"https://databricks.com",children:"Databricks"})," platform provides an easy accessible and easy configurable way to implement a modern analytics platform.\nSmart Data Lake Builder on the other hand provides an open source, portable automation tool to load and transform the data."]}),"\n",(0,a.jsxs)(n.p,{children:["In this article the deployment of Smart Data Lake Builder (SDLB) on ",(0,a.jsx)(n.a,{href:"https://databricks.com",children:"Databricks"})," is described."]}),"\n",(0,a.jsx)(n.p,{children:"Before jumping in, it should be mentioned, that there are also many other methods to deploy SDLB in the cloud, e.g. using containers on Azure, Azure Kubernetes Service, Azure Synapse Clusters, Google Dataproc...\nThe present method provides the advantage of having many aspects taken care of by Databricks like Cluster management, Job scheduling and integrated data science notebooks.\nFurther, the presented SDLB pipeline is just a simple example, focusing on the integration into Databricks environment.\nSDLB provides a wide range of features and its full power is not revealed here."}),"\n",(0,a.jsx)(n.p,{children:"Let's get started:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://databricks.com",children:(0,a.jsx)(n.strong,{children:"Databricks"})})," accounts can be created as ",(0,a.jsx)(n.a,{href:"https://databricks.com/try-databricks",children:"Free Trial"})," or as ",(0,a.jsx)(n.a,{href:"https://community.databricks.com/s/login/SelfRegister",children:"Community Account"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Account and Workspace creation are described in detail ",(0,a.jsx)(n.a,{href:"https://docs.databricks.com/getting-started/account-setup.html",children:"here"}),", there are few hints and modifications presented below."]}),"\n",(0,a.jsx)(n.li,{children:"I selected AWS backend, but there are conceptually no differences to the other providers. If you already have an Azure, AWS or Google Cloud account/subscription this can be used, otherwise you can register a trial subscription there."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Workspace stack"})," is created using the Quickstart as described in the documentation. When finished launch the Workspace."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Databricks CLI"}),": for file transfer of configuration files, scripts and data, the ",(0,a.jsx)(n.a,{href:"https://docs.databricks.com/dev-tools/cli/index.html",children:"Databricks CLI"})," is installed locally. ",(0,a.jsx)(n.strong,{children:"Configure"}),' the CLI, using the Workspace URL and in the Workspace "Settings" -> "User Settings" -> "Access tokens" create a new token.']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Cluster"})," creation, in the Workspace open the ",(0,a.jsx)(n.em,{children:"Cluster"})," Creation form."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Spark version: When selecting the ",(0,a.jsx)(n.em,{children:"Databricks version"})," pay attention to the related Spark version.\nThis needs to match the Spark version we build SDLB with later. Here, ",(0,a.jsx)(n.code,{children:"10.4 LTS"})," is selected with ",(0,a.jsx)(n.code,{children:"Spark 3.2.1"})," and ",(0,a.jsx)(n.code,{children:"Scala 2.12"}),".\nAlternatively, SDLB can be build with a different Spark version, see also ",(0,a.jsx)(n.a,{href:"../../docs/architecture",children:"Architecture"})," for supported versions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["typesafe library version correction script: the workspace currently includes version 1.2.1 from com.typesafe",":config"," java library.\nSDLB relies on functions of a newer version (>1.3.0) of this library.\nThus, we provide a newer version of the com.typesafe",":config"," java library in an initialization script: ",(0,a.jsx)(n.em,{children:"Advanced options"})," -> ",(0,a.jsx)(n.em,{children:"Init Scripts"})," specify ",(0,a.jsx)(n.code,{children:"dbfs:/databricks/scripts/config-install.sh"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Further, the script needs to be created and uploaded. You can use the following script in a local terminal:"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"cat << EOF >> ./config-install.sh\n#!/bin/bash\nwget -O /databricks/jars/-----config-1.4.1.jar https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar\nEOF\ndatabricks fs mkdirs dbfs:/databricks/scripts\ndatabricks fs cp ./config-install.sh dbfs:/databricks/scripts/\n"})}),"\n",(0,a.jsx)(n.p,{children:"Alternatively, you can also use a Databricks notebook for the script upload by executing the following cell:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"%sh\ncat << EOF >> ./config-install.sh\n#!/bin/bash\nwget -O /databricks/jars/-----config-1.4.1.jar https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar\nEOF\nmkdir /dbfs/databricks/scripts\ncp ./config-install.sh /dbfs/databricks/scripts/\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Note: to double-check the library version I ran ",(0,a.jsx)(n.code,{children:"grep typesafe pom.xml"})," in the ",(0,a.jsx)(n.a,{href:"https://github.com/smart-data-lake/smart-data-lake.git",children:"SmartDataLake"})," source"]}),"\n",(0,a.jsxs)(n.p,{children:["Note: the added ",(0,a.jsx)(n.code,{children:"-----"})," will ensure that this ",(0,a.jsx)(n.code,{children:".jar"})," is preferred before the default Workspace Spark version (which starts with ",(0,a.jsx)(n.code,{children:"----"}),").\nIf you are curious you could double-check e.g. with a Workspace Shell Notebook running ",(0,a.jsx)(n.code,{children:"ls /databricks/jars/*config*"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"fat-jar"}),":\nWe need to provide the SDLB sources and all required libraries. Therefore, we compile and pack the Scala code into a Jar including the dependencies. We use the ",(0,a.jsx)(n.a,{href:"https://github.com/smart-data-lake/getting-started.git",children:"getting-started"})," as dummy project, which itself pulls the SDLB sources."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["download the ",(0,a.jsx)(n.a,{href:"https://github.com/smart-data-lake/getting-started.git",children:"getting-started"})," source and build it with the ",(0,a.jsx)(n.code,{children:"-P fat-jar"})," profile"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'podman run -v ${PWD}:/mnt/project -v ${PWD}/.mvnrepo:/mnt/.mvnrepo maven:3.6.0-jdk-11-slim -- mvn -DskipTests  -P fat-jar  -f /mnt/project/pom.xml "-Dmaven.repo.local=/mnt/.mvnrepo" package\n'})}),"\n",(0,a.jsxs)(n.p,{children:["General build instructions can be found in the ",(0,a.jsx)(n.a,{href:"../../docs/getting-started/setup#compile-scala-classes",children:"getting-started"})," documentation.\nTherewith, the file ",(0,a.jsx)(n.code,{children:"target/getting-started-1.0-jar-with-dependencies.jar"})," is created.\nThe ",(0,a.jsx)(n.em,{children:"fat-jar"})," profile will include all required dependencies. The profile is defined in the ",(0,a.jsx)(n.a,{href:"https://github.com/smart-data-lake/smart-data-lake",children:"smart-data-lake"})," pom.xml."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"upload files"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:['JAR: in the "Workspace" -> your user -> create a directory ',(0,a.jsx)(n.code,{children:"jars"}),' and "import" the library using the link in "(To import a library, such as a jar or egg, click here)" and select the above created fat-jar to upload. As a result the jar will be listed in the Workspace directory.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"SDLB application"}),": As an example a dataset from Airbnb NYC will be downloaded from Github, first written into a CSV file and later partially ported into a table. Therefore, the pipeline is defined first locally in a new file ",(0,a.jsx)(n.code,{children:"application.conf"}),":"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'dataObjects {\n  ext-ab-csv-web {\n    type = WebserviceFileDataObject\n    url = "https://raw.githubusercontent.com/adishourya/Airbnb/master/new-york-city-airbnb-open-data/AB_NYC_2019.csv"\n    followRedirects = true\n    readTimeoutMs=200000\n  }\n  stg-ab {\n    type = CsvFileDataObject\n    schema = """id integer, name string, host_id integer, host_name string, neighbourhood_group string, neighbourhood string, latitude double, longitude double, room_type  string, price integer, minimum_nights integer, number_of_reviews integer, last_review timestamp, reviews_per_month double, calculated_host_listings_count integer, availability_365 integer"""\n    path = "file:///dbfs/data/~{id}"\n  }\n  int-ab {\n    type = DeltaLakeTableDataObject\n    path = "~{id}"\n    table {\n      db = "default"\n      name = "int_ab"\n      primaryKey = [id]\n    }\n  }\n}\n\nactions {\n  loadWeb2Csv {\n    type = FileTransferAction\n    inputId = ext-ab-csv-web\n    outputId = stg-ab\n    metadata {\n      feed = download\n    }\n  }\n  loadCsvLoc2Db {\n    type = CopyAction\n    inputId = stg-ab\n    outputId = int-ab\n    transformers = [{\n      type = SQLDfTransformer\n      code = "select id, name, host_id,host_name,neighbourhood_group,neighbourhood,latitude,longitude from stg_ab"\n    }]\n    metadata {\n      feed = copy\n    }\n  }\n}\n'})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"upload using Databricks CLI"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"databricks fs mkdirs dbfs:/conf/\ndatabricks fs cp application.conf dbfs:/conf/application.conf\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Job creation"}),":\nHere, the Databricks job gets defined, specifying the SDL library and, the entry point and the arguments. Here we specify only the download feed.\nTherefore, open in the sidebar ",(0,a.jsx)(n.em,{children:"Jobs"})," -> ",(0,a.jsx)(n.em,{children:"Create Job"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Type"}),": ",(0,a.jsx)(n.code,{children:"JAR"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Main Class"}),": ",(0,a.jsx)(n.code,{children:"io.smartdatalake.app.LocalSmartDataLakeBuilder"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"add"})," ",(0,a.jsx)(n.em,{children:"Dependent Libraries"}),': "Workspace" -> select the file previously uploaded "getting-started..." file in the "jars" directory\n',(0,a.jsx)(n.img,{alt:"jar select",src:t(2180).A+"",width:"630",height:"652"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cluster"})," select the cluster created above with the corrected typesafe library"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Parameters"}),": ",(0,a.jsx)(n.code,{children:'["-c", "file:///dbfs/conf/", "--feed-sel", "download"]'}),', which specifies the location of the SDLB configuration and selects the feed "download"\n',(0,a.jsx)(n.img,{alt:"download task",src:t(1990).A+"",width:"589",height:"641"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Launch"}),' the job:\nLaunch the job.\nWhen finished in the "Runs" section of that job we can verify the successful run status']}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Results"}),"\nAfter running the SDLB pipeline the data should be downloaded into the staging file ",(0,a.jsx)(n.code,{children:"stg_ab/result.csv"})," and selected parts into the table ",(0,a.jsx)(n.code,{children:"int_ab"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["csv file: in the first step we downloaded the CSV file. This can be verified, e.g. by inspecting the data directory in the Databricks CLI using ",(0,a.jsx)(n.code,{children:"databricks fs ls dbfs:/data/stg-ab"})," or running in a Workspace shell notebook ",(0,a.jsx)(n.code,{children:"ls /dbfs/data/stg-ab"})]}),"\n",(0,a.jsxs)(n.li,{children:["database: in the second phase specific columns are put into the database. This can be verified in the Workspace -> Data -> default -> int_ab\n",(0,a.jsx)(n.img,{alt:"select table",src:t(8869).A+"",width:"726",height:"476"}),"\n",(0,a.jsx)(n.img,{alt:"table",src:t(5718).A+"",width:"1694",height:"821"})]}),"\n"]}),"\n",(0,a.jsx)(n.admonition,{type:"info",children:(0,a.jsxs)(n.p,{children:["Note that our final table was defined as ",(0,a.jsx)(n.code,{children:"DeltaLakeTableDataObject"}),".\nWith that, Smart Data Lake Builder automatically generates a Delta Lake Table in your Databricks workspace."]})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"lessons-learned",children:"Lessons Learned"}),"\n",(0,a.jsx)(n.p,{children:"There are a few steps necessary, including building and uploading SDLB.\nFurther, we need to be careful with the used versions of the underlying libraries.\nWith these few steps we can reveal the power of SDLB and Databricks, creating a portable and reproducible pipeline into a Databricks Lakehouse."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},2180:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/add_library-004c4b45355447e0191352a0c3d1e26c.png"},1990:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/download_task-104bd82fd84e5fb593b161282586942e.png"},8869:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/select_table-7e7e1d0657aff229863bfe5d87b16c2e.png"},5718:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/table-f62d6019e5cbbb9c404209061ceb6b80.png"},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var a=t(6540);const s={},i=a.createContext(s);function r(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);