"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[5927],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>g});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),u=p(a),m=r,g=u["".concat(s,".").concat(m)]||u[m]||d[m]||i;return a?n.createElement(g,l(l({ref:t},c),{},{components:a})):n.createElement(g,l({ref:t},c))}));function g(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,l=new Array(i);l[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[u]="string"==typeof e?e:r,l[1]=o;for(var p=2;p<i;p++)l[p]=a[p];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},1527:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const i={id:"architecture",title:"Architecture"},l=void 0,o={unversionedId:"architecture",id:"architecture",title:"Architecture",description:"Smart Data Lake Builder (SDLB) is basically a Java application which is started on the command line.",source:"@site/docs/architecture.md",sourceDirName:".",slug:"/architecture",permalink:"/docs/architecture",draft:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/architecture.md",tags:[],version:"current",frontMatter:{id:"architecture",title:"Architecture"},sidebar:"docs",previous:{title:"Features",permalink:"/docs/features"},next:{title:"Technical Setup",permalink:"/docs/getting-started/setup"}},s={},p=[{value:"Basic Requirements",id:"basic-requirements",level:2},{value:"Versions and supported configuration",id:"versions-and-supported-configuration",level:2},{value:"Release Notes",id:"release-notes",level:2},{value:"Logging",id:"logging",level:2}],c={toc:p};function u(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Smart Data Lake Builder (SDLB) is basically a Java application which is started on the ",(0,r.kt)("a",{parentName:"p",href:"/docs/reference/commandLine"},"command line"),".\nIt can run in many environments and platforms like a Databricks cluster, Azure Synapse, Google Dataproc, and also on your local machine, see ",(0,r.kt)("a",{parentName:"p",href:"getting-started/setup"},"Getting Started"),"."),(0,r.kt)("p",null,"Find below an overview of requirements, versions and supported configurations."),(0,r.kt)("h2",{id:"basic-requirements"},"Basic Requirements"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Needs Java 8+ to run"),(0,r.kt)("li",{parentName:"ul"},"Uses Hadoop Java library to read local and remote files (S3, ADLS, HDFS, ...)"),(0,r.kt)("li",{parentName:"ul"},"Is programmed in Scala"),(0,r.kt)("li",{parentName:"ul"},"Uses Maven 3+ as build system")),(0,r.kt)("h2",{id:"versions-and-supported-configuration"},"Versions and supported configuration"),(0,r.kt)("p",null,"SDLB currently maintains the following two major versions, which are published as Maven artifacts on maven central:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"SDL Version"),(0,r.kt)("th",{parentName:"tr",align:null},"Java/Scala/Hadoop Version"),(0,r.kt)("th",{parentName:"tr",align:null},"File Engine"),(0,r.kt)("th",{parentName:"tr",align:null},"Spark Engine"),(0,r.kt)("th",{parentName:"tr",align:null},"Snowflake/Snowpark Engine"),(0,r.kt)("th",{parentName:"tr",align:null},"Comments"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1.x, branch master/develop-spark2"),(0,r.kt)("td",{parentName:"tr",align:null},"Java 8, Scala 2.11 & 2.12"),(0,r.kt)("td",{parentName:"tr",align:null},"Hadoop 2.7.x"),(0,r.kt)("td",{parentName:"tr",align:null},"Spark 2.4.x"),(0,r.kt)("td",{parentName:"tr",align:null},"not supported"),(0,r.kt)("td",{parentName:"tr",align:null},"Delta lake has limited functionality in Spark 2.x")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2.x, branch master/develop-spark3"),(0,r.kt)("td",{parentName:"tr",align:null},"Java 8+, Scala 2.12"),(0,r.kt)("td",{parentName:"tr",align:null},"Hadoop 3.3.x (2.7.x)"),(0,r.kt)("td",{parentName:"tr",align:null},"Spark 3.2.x (3.1.x)"),(0,r.kt)("td",{parentName:"tr",align:null},"Snowpark 1.2.x"),(0,r.kt)("td",{parentName:"tr",align:null},"Delta lake, spark-snowflake and spark-extensions need specific library versions matching the corresponding spark minor version")))),(0,r.kt)("p",null,"Configurations using alternative versions mentioned in parentheses can be build manually by setting corresponding maven profiles."),(0,r.kt)("p",null,"It's possible to customize dependencies and make Smart Data Lake Builder work with other version combinations, but this needs manual tuning of dependencies in your own maven project."),(0,r.kt)("p",null,"In general, Java library versions are held as close as possible to the ones used in the corresponding Spark version."),(0,r.kt)("h2",{id:"release-notes"},"Release Notes"),(0,r.kt)("p",null,"See SDBL Release Notes including breaking changes on ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/smart-data-lake/smart-data-lake/releases"},"Github")),(0,r.kt)("h2",{id:"logging"},"Logging"),(0,r.kt)("p",null,"By default, SDLB uses the logging libraries included in the corresponding Spark version. This is Log4j 1.2.x for Spark 2.4.x up to Spark 3.2.x.\nStarting from Spark 3.3.x it will use Log4j 2.x, see ",(0,r.kt)("a",{parentName:"p",href:"https://issues.apache.org/jira/browse/SPARK-6305"},"SPARK-6305"),"."),(0,r.kt)("p",null,"You can customize logging dependencies manually by creating your own maven project."))}u.isMDXComponent=!0}}]);