"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[1099],{8682:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>o,contentTitle:()=>i,default:()=>h,frontMatter:()=>s,metadata:()=>l,toc:()=>d});var n=a(4848),r=a(8453);const s={title:"Delta Lake - a better data format"},i=void 0,l={id:"getting-started/part-2/delta-lake-format",title:"Delta Lake - a better data format",description:"Goal",source:"@site/docs/getting-started/part-2/delta-lake-format.md",sourceDirName:"getting-started/part-2",slug:"/getting-started/part-2/delta-lake-format",permalink:"/docs/getting-started/part-2/delta-lake-format",draft:!1,unlisted:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/getting-started/part-2/delta-lake-format.md",tags:[],version:"current",frontMatter:{title:"Delta Lake - a better data format"},sidebar:"tutorialSidebar",previous:{title:"Industrializing our data pipeline",permalink:"/docs/getting-started/part-2/industrializing"},next:{title:"Keeping historical data",permalink:"/docs/getting-started/part-2/historical-data"}},o={},d=[{value:"Goal",id:"goal",level:2},{value:"File formats",id:"file-formats",level:2},{value:"Catalog",id:"catalog",level:2},{value:"Transactions",id:"transactions",level:2},{value:"DeltaLakeTableDataObject",id:"deltalaketabledataobject",level:2},{value:"Reading Delta Lake Format with Spark",id:"reading-delta-lake-format-with-spark",level:2}];function c(e){const t={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h2,{id:"goal",children:"Goal"}),"\n",(0,n.jsx)(t.p,{children:"Up to now we have used CSV with CsvFileDataObject as file format. We will switch to a more modern data format in this step which supports a catalog, compression and even transactions."}),"\n",(0,n.jsx)(t.h2,{id:"file-formats",children:"File formats"}),"\n",(0,n.jsx)(t.p,{children:"Smart Data Lake Builder has built in support for many data formats and technologies.\nAn important one is storing files on a Hadoop filesystem, supporting standard file formats such as CSV, Json, Avro or Parquet.\nIn Part 1 we have used CSV through the CsvFileDataObject. CSV files can be easily checked in an editor or Excel, but the format also has many problems, e.g. support of multi-line strings or lack of data type definition.\nOften Parquet format is used, as it includes a schema definition and is very space efficient through its columnar compression."}),"\n",(0,n.jsx)(t.h2,{id:"catalog",children:"Catalog"}),"\n",(0,n.jsx)(t.p,{children:"Just storing files on Hadoop filesystem makes it difficult to use them in a SQL engine such as Spark SQL. You need a metadata layer on top which stores table definitions. This is also called a metastore or catalog.\nIf you start a Spark session, a configuration to connect to an external catalog can be set, or otherwise Spark creates an internal catalog for the session.\nWe could register our CSV files in this catalog by creating a table via a DDL-statement, including the definition of all columns, a path and the format of our data.\nBut you could also directly create and write into a table by using Spark Hive tables.\nSDLB supports this by the HiveTableDataObject. It always uses Parquet file format in the background as a best practice, although Hive tables could also be created on top of CSV files."}),"\n",(0,n.jsx)(t.admonition,{type:"info",children:(0,n.jsx)(t.p,{children:"Hive is a Metadata layer and SQL engine on top of a Hadoop filesystem. Spark uses the metadata layer of Hive, but implements its own SQL engine."})}),"\n",(0,n.jsx)(t.h2,{id:"transactions",children:"Transactions"}),"\n",(0,n.jsx)(t.p,{children:"Hive tables with Parquet format are lacking transactions. This means for example that writing and reading the table at the same time could result in failure or empty results.\nIn consequence"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"consecutive jobs need to be synchronized"}),"\n",(0,n.jsx)(t.li,{children:"it's not recommended having end-user accessing the table while data processing jobs are running"}),"\n",(0,n.jsx)(t.li,{children:"update and deletes are not supported"}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:["There are other options like classical databases which always had a metadata layer, offer transactions but don't integrate easily with Hive metastore and cheap, scalable Hadoop file storage.\nNevertheless, SDLB supports classical databases through the JdbcTableDataObject.\nFortunately there is a new technology called ",(0,n.jsx)(t.em,{children:"Open Table Formats"})," with implementations like Delta Lake (see also ",(0,n.jsx)(t.a,{href:"https://delta.io/",children:"delta.io"}),"), Iceberg or Hudi.\nThey integrate tables into a Hive metastore, supports transactions and store Parquet files and a transaction log on hadoop filesystems.\nSDLB supports provides a DeltaLakeTableDataObject and IcebergTableDataObject.\nWe are going to use DeltaLakeTableDataObject for our airport and departure data now."]}),"\n",(0,n.jsx)(t.h2,{id:"deltalaketabledataobject",children:"DeltaLakeTableDataObject"}),"\n",(0,n.jsxs)(t.p,{children:["Switching to Delta Lake format is easy with SDLB, just replace ",(0,n.jsx)(t.code,{children:"CsvFileDataObject"})," with ",(0,n.jsx)(t.code,{children:"DeltaLakeTableDataObject"})," and define the table's db and name.\nLet's start by changing the existing definitions for ",(0,n.jsx)(t.code,{children:"int-airports"}),", ",(0,n.jsx)(t.code,{children:"btl-departures-arrivals-airports"})," and ",(0,n.jsx)(t.code,{children:"btl-distances"}),"."]}),"\n",(0,n.jsxs)(t.p,{children:["Update DataObject ",(0,n.jsx)(t.code,{children:"int-airports"})," in airports.conf:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'  int-airports {\n    type = DeltaLakeTableDataObject\n    path = "~{id}"\n    table = {\n      db = "default"\n      name = "int_airports"\n    }\n  }\n'})}),"\n",(0,n.jsxs)(t.p,{children:["Update DataObjects ",(0,n.jsx)(t.code,{children:"btl-departures-arrivals-airports"})," and ",(0,n.jsx)(t.code,{children:"btl-distances"})," in btl.conf:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'  btl-departures-arrivals-airports {\n    type = DeltaLakeTableDataObject\n    path = "~{id}"\n    table {\n      db = "default"\n      name = "btl_departures_arrivals_airports"\n    }\n  }\n  \n  btl-distances {\n    type = DeltaLakeTableDataObject\n    path = "~{id}"\n    table {\n      db = "default"\n      name = "btl_distances"\n    }\n  }\n'})}),"\n",(0,n.jsxs)(t.p,{children:["Then create a new, similar data object ",(0,n.jsx)(t.code,{children:"int-departures"})," in departures.conf:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'  int-departures {\n    type = DeltaLakeTableDataObject\n    path = "~{id}"\n    table = {\n      db = default\n      name = int_departures\n    }\n  }\n'})}),"\n",(0,n.jsxs)(t.p,{children:["Next, create a new action ",(0,n.jsx)(t.code,{children:"prepare-departures"})," in the ",(0,n.jsx)(t.code,{children:"actions"})," section of departures.conf to fill the new table with the data:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"  prepare-departures {\n    type = CopyAction\n    inputId = stg-departures\n    outputId = int-departures\n    metadata {\n      feed = compute\n    }\n  }\n"})}),"\n",(0,n.jsxs)(t.p,{children:["Finally, adapt the action definition for ",(0,n.jsx)(t.code,{children:"join-departures-airports"})," in btl.conf:"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["change ",(0,n.jsx)(t.code,{children:"stg-departures"})," to ",(0,n.jsx)(t.code,{children:"int-departures"})," in inputIds"]}),"\n",(0,n.jsxs)(t.li,{children:["change ",(0,n.jsx)(t.code,{children:"stg_departures"})," to ",(0,n.jsx)(t.code,{children:"int_departures"})," in the first SQLDfsTransformer (watch out, you need to replace the string 4 times)"]}),"\n"]}),"\n",(0,n.jsx)(t.admonition,{title:"Explanation",type:"info",children:(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["We changed ",(0,n.jsx)(t.code,{children:"int-airports"}),", ",(0,n.jsx)(t.code,{children:"btl-departures-arrivals-airports"})," and ",(0,n.jsx)(t.code,{children:"btl-distances"})," from CSV to Delta Lake format"]}),"\n",(0,n.jsxs)(t.li,{children:["Created an additional table ",(0,n.jsx)(t.code,{children:"int-departures"})]}),"\n",(0,n.jsxs)(t.li,{children:["Created an action ",(0,n.jsx)(t.code,{children:"prepare-departures"})," to fill the new integration layer table ",(0,n.jsx)(t.code,{children:"int-departures"})]}),"\n",(0,n.jsxs)(t.li,{children:["Adapted the existing action ",(0,n.jsx)(t.code,{children:"join-departures-airports"})," to use the new table ",(0,n.jsx)(t.code,{children:"int-departures"})]}),"\n"]})}),"\n",(0,n.jsxs)(t.p,{children:["To run our data pipeline, first delete ",(0,n.jsx)(t.code,{children:"data/int-*"})," and ",(0,n.jsx)(t.code,{children:"data/btl-*"})," - otherwise DeltaLakeTableDataObject will fail because of existing files in different format.\nThen you can execute the usual ",(0,n.jsx)(t.em,{children:"./startJob.sh"})," command for ",(0,n.jsx)(t.code,{children:"compute"})," feed:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"./startJob.sh -c /mnt/config --feed-sel 'compute'\n"})}),"\n",(0,n.jsxs)(t.p,{children:["Getting an error like ",(0,n.jsx)(t.code,{children:"io.smartdatalake.util.webservice.WebserviceException: Read timed out"}),"? Check the list of ",(0,n.jsx)(t.a,{href:"../troubleshooting/common-problems",children:"Common Problems"})," for a workaround."]}),"\n",(0,n.jsxs)(t.p,{children:["Getting an error like ",(0,n.jsx)(t.code,{children:"DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table default.int_departures doesn't exist."}),"?\nThis happens when you delete the files of a Delta Table on the filesystem, but the table is still registered in the metastore.\nSolution: delete data/metastore_db directory to reset the metastore completely, or issue a ",(0,n.jsx)(t.code,{children:'spark.sql("DROP TABLE default.int_departures")'})," in spark-shell. See below on how to start spark-shell."]}),"\n",(0,n.jsx)(t.h2,{id:"reading-delta-lake-format-with-spark",children:"Reading Delta Lake Format with Spark"}),"\n",(0,n.jsxs)(t.p,{children:["Checking our results gets more complicated now - we can't just open delta lake format in a text editor like we used to do for CSV files.\nBut we can now use SQL to query our results, that is even better.\nWe will use a Spark session for this, i.e. by starting a spark-shell.\nAlternatively one could use notebooks like Jupyter, Polynote or Databricks for this.\nSee our ",(0,n.jsx)(t.a,{href:"https://github.com/smart-data-lake/polynote-lab",children:"Polynote-lab"})," for a local solution."]}),"\n",(0,n.jsx)(t.p,{children:"Execute the following command to start a spark-shell. Note that his starts a spark-shell having SDLB and your Apps code in the class-path, using the same data/metastore_db as metastore like ./startJob.sh."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"./sparkShell.sh\n"})}),"\n",(0,n.jsx)(t.p,{children:'List existing tables in schema "default":'}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'spark.catalog.listTables("default").show(false)\n'})}),"\n",(0,n.jsx)(t.p,{children:"should show"}),"\n",(0,n.jsx)(t.p,{children:"+--------------------------------+-------------+---------+-----------+---------+-----------+\n|name                            |catalog      |namespace|description|tableType|isTemporary|\n+--------------------------------+-------------+---------+-----------+---------+-----------+\n|btl_departures_arrivals_airports|spark_catalog|[default]|NULL       |EXTERNAL |false      |\n|btl_distances                   |spark_catalog|[default]|NULL       |EXTERNAL |false      |\n|int_airports                    |spark_catalog|[default]|NULL       |EXTERNAL |false      |\n|int_departures                  |spark_catalog|[default]|NULL       |EXTERNAL |false      |\n+--------------------------------+-------------+---------+-----------+---------+-----------+"}),"\n",(0,n.jsx)(t.p,{children:"List schema and data of table btl_distances:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:'spark.table("default.btl_distances").printSchema\nspark.table("default.btl_distances").limit(5).show\n'})}),"\n",(0,n.jsx)(t.p,{children:"should look similar to"}),"\n",(0,n.jsx)(t.p,{children:"root\n|-- estdepartureairport: string (nullable = true)\n|-- estarrivalairport: string (nullable = true)\n|-- arr_name: string (nullable = true)\n|-- arr_latitude_deg: string (nullable = true)\n|-- arr_longitude_deg: string (nullable = true)\n|-- dep_name: string (nullable = true)\n|-- dep_latitude_deg: string (nullable = true)\n|-- dep_longitude_deg: string (nullable = true)\n|-- distance: double (nullable = true)\n|-- could_be_done_by_rail: boolean (nullable = true)"}),"\n",(0,n.jsx)(t.p,{children:"+-------------------+-----------------+--------------------+----------------+-----------------+------------+----------------+-----------------+------------------+---------------------+\n|estdepartureairport|estarrivalairport|            arr_name|arr_latitude_deg|arr_longitude_deg|    dep_name|dep_latitude_deg|dep_longitude_deg|          distance|could_be_done_by_rail|\n+-------------------+-----------------+--------------------+----------------+-----------------+------------+----------------+-----------------+------------------+---------------------+\n|               LSZB|             LEAL|Alicante-Elche Mi...|         38.2822|        -0.558156|Bern Airport|       46.912868|         7.498512|1163.0363811380448|                false|\n|               LSZB|             LFLY|   Lyon Bron Airport|       45.727947|         4.943991|Bern Airport|       46.912868|         7.498512|236.29247119523134|                 true|\n|               LSZB|             LFMD|Cannes-Mandelieu ...|       43.547998|         6.955176|Bern Airport|       46.912868|         7.498512|376.56517532159864|                 true|\n|               LSZB|             LFMN|Nice-C\xf4te d'Azur ...|       43.658401|          7.21587|Bern Airport|       46.912868|         7.498512|362.55441725133795|                 true|\n|               LSZB|             LGRP|    Diagoras Airport|       36.405399|        28.086201|Bern Airport|       46.912868|         7.498512| 2061.217367266584|                false|\n+-------------------+-----------------+--------------------+----------------+-----------------+------------+----------------+-----------------+------------------+---------------------+"}),"\n",(0,n.jsxs)(t.p,{children:["You can also use SDLB's scala interface to access DataObjects and Actions in the spark-shell. The interface is generated through ",(0,n.jsx)(t.code,{children:"./buildJob.sh"})," and it is important to re-execute buildJob.sh after changes on configurations files before starting the spark-shell."]}),"\n",(0,n.jsx)(t.p,{children:"Now you can show or drop DataObjects as follows. Note that DataObjectId is converted from hyphen separated to camelCase style for Java/Scala compatibility."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"sdlb.dataObjects.intDepartures.printSchema\nsdlb.dataObjects.intDepartures.get.limit(5).show\nsdlb.dataObjects.intDepartures.dataObject.dropTable\n"})}),"\n",(0,n.jsxs)(t.p,{children:["You can find a detailed description of SDLB's scala interface ",(0,n.jsx)(t.a,{href:"../../reference/notebookCatalog",children:"here"})]}),"\n",(0,n.jsxs)(t.p,{children:["To automatically initialize SDLB's scala interface on spark-shell startup, uncomment the corresponding code in ",(0,n.jsx)(t.code,{children:"shell.scala"}),"."]}),"\n",(0,n.jsxs)(t.admonition,{title:"Delta Lake tuning",type:"tip",children:[(0,n.jsxs)(t.p,{children:["You might have seen that our data pipeline with DeltaTableDataObject runs a Spark stage with 50 tasks several times.\nThis is delta lake reading its transaction log with Spark. For our data volume, 50 tasks are way too much.\nYou can reduce the number of snapshot partitions to speed up the execution by setting the following Spark property in your ",(0,n.jsx)(t.code,{children:"global.conf"})," under ",(0,n.jsx)(t.code,{children:"global.spark-options"}),":"]}),(0,n.jsx)(t.p,{children:'"spark.databricks.delta.snapshotPartitions" = 2'})]}),"\n",(0,n.jsx)(t.admonition,{title:"Hocon file splitting - global.conf",type:"tip",children:(0,n.jsx)(t.p,{children:"Note that SDLB config files can be split arbitrarily. They will be merged by the Hocon parser.\nAn SDLB best practice is to use a separate global.conf file for the global configuration."})}),"\n",(0,n.jsxs)(t.p,{children:["Your departures/airports/btl.conf should now look like the files ending with ",(0,n.jsx)(t.code,{children:"part-2a-solution"})," in ",(0,n.jsx)(t.a,{href:"https://github.com/smart-data-lake/getting-started/tree/master/config",children:"this directory"}),".\nFeel free to play around."]}),"\n",(0,n.jsx)(t.p,{children:"In the next step, we are going to take a look at keeping historical data..."})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},8453:(e,t,a)=>{a.d(t,{R:()=>i,x:()=>l});var n=a(6540);const r={},s=n.createContext(r);function i(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);