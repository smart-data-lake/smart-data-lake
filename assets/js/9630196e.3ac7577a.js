"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[7014],{3905:(e,t,n)=>{n.d(t,{Zo:()=>s,kt:()=>g});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),u=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},s=function(e){var t=u(e.components);return a.createElement(l.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,s=c(e,["components","mdxType","originalType","parentName"]),p=u(n),m=i,g=p["".concat(l,".").concat(m)]||p[m]||d[m]||r;return n?a.createElement(g,o(o({ref:t},s),{},{components:n})):a.createElement(g,o({ref:t},s))}));function g(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=m;var c={};for(var l in t)hasOwnProperty.call(t,l)&&(c[l]=t[l]);c.originalType=e,c[p]="string"==typeof e?e:i,o[1]=c;for(var u=2;u<r;u++)o[u]=n[u];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},211:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>c,toc:()=>u});var a=n(7462),i=(n(7294),n(3905));const r={id:"executionEngines",title:"Execution Engines"},o=void 0,c={unversionedId:"reference/executionEngines",id:"reference/executionEngines",title:"Execution Engines",description:"An execution engine is a technology/library used by SDLB to transform data. SDLB supports different execution engines and is able to combine different execution engines in the same data pipeline / job.",source:"@site/docs/reference/executionEngines.md",sourceDirName:"reference",slug:"/reference/executionEngines",permalink:"/docs/reference/executionEngines",draft:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/reference/executionEngines.md",tags:[],version:"current",frontMatter:{id:"executionEngines",title:"Execution Engines"},sidebar:"docs",previous:{title:"Execution Phases",permalink:"/docs/reference/executionPhases"},next:{title:"Execution Modes",permalink:"/docs/reference/executionModes"}},l={},u=[{value:"Connecting different execution engines",id:"connecting-different-execution-engines",level:3},{value:"Schema propagation",id:"schema-propagation",level:3},{value:"Determining execution engine to use in &quot;Generic DataFrame API&quot; Actions",id:"determining-execution-engine-to-use-in-generic-dataframe-api-actions",level:3},{value:"Execution Engines vs Execution Environments",id:"execution-engines-vs-execution-environments",level:3}],s={toc:u};function p(e){let{components:t,...r}=e;return(0,i.kt)("wrapper",(0,a.Z)({},s,r,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"An execution engine is a technology/library used by SDLB to transform data. SDLB supports different execution engines and is able to combine different execution engines in the same data pipeline / job.\nThe data structure used to transport data between DataObjects and Actions is called a SubFeed.\nEach Execution Engine has Subfeeds, Actions and Dataobjects associated with it. "),(0,i.kt)("p",null,"Currently SDLB supports the following execution engines:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Category"),(0,i.kt)("th",{parentName:"tr",align:null},"Execution Engine"),(0,i.kt)("th",{parentName:"tr",align:null},"SubFeed Name"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"),(0,i.kt)("th",{parentName:"tr",align:null},"Supported Actions"),(0,i.kt)("th",{parentName:"tr",align:null},"Supported DataObjects"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Java-Byte-Stream"),(0,i.kt)("td",{parentName:"tr",align:null},"File Engine"),(0,i.kt)("td",{parentName:"tr",align:null},"FileSubFeed"),(0,i.kt)("td",{parentName:"tr",align:null},"Transfer Byte-Streams without further knowledge about their content"),(0,i.kt)("td",{parentName:"tr",align:null},"FileTransferAction, CustomFileAction"),(0,i.kt)("td",{parentName:"tr",align:null},"all HadoopFileDataObjects, WebserviceFileDataObject, SFtpFileDataObject")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Generic DataFrame API"),(0,i.kt)("td",{parentName:"tr",align:null},"Spark Engine"),(0,i.kt)("td",{parentName:"tr",align:null},"SparkSubFeed"),(0,i.kt)("td",{parentName:"tr",align:null},"Transform data with Spark DataFrame API"),(0,i.kt)("td",{parentName:"tr",align:null},"CopyAction, CustomDataFrameAction, DeduplicateAction, HistorizeAction"),(0,i.kt)("td",{parentName:"tr",align:null},"all Hadoop/SparkFileDataObject, AccessTableDataObject, AirbyteDataObject, CustomDfDataObject, DeltaLakeTableDataObject, HiveTableDataObject, JdbcTableDataObject, JmsDataObject, KafkaTopicDataObject, SnowflakeTableDataObject, SplunkDataObject, TickTockHiveTableDataObject")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Generic DataFrame API"),(0,i.kt)("td",{parentName:"tr",align:null},"Snowflake-Snowpark Engine"),(0,i.kt)("td",{parentName:"tr",align:null},"SnowparkSubFeed"),(0,i.kt)("td",{parentName:"tr",align:null},"Transform data within Snowflake with Snowpark DataFrame API"),(0,i.kt)("td",{parentName:"tr",align:null},"CopyAction, CustomDataFrameAction, DeduplicateAction, (HistorizeAction)"),(0,i.kt)("td",{parentName:"tr",align:null},"SnowflakeTableDataObject")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"Script"),(0,i.kt)("td",{parentName:"tr",align:null},"Script Engine"),(0,i.kt)("td",{parentName:"tr",align:null},"ScriptSubFeed"),(0,i.kt)("td",{parentName:"tr",align:null},"Coordinate script task execution and notify DataObjects about script results"),(0,i.kt)("td",{parentName:"tr",align:null},"No public implementation for now"),(0,i.kt)("td",{parentName:"tr",align:null},"all DataObjects")))),(0,i.kt)("h3",{id:"connecting-different-execution-engines"},"Connecting different execution engines"),(0,i.kt)("p",null,"In order to build a data pipeline using different execution engines, you need a DataObject that supports both execution engines as interface, so that one execution engine can write the data in the DataObject and the other one can read from it."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"from FileSubFeed to SparkSubFeed (and vice-versa): any Hadoop/SparkFileDataObject like ParquetFileDataObject"),(0,i.kt)("li",{parentName:"ul"},"from SparkSubFeed to SnowparkSubFeed (and vice-versa): SnowflakeTableDataObject"),(0,i.kt)("li",{parentName:"ul"},"from ScriptSubFeed to any (and vice-versa): every DataObject is suitable")),(0,i.kt)("h3",{id:"schema-propagation"},"Schema propagation"),(0,i.kt)("p",null,'Note that a schema can only be propagated within a data pipeline for consecutive actions running with an execution engine of category "Generic DataFrame API". Whenever such an Action has an input from a different category, the schema is read again from the DataObject.'),(0,i.kt)("p",null,'SDLB is able to convert schemas between different execution engines of category "Generic DataFrame API", e.g. Spark and Snowpark.'),(0,i.kt)("h3",{id:"determining-execution-engine-to-use-in-generic-dataframe-api-actions"},'Determining execution engine to use in "Generic DataFrame API" Actions'),(0,i.kt)("p",null,'A "Generic DataFrame API" Action can run with different execution engines like Spark or Snowpark. It determines the execution engine to use in Init-phase by checking the supported types of inputs, outputs and transformations. The first common type is chosen. If there is no common type an exception is thrown.\nTo check which execution engine was chosen, look for logs like the following:'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"  INFO  CustomDataFrameAction - (Action~...) selected subFeedType SparkSubFeed\n")),(0,i.kt)("h3",{id:"execution-engines-vs-execution-environments"},"Execution Engines vs Execution Environments"),(0,i.kt)("p",null,"As mentioned in ",(0,i.kt)("a",{parentName:"p",href:"../../docs/architecture"},"Architecture"),", SDLB is first and foremost a Java (Scala) application.\nIt can run in any Execution Environment where you can install a JVM, executing Actions with any of its Execution Engines. SDLB chooses the Execution Engines for your data pipeline independently from the Execution Environment that SDLB lives in.\nFor example: Let's say you run SDLB in a distributed fashion on a Spark Cluster using spark-submit.\nIf one of your Actions only has SnowflakeTableDataObjects as input and output, SDLB will run it using the Snowpark-Engine.\nIn practice, this means that SDLB will connect to the Snowflake Environment from inside your Spark-Cluster and then execute your Action from there using Snowpark's Java/Scala Library."),(0,i.kt)("p",null,"Of course, the Execution Environment you have influences the DataObjects that you have at your disposal: for instance, if you want to connect to Snowflake, you need a Snowflake account and be able to connect to Snowflake.\nBut the Execution Environment does not determine the Execution Engines SDLB will use - your DataObjects, Actions and Transformations do.\n",(0,i.kt)("img",{alt:"img.png",src:n(2290).Z,width:"1148",height:"588"})))}p.isMDXComponent=!0},2290:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/execution_engines_layers-a188259e2ae4a851154ebbcf23d993ae.png"}}]);