"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[5663],{4055:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var a=n(4848),i=n(8453);const r={title:"Keeping historical data"},o=void 0,s={id:"getting-started/part-2/historical-data",title:"Keeping historical data",description:"Goal",source:"@site/docs/getting-started/part-2/historical-data.md",sourceDirName:"getting-started/part-2",slug:"/getting-started/part-2/historical-data",permalink:"/docs/getting-started/part-2/historical-data",draft:!1,unlisted:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/getting-started/part-2/historical-data.md",tags:[],version:"current",frontMatter:{title:"Keeping historical data"},sidebar:"tutorialSidebar",previous:{title:"Delta Lake - a better data format",permalink:"/docs/getting-started/part-2/delta-lake-format"},next:{title:"Configure for different environments",permalink:"/docs/getting-started/part-2/environments"}},d={},l=[{value:"Goal",id:"goal",level:2},{value:"Requirements",id:"requirements",level:2},{value:"Historization of airport data",id:"historization-of-airport-data",level:2},{value:"Deduplication of flight data",id:"deduplication-of-flight-data",level:2}];function c(e){const t={a:"a",admonition:"admonition",br:"br",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h2,{id:"goal",children:"Goal"}),"\n",(0,a.jsx)(t.p,{children:"Data generally can be split into two groups:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["Master data:",(0,a.jsx)(t.br,{}),"\n","data about objects that evolve over time, e.g. an airport, a person, a product..."]}),"\n",(0,a.jsxs)(t.li,{children:["Transactional data:",(0,a.jsx)(t.br,{}),"\n","data about events that took place at a certain point in time, e.g. a flight, a payment..."]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"To keep historical data for both these categories, different strategies are applied:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Master data"})," is most often ",(0,a.jsx)(t.strong,{children:"historized"}),' - this means tracking the evolution of objects over time by introducing a time dimension.\nUsually this is modelled with two additional attributes "valid_from" and "valid_to", where "valid_from" is an additional primary key column.']}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Transactional data"})," is usually ",(0,a.jsx)(t.strong,{children:"deduplicated"}),", as only the latest state of a specific event is of interest. If an update for an event occurs, the previous information is discarded (or consolidated in special cases).\nAdditional care must be taken to keep all historical events, even if they are no longer present in the source system. Often specific housekeeping rules are applied (e.g. retention period), either for legal or cost saving reasons."]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"requirements",children:"Requirements"}),"\n",(0,a.jsx)(t.p,{children:"For Historization and Deduplication a data pipeline needs to read the state of the output DataObject, merge it with the new state of the input DataObject and write the result to the output DataObject.\nTo read and write the same DataObject in the same SDLB Action, this must be a transactional DataObject.\nIt means the DataObject must implement the interface TransactionalSparkTableDataObject of SDLB.\nLuckily in the previous chapter we already upgraded our data pipeline to use DeltaLakeTableDataObject, which is a TransactionalSparkTableDataObject."}),"\n",(0,a.jsx)(t.p,{children:"Further, we need a key to identify records for a specific object in our data, so we can build the time dimension or deduplicate records of the same object:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["For airport masterdata (",(0,a.jsx)(t.code,{children:"int_airports"}),') the attribute "ident" clearly serves this purpose.']}),"\n",(0,a.jsxs)(t.li,{children:["For departure data (",(0,a.jsx)(t.code,{children:"int_departures"}),") it gets more complicated to identify a flight. To simplify, let's assume we're only interested in one flight per aircraft, departure airport and day.\nThe key would then be the attributes ",(0,a.jsx)(t.code,{children:"icao24"}),", ",(0,a.jsx)(t.code,{children:"estdepartureairport"})," and ",(0,a.jsx)(t.code,{children:"trunc_date"}),"."]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"historization-of-airport-data",children:"Historization of airport data"}),"\n",(0,a.jsx)(t.p,{children:"To historize airport master data, we have to adapt our configuration as follows:"}),"\n",(0,a.jsxs)(t.p,{children:["Add a primary key to the table definition of ",(0,a.jsx)(t.code,{children:"int-airports"}),":"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'  table {\n    db = "default"\n    name = "int_airports"\n    primaryKey = [ident]\n  }\n'})}),"\n",(0,a.jsxs)(t.p,{children:["Note, that a primary key can be a composite primary key, therefore you need to define an array of columns ",(0,a.jsx)(t.code,{children:"[ident]"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["For the action ",(0,a.jsx)(t.code,{children:"select-airport-cols"}),", change its type from ",(0,a.jsx)(t.code,{children:"CopyAction"})," to ",(0,a.jsx)(t.code,{children:"HistorizeAction"}),".",(0,a.jsx)(t.br,{}),"\n","While you're at it, rename it to ",(0,a.jsx)(t.code,{children:"historize-airports"})," to reflect its new function."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"  historize-airports {\n    type = HistorizeAction\n    ...\n  }\n"})}),"\n",(0,a.jsxs)(t.p,{children:["With historization, this table will now get two additional columns called ",(0,a.jsx)(t.code,{children:"dl_ts_captured"})," and ",(0,a.jsx)(t.code,{children:"dl_ts_delimited"}),".\nSchema evolution of existing tables will be explained later, so for now, just delete the table and it's data for the DataObject ",(0,a.jsx)(t.code,{children:"int-airports"})," through spark-shell and SDLB's scala interface :"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"sdlb.dataObjects.intAirports.dataObject.dropTable\n"})}),"\n",(0,a.jsx)(t.p,{children:"See previous chapter for more details."}),"\n",(0,a.jsxs)(t.p,{children:["Then start Action ",(0,a.jsx)(t.code,{children:"historize-airports"}),".\nYou may have seen that the ",(0,a.jsx)(t.code,{children:"--feed-sel"})," parameter of SDLB command line supports more options to select actions to execute (see command line help).\nWe will now only execute this single action by changing this parameter to ",(0,a.jsx)(t.code,{children:"--feed-sel ids:historize-airports"}),":"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"./startJob.sh -c /mnt/config --feed-sel ids:historize-airports\n"})}),"\n",(0,a.jsx)(t.admonition,{title:"JDOFatalDataStoreException",type:"tip",children:(0,a.jsxs)(t.p,{children:["Getting error ",(0,a.jsx)(t.code,{children:"javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database"})," and ",(0,a.jsx)(t.code,{children:"Another instance of Derby may have already booted the database /mnt/data/metastore_db"}),"?\nThis is due to the fact that we are using a file-based Derby database as metastore, which can be only opened by one process at a time.\nSolution: close spark-shell (Ctrl-D) before running startJob.sh."]})}),"\n",(0,a.jsxs)(t.p,{children:["After successful execution you can check the schema and data of our table in spark-shell.\nIt now has a time dimension through the two new columns ",(0,a.jsx)(t.code,{children:"dl_ts_captured"})," and ",(0,a.jsx)(t.code,{children:"dl_ts_delimited"}),".\nThey form a closed interval, meaning start and end time are inclusive.\nIt has millisecond precision, but the timestamp value is set to the current time of our data pipeline run.\nThe two attributes show the time period in which an object with this combination of attribute values has existed in our data source.\nThe sampling rate is given by the frequency that our data pipeline is scheduled."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"sdlb.dataObjects.intAirports.printSchema\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"    root\n    |-- ident: string (nullable = true)\n    |-- name: string (nullable = true)\n    |-- latitude_deg: string (nullable = true)\n    |-- longitude_deg: string (nullable = true)\n    |-- dl_ts_captured: timestamp (nullable = true)\n    |-- dl_ts_delimited: timestamp (nullable = true)\n"})}),"\n",(0,a.jsx)(t.p,{children:"If you look at the data, there should be only one record per object for now, as we didn't run our data pipeline with historical data yet."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'  dataIntAirports.get.orderBy($"ident",$"dl_ts_captured").show\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"    +-----+--------------------+------------------+-------------------+--------------------+-------------------+\n    |ident|                name|      latitude_deg|      longitude_deg|      dl_ts_captured|    dl_ts_delimited|\n    +-----+--------------------+------------------+-------------------+--------------------+-------------------+\n    |  00A|   Total Rf Heliport|    40.07080078125| -74.93360137939453|2021-12-05 13:23:...|9999-12-31 00:00:00|\n    | 00AA|Aero B Ranch Airport|         38.704022|        -101.473911|2021-12-05 13:23:...|9999-12-31 00:00:00|\n    | 00AK|        Lowell Field|         59.947733|        -151.692524|2021-12-05 13:23:...|9999-12-31 00:00:00|\n    | 00AL|        Epps Airpark| 34.86479949951172| -86.77030181884766|2021-12-05 13:23:...|9999-12-31 00:00:00|\n    | 00AR|Newport Hospital ...|           35.6087|         -91.254898|2021-12-05 13:23:...|9999-12-31 00:00:00|\n    ...\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Let's try to simulate the historization process by loading a historical state of the data and see if any of the airports have changed since then.\nFor this, drop table ",(0,a.jsx)(t.code,{children:"int-airports"})," again.\nThen, delete all files in ",(0,a.jsx)(t.code,{children:"data/stg-airport"})," and copy the historical ",(0,a.jsx)(t.code,{children:"result.csv"})," from the folder ",(0,a.jsx)(t.code,{children:"data/stg-airports-fallback"})," into the folder ",(0,a.jsx)(t.code,{children:"data/stg-airports"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["Now start the action ",(0,a.jsx)(t.code,{children:"historize-airports"}),' (and only historize-airports) again to do an "initial load".\nRemember how you do that? That\'s right, you can define a single action with ',(0,a.jsx)(t.code,{children:"--feed-sel ids:historize-airports"}),".",(0,a.jsx)(t.br,{}),"\n","Afterward, start actions ",(0,a.jsx)(t.code,{children:"download-airports"})," and ",(0,a.jsx)(t.code,{children:"historize-airports"})," by using the parameter ",(0,a.jsx)(t.code,{children:"--feed-sel 'ids:(download|historize)-airports'"})," to download fresh data and build up the airport history."]}),"\n",(0,a.jsxs)(t.p,{children:["Uff, getting error ",(0,a.jsx)(t.code,{children:"AnalysisException: [UNSUPPORTED_OVERWRITE.TABLE] Can't overwrite the target that is also being read from"}),'.\nRecent versions of Delta Lake don\'t like if we overwrite a table "also being read from". Unfortunately this is what HistoryAction does.\nIt reads the historical data from the table, and adds new records for data changed in the current snapshot.\nBy default, this happens by overwriting the whole table. This is not optimal from a performance point of view, but there was no other way to change data with Hive tables.\nAs Delta Lake supports merge statement, we can add ',(0,a.jsx)(t.code,{children:"mergeModeEnable = true"})," to our HistorizeAction configuration as follows:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"  historize-airports {\n    type = HistorizeAction\n    inputId = stg-airports\n    outputId = int-airports    \n    mergeModeEnable = true\n    ...\n  }\n"})}),"\n",(0,a.jsx)(t.admonition,{title:"Merge Mode",type:"info",children:(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.code,{children:"mergeModeEnable = true"})," tells Deduplicate/HistorizeAction to merge changed data into the output DataObject, instead of overwriting the whole DataObject.\nThe output DataObject must implement CanMergeDataFrame interface (also called trait in Scala) for this.\nDeltaLakeTableDataObject will then create a complex SQL-Upsert statement to merge new and changed data into existing output data."]})}),"\n",(0,a.jsxs)(t.p,{children:["Uff, getting error ",(0,a.jsx)(t.code,{children:"'SchemaViolationException: (DataObject~int-airports) Schema does not match schema defined on write:"})," with ",(0,a.jsx)(t.code,{children:"superfluousCols=dl_hash"}),"?\nThis is due to merge mode needing an additional column ",(0,a.jsx)(t.code,{children:"dl_hash"})," to efficiently check for changed data.\nDrop table ",(0,a.jsx)(t.code,{children:"int-airports"})," again, then startJob ",(0,a.jsx)(t.code,{children:"--feed-sel ids:historize-airports"})," and afterward ",(0,a.jsx)(t.code,{children:"--feed-sel 'ids:(download|historize)-airports'"}),"."]}),"\n",(0,a.jsx)(t.p,{children:"Now check in spark-shell again, and you'll find several airports that have changed between the initial and the current state:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'  sdlb.dataObjects.intAirports.get\n  .groupBy($"ident").count\n  .orderBy($"count".desc)\n  .show\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"    +-------+-----+\n    |  ident|count|\n    +-------+-----+\n    |RU-4111|    2|\n    |   LL33|    2|\n    |   73CA|    2|\n    |CA-0120|    2|\n    |   CDV3|    2|\n    ...\n"})}),"\n",(0,a.jsxs)(t.admonition,{title:"Tips for spark-shell",type:"tip",children:[(0,a.jsx)(t.p,{children:"You may have noticed that pasting multi-line text into spark-shell executes every line separately.\nThis works in the example above, but does not work with every scala code. Also, it doesn't look that nice to see all intermediate results."}),(0,a.jsxs)(t.p,{children:["Enter ",(0,a.jsx)(t.code,{children:":paste"})," in spark-shell before pasting multi-line text for a better experience."]}),(0,a.jsx)(t.p,{children:"Also, keep in mind you can autocomplete commands with the tab key. This allows you for instance to autocomplete the commands of the SDLB Interface."})]}),"\n",(0,a.jsx)(t.p,{children:"When checking the details it seems that for many airports the number of significant digits was reduced for the position:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'  sdlb.dataObjects.intAirports.get\n  .where($"ident"==="CDV3")\n  .drop("dl_hash")\n  .show(false)\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"    +-----+-------------------------------------------------+-------------+--------------+--------------------------+--------------------------+\n    |ident|name                                             |latitude_deg |longitude_deg |dl_ts_captured            |dl_ts_delimited           |\n    +-----+-------------------------------------------------+-------------+--------------+--------------------------+--------------------------+\n    |CDV3 |Charlottetown (Queen Elizabeth Hospital) Heliport|46.255493    |-63.098887    |2021-12-05 20:52:58.800645|9999-12-31 00:00:00       |\n    |CDV3 |Charlottetown (Queen Elizabeth Hospital) Heliport|46.2554925916|-63.0988866091|2021-12-05 20:40:31.629764|2021-12-05 20:52:58.799645|\n    +-----+-------------------------------------------------+-------------+--------------+--------------------------+--------------------------+\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Values for ",(0,a.jsx)(t.code,{children:"dl_ts_capture"})," and ",(0,a.jsx)(t.code,{children:"dl_ts_delimited"})," respectively were set to the current time of our data pipeline run.\nFor an initial load, this should be set to the time of the historical data set.\nCurrently, this is not possible in SDLB, but there are plans to implement this, see issue ",(0,a.jsx)(t.a,{href:"https://github.com/smart-data-lake/smart-data-lake/issues/427",children:"#427"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["If you would try to run ",(0,a.jsx)(t.code,{children:"download-airports"})," and ",(0,a.jsx)(t.code,{children:"historize-airports"})," Action in a fresh environment (data/stg-airports directory removed),\nthe job would fail with ",(0,a.jsx)(t.code,{children:"requirement failed: (DataObject~stg-airports) DataObject schema is undefined."})," in prepare phase.\nThis is bug ",(0,a.jsx)(t.a,{href:"https://github.com/smart-data-lake/smart-data-lake/issues/900",children:"#900"}),".\nFor now lets make it more stable by adding a fixed schema to ",(0,a.jsx)(t.code,{children:"stg-airports"})," DataObject as follows:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'  stg-airports {\n    type = CsvFileDataObject\n    path = ${env.basePath}"~{id}"\n    schema = "id string, ident string, type string, name string, latitude_deg string, longitude_deg string, elevation_ft string, continent string, iso_country string, iso_region string, municipality string, scheduled_service string, gps_code string, iata_code string, local_code string, home_link string, wikipedia_link string, keywords string"\n  }\n'})}),"\n",(0,a.jsx)(t.p,{children:"Now let's continue with flight data."}),"\n",(0,a.jsxs)(t.admonition,{title:"Spark performance",type:"tip",children:[(0,a.jsxs)(t.p,{children:["Spark automatically splits jobs into multiple tasks to distribute to its workers. This is how Spark can process large scale datasets.\nThe HistorizeAction needs to join all existing data with the new input data and check for changes.",(0,a.jsx)(t.br,{}),"\n","If Spark joins data, it needs two processing stages and a shuffle in between to do so (you can read more about this in various Spark tutorials).\nThere is a Spark property we can tune for small datasets to reduce the number of tasks created.\nThe default value is to create 200 tasks in each shuffle. With our dataset, 2 tasks should be enough already.\nYou can tune this by setting the following property in global.spark-options of global.conf configuration file:"]}),(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'    "spark.sql.shuffle.partitions" = 2\n'})})]}),"\n",(0,a.jsx)(t.h2,{id:"deduplication-of-flight-data",children:"Deduplication of flight data"}),"\n",(0,a.jsx)(t.p,{children:"To deduplicate departure flight data, we have to adapt our configuration as follows:"}),"\n",(0,a.jsxs)(t.p,{children:["Add a primary key to the table definition of ",(0,a.jsx)(t.code,{children:"int-departures"}),":"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'  table {\n    db = "default"\n    name = "int_departures"\n    primaryKey = [icao24, estdepartureairport, dt]\n  }\n'})}),"\n",(0,a.jsxs)(t.p,{children:["Change the type of action ",(0,a.jsx)(t.code,{children:"prepare-departures"})," from ",(0,a.jsx)(t.code,{children:"CopyAction"}),", this time to ",(0,a.jsx)(t.code,{children:"DeduplicateAction"})," and rename it to ",(0,a.jsx)(t.code,{children:"deduplicate-departures"}),", again to reflect its new type.\nIt also needs an additional transformers to calculate the new primary key column ",(0,a.jsx)(t.code,{children:"dt"})," derived from the column ",(0,a.jsx)(t.code,{children:"firstseen"}),", and to make sure input data is unique across the primary key of the output DataObject.\nFinally, we need to set ",(0,a.jsx)(t.code,{children:"mergeModeEnable = true"})," and ",(0,a.jsx)(t.code,{children:"updateCapturedColumnOnlyWhenChanged = true"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.code,{children:"deduplicate-departures"})," Action definition should then look as follows:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'  deduplicate-departures {\n    type = DeduplicateAction\n    inputId = stg-departures\n    outputId = int-departures\n    mergeModeEnable = true\n    updateCapturedColumnOnlyWhenChanged = true\n    transformers = [{\n      type = SQLDfTransformer\n      code = "select stg_departures.*, date_format(from_unixtime(firstseen),\'yyyyMMdd\') dt from stg_departures"\n    },{\n      type = DeduplicateTransformer\n      rankingExpression = "lastSeen desc"\n    }]\n  }\n'})}),"\n",(0,a.jsx)(t.admonition,{title:"Effect of updateCapturedColumnOnlyWhenChanged",type:"tip",children:(0,a.jsxs)(t.p,{children:["By default DeduplicateAction updates column dl_captured in the output for every record it receives. To reduce the number of updated records, ",(0,a.jsx)(t.code,{children:"updateCapturedColumnOnlyWhenChanged = true"})," can be set.\nIn this case column dl_captured is only updated in the output, when some attribute of the record changed."]})}),"\n",(0,a.jsxs)(t.p,{children:["Now, delete the table and data of the DataObject ",(0,a.jsx)(t.code,{children:"int-departures"})," in spark-shell, to prepare it for the new columns ",(0,a.jsx)(t.code,{children:"dt"})," and ",(0,a.jsx)(t.code,{children:"dl_ts_captured"}),"."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"sdlb.dataObjects.intDepartures.dataObject.dropTable\n"})}),"\n",(0,a.jsx)(t.p,{children:"Then start Action deduplicate-departures:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"./startJob.sh -c /mnt/config --feed-sel ids:deduplicate-departures\n"})}),"\n",(0,a.jsxs)(t.p,{children:["After successful execution you can check the schema and data of our table in spark-shell.\nThe new column ",(0,a.jsx)(t.code,{children:"dl_ts_captured"})," shows the current time of the data pipeline run when this object first occurred in the input data."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"  sdlb.dataObjects.intDepartures.printSchema\n"})}),"\n",(0,a.jsx)(t.p,{children:"prints:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"  root\n  |-- arrivalairportcandidatescount: long (nullable = true)\n  |-- callsign: string (nullable = true)\n  |-- departureairportcandidatescount: long (nullable = true)\n  |-- estarrivalairport: string (nullable = true)\n  |-- estarrivalairporthorizdistance: long (nullable = true)\n  |-- estarrivalairportvertdistance: long (nullable = true)\n  |-- estdepartureairport: string (nullable = true)\n  |-- estdepartureairporthorizdistance: long (nullable = true)\n  |-- estdepartureairportvertdistance: long (nullable = true)\n  |-- firstseen: long (nullable = true)\n  |-- icao24: string (nullable = true)\n  |-- lastseen: long (nullable = true)\n  |-- dt: string (nullable = true)\n  |-- dl_ts_captured: timestamp (nullable = true)\n"})}),"\n",(0,a.jsx)(t.p,{children:"We can check the work of DeduplicateAction by the following query in spark-shell:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'sdlb.dataObjects.intDepartures.get\n.groupBy($"icao24", $"estdepartureairport", $"dt")\n.count\n.orderBy($"count".desc)\n.show\n'})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"    +------+-------------------+--------+-----+\n    |icao24|estdepartureairport|      dt|count|\n    +------+-------------------+--------+-----+\n    |4b43ab|               LSZB|20210829|    1|\n    |4b4b8d|               LSZB|20210829|    1|\n    |4b1b13|               LSZB|20210829|    1|\n    |4b4445|               LSZB|20210829|    1|\n    |4b0f70|               LSZB|20210830|    1|\n    |4b1a01|               LSZB|20210829|    1|\n    |346603|               LSZB|20210829|    1|\n    |4b4442|               LSZB|20210829|    1|\n    |4d02d7|               LSZB|20210829|    1|\n    |4b43ab|               LSZB|20210830|    1|\n    ...\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Note that DeduplicateAction assumes that input data is already unique across the given primary key. With ",(0,a.jsx)(t.code,{children:"mergeModeEnable = true"})," we even get errors otherwise.\nDeduplicateAction doesn't deduplicate your input data by default, because deduplication is costly and data often is already unique.\nIn our example we have duplicates in the input data set, and added the DeduplicateTransformer to our input data.\nInstead of using DeduplicateTransformer, we could also implement our own deduplicate logic using the Scala Spark API with ScalaCodeSparkDfTransformer as follows:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:'    transformers = [{\n      type = ScalaCodeSparkDfTransformer\n      code = """\n        import org.apache.spark.sql.{DataFrame, SparkSession}\n        def transform(session: SparkSession, options: Map[String,String], df: DataFrame, dataObjectId: String) : DataFrame = {\n          import session.implicits._\n          df.dropDuplicates("icao24", "estdepartureairport", "dt")\n        }\n        // return as function\n        transform _\n      """\n    }]\n'})}),"\n",(0,a.jsx)(t.admonition,{type:"info",children:(0,a.jsxs)(t.p,{children:["Note how we have used a third way of defining transformation logic now:",(0,a.jsx)(t.br,{}),"\n","In part 1 we first used a SQLDfsTransformer writing SQL code.",(0,a.jsx)(t.br,{}),"\n","Then for the more complex example of computing distances, we used a ScalaClassSparkDfTransformer pointing to a Scala class.",(0,a.jsx)(t.br,{}),"\n","Here, we simply include Scala code in our configuration file directly."]})}),"\n",(0,a.jsxs)(t.p,{children:["For sure DeduplicateAction did not have much work to do, as this was the first data load.\nIn order to get different data you would need to adjust the unix timestamp parameters in the URL of DataObject ",(0,a.jsx)(t.code,{children:"ext-departures"}),".\nFeel free to play around."]}),"\n",(0,a.jsxs)(t.admonition,{title:"Scala Code",type:"info",children:[(0,a.jsx)(t.p,{children:"Scala is a compiled language. The compiler creates bytecode which can be run on a JVM.\nNormally compilation takes place before execution. So how does it work with scala code in the configuration as in our deduplication logic above?"}),(0,a.jsx)(t.p,{children:"With Scala, you can compile code on the fly. This is actually what the Scala Shell/REPL is doing as well.\nThe Scala code in the configuration above gets compiled when ScalaCodeSparkDfTransformer is instantiated during startup of SDLB."})]}),"\n",(0,a.jsxs)(t.p,{children:["Your departures/airports.conf should now look like the files ending with ",(0,a.jsx)(t.code,{children:"part-2b-solution"})," in ",(0,a.jsx)(t.a,{href:"https://github.com/smart-data-lake/getting-started/tree/master/config",children:"this directory"}),"."]}),"\n",(0,a.jsx)(t.p,{children:"In the next step we are going to configure the pipeline for different environments like development, integration and production."})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>s});var a=n(6540);const i={},r=a.createContext(i);function o(e){const t=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(r.Provider,{value:t},e.children)}}}]);