"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[8216],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return f}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),u=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=u(e.components);return a.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=u(n),f=r,m=d["".concat(l,".").concat(f)]||d[f]||p[f]||i;return n?a.createElement(m,o(o({ref:t},c),{},{components:n})):a.createElement(m,o({ref:t},c))}));function f(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var u=2;u<i;u++)o[u]=n[u];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},9831:function(e,t,n){n.r(t),n.d(t,{contentTitle:function(){return l},default:function(){return d},frontMatter:function(){return s},metadata:function(){return u},toc:function(){return c}});var a=n(7462),r=n(3366),i=(n(7294),n(3905)),o=["components"],s={id:"testing",title:"Testing"},l=void 0,u={unversionedId:"reference/testing",id:"reference/testing",title:"Testing",description:"This page is under review and currently not visible in the menu.",source:"@site/docs/reference/testing.md",sourceDirName:"reference",slug:"/reference/testing",permalink:"/docs/reference/testing",editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/reference/testing.md",tags:[],version:"current",frontMatter:{id:"testing",title:"Testing"}},c=[{value:"Config validation",id:"config-validation",children:[],level:2},{value:"Dry run",id:"dry-run",children:[],level:2},{value:"Custom transformation logic unit tests",id:"custom-transformation-logic-unit-tests",children:[],level:2},{value:"Simulation of spark data pipeline",id:"simulation-of-spark-data-pipeline",children:[],level:2}],p={toc:c};function d(e){var t=e.components,n=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("div",{className:"admonition admonition-warning alert alert--danger"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"warning")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"This page is under review and currently not visible in the menu."))),(0,i.kt)("p",null,"Testing is crucial for software quality and maintenance. This is also true for data pipelines."),(0,i.kt)("p",null,"SDL provides the following possibilities for Testing:"),(0,i.kt)("h2",{id:"config-validation"},"Config validation"),(0,i.kt)("p",null,"Parsing of configuration can be validated by specifying command line parameter ",(0,i.kt)("inlineCode",{parentName:"p"},"--test config")," or programmatically:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},'class ConfigTest extends FunSuite with TestUtil {\n  test("validate configuration") {\n    val (registry, globalConfig) = ConfigToolbox.loadAndParseConfig(Seq("src/main/resources"))\n    assert(registry.getActions.nonEmpty)\n  }\n}\n')),(0,i.kt)("p",null,"This should be done in continuous integration."),(0,i.kt)("h2",{id:"dry-run"},"Dry run"),(0,i.kt)("p",null,"A dry-run can be started by specifying command line parameter ",(0,i.kt)("inlineCode",{parentName:"p"},"--test dry-run"),".\nThe dry-run executes only prepare and init phase. It validates configuration, checks connections and validates Spark lineage.\nIt doesn't change anything in the environment."),(0,i.kt)("p",null,"This is suitable for smoke testing after deployment."),(0,i.kt)("h2",{id:"custom-transformation-logic-unit-tests"},"Custom transformation logic unit tests"),(0,i.kt)("p",null,"Logic of custom transformation can easily be unit tested. Example:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},'class MyCustomTransformerTest extends FunSuite {\n\n  // init spark session\n  val sparkSession = Map("spark.default.parallelism" -> "1", "spark.sql.shuffle.partitions" -> "1", "spark.task.maxFailures" -> "1")\n  lazy val session: SparkSession = GlobalConfig(enableHive = false, sparkOptions = Some(sparkSession))\n    .createSparkSession("test", Some("local[*]"))\n  import session.implicits._\n\n  test("test my custom transformer") {\n\n    // define input\n    val dfInput = Seq(("joe",1)).toDF("name", "cnt")\n\n    // transform\n    val transformer = new MyCustomTransformer\n    val dfsInput = Map("input" -> dfInput)\n    val dfsTransformed = transformer.transform(session, Map(), dfsInput)\n    val dfOutput = dfsTransformed("output")\n\n    // check\n    assert(dfOutput.count == 1)\n  }\n}\n')),(0,i.kt)("h2",{id:"simulation-of-spark-data-pipeline"},"Simulation of spark data pipeline"),(0,i.kt)("p",null,'Instead of testing single transformation logic, it would be interesting to test whole pipelines of spark actions.\nFor this you can start a simulation run programmatically. You have to provide all input data frames and get the output data frames of the end nodes of the DAG.\nThe simulation mode only executes the init phase with special modification, so it runs without any environment. Of course there are some exceptions like kafka/confluent schema registry.\nNote that simulation mode only supports spark actions for now, you might need to choose "feedSel" accordingly.'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},'class MyDataPipelineTest extends FunSuite with TestUtil {\n\n  // load config\n  val (registry, globalConfig) = ConfigToolbox.loadAndParseConfig(Seq("src/main/resources"))\n\n  // init spark session\n  val sparkSession = Map("spark.default.parallelism" -> "1", "spark.sql.shuffle.partitions" -> "1", "spark.task.maxFailures" -> "1")\n  lazy val session: SparkSession = GlobalConfig(enableHive = false, sparkOptions = Some(sparkSession))\n    .createSparkSession("test", Some("local[*]"))\n  import session.implicits._\n\n  test("enrich and process: create alarm") {\n  \n    // get input1 from modified data object\n    val input1DO = registry.get[CsvFileDataObject]("input1")\n      .copy(connectionId = None, path = "src/test/resources/sample_input1.csv")\n    val dfInput1 = input1DO.getDataFrame()\n\n    // define input2 manually\n    val dfInput2 = Seq(("joe",1)).toDF("name", "cnt")\n\n    // transform\n    val inputSubFeeds = Seq(\n      SparkSubFeed(Some(dfInput1), DataObjectId("input1"), Seq()),\n      SparkSubFeed(Some(dfInput2), DataObjectId("ipnut2"), Seq())\n    )\n    val config = SmartDataLakeBuilderConfig(feedSel = s"my-feed", applicationName = Some("test"), configuration = Some("test"))\n    val sdlb = new DefaultSmartDataLakeBuilder()\n    val (finalSubFeeds, stats) = sdlb.startSimulation(config, inputSubFeeds)\n    val dfOutput = finalSubFeeds.find(_.dataObjectId.id == s"output").get.dataFrame.get.cache\n\n    // check\n    val output = dfOutput.select($"name", $"test").as[(String,Boolean)].collect.toSeq\n    assert(output == Seq(("joe", true)))\n  }\n}\n')))}d.isMDXComponent=!0}}]);