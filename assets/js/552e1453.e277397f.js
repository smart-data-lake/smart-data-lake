"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[5161],{6499:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});var t=a(4848),r=a(8453);const s={id:"deployYarn",title:"Deploy on YARN"},o=void 0,i={id:"reference/deployYarn",title:"Deploy on YARN",description:"This page is under review and currently not visible in the menu.",source:"@site/docs/reference/deployYarn.md",sourceDirName:"reference",slug:"/reference/deployYarn",permalink:"/docs/reference/deployYarn",draft:!1,unlisted:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/reference/deployYarn.md",tags:[],version:"current",frontMatter:{id:"deployYarn",title:"Deploy on YARN"}},c={},l=[];function d(e){const n={a:"a",admonition:"admonition",code:"code",li:"li",ol:"ol",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsx)(n.p,{children:"This page is under review and currently not visible in the menu."})}),"\n",(0,t.jsxs)(n.p,{children:["Smart Data Lake can be easily executed on a YARN cluster by spark-submit.\nThe following steps will show you how to set everything up and start a first data load.\nSee ",(0,t.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/running-on-yarn.html",children:"Running Spark on YARN"})," for detailed Spark configuration options."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Make sure you have a Spark 2.x Scala 2.11 release installed (spark-submit command needed)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Build the project (with activated profile fat-jar) if you haven't done that already:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mvn package -DskipTests -Pscala-2.11 -Pfat-jar\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Copy test data file to hdfs home directory:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"hdfs dfs -put src/test/resources/AB_NYC_2019.csv\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create an application.conf file:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-hocon",children:'dataObjects {\n  ab-csv1 {\n    type = CsvFileDataObject\n    path = "AB_NYC_2019.csv"\n  }\n  ab-csv2 {\n    type = CsvFileDataObject\n    path = "AB_NYC_copy.csv"\n  }\n}\n\nactions {\n  loadCsv2Csv {\n    type = CopyAction\n    inputId = ab-csv1\n    outputId = ab-csv2\n    metadata {\n      feed = ab-csv\n    }\n  }\n}\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Submit application to YARN cluster with spark-submit.  Don't forget to replace the SmartDataLake version (2x). On windows you also need to manually local directory of application.conf file in the following command."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"spark-submit --master yarn --deploy-mode client --jars target/smartdatalake_2.11-1.0.3-jar-with-dependencies.jar --class io.smartdatalake.app.DefaultSmartDataLakeBuilder target/smartdatalake_2.11-1.0.3-jar-with-dependencies.jar --feed-sel ab-csv -c file://`pwd`/application.conf\n"})}),"\n"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>i});var t=a(6540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);