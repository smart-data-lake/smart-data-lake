"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[795],{3905:function(e,t,a){a.d(t,{Zo:function(){return p},kt:function(){return u}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var c=n.createContext({}),l=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=l(e.components);return n.createElement(c.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},f=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),f=l(a),u=r,m=f["".concat(c,".").concat(u)]||f[u]||d[u]||i;return a?n.createElement(m,o(o({ref:t},p),{},{components:a})):n.createElement(m,o({ref:t},p))}));function u(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=f;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var l=2;l<i;l++)o[l]=a[l];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}f.displayName="MDXCreateElement"},4039:function(e,t,a){a.r(t),a.d(t,{contentTitle:function(){return c},default:function(){return f},frontMatter:function(){return s},metadata:function(){return l},toc:function(){return p}});var n=a(7462),r=a(3366),i=(a(7294),a(3905)),o=["components"],s={id:"deploy-microsoft-azure",title:"Deploy on Microsoft Azure Databricks"},c=void 0,l={unversionedId:"reference/deploy-microsoft-azure",id:"reference/deploy-microsoft-azure",title:"Deploy on Microsoft Azure Databricks",description:"This page is under review and currently not visible in the menu.",source:"@site/docs/reference/deploy-microsoft-azure.md",sourceDirName:"reference",slug:"/reference/deploy-microsoft-azure",permalink:"/docs/reference/deploy-microsoft-azure",editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/reference/deploy-microsoft-azure.md",tags:[],version:"current",frontMatter:{id:"deploy-microsoft-azure",title:"Deploy on Microsoft Azure Databricks"}},p=[],d={toc:p};function f(e){var t=e.components,a=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("div",{className:"admonition admonition-warning alert alert--danger"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))),"warning")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"This page is under review and currently not visible in the menu."))),(0,i.kt)("p",null,"Smart Data Lake can be executed on the Databricks Service running on Microsoft Azure.\nThe following steps show how to execute a simple copy feed."),(0,i.kt)("p",null,"At the time of this writing, a few extra steps are needed to overwrite specific libraries.\nWhen running a job in Databricks, a few dependencies are given and can not be simply overwritten with your own as described in the\n",(0,i.kt)("a",{parentName:"p",href:"https://docs.microsoft.com/en-us/azure/databricks/jobs#library-dependencies"},"Azure documentation"),".\nSince we use a newer version of typesafe config, we need to force the overwrite of this dependency.\nWe will create a cluster init script that downloads the library and saves it on the cluster, then use Sparks ChildFirstURLClassLoader to explicitly load our library first.\nThis can hopefully be simplified in the future."),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"In your Azure portal, create a Databricks Workspace and launch it")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Create a cluster that fits your needs. For a first test you can use the miminal configuration of 1 Worker and 1 Driver node.\nThis example was tested on Databricks Runtime Version 6.2.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Open the Advanced Options, Init Scripts and configure the path:\ndbfs:/databricks/scripts/config-install.sh")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"On your local machine, create a simple script called config-install.sh with the following content"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"#!/bin/bash\nwget -O /databricks/jars/config-1.3.4.jar https://repo1.maven.org/maven2/com/typesafe/config/1.3.4/config-1.3.4.jar\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"To copy this local file to your Databricks filesystem, use the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.databricks.com/dev-tools/cli/index.html"},"Databricks CLI"),":"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"databricks fs mkdirs dbfs:/databricks/scripts\ndatabricks fs cp \\&ltpath-to/config-install.sh\\&gt dbfs:/databricks/scripts/\n")),(0,i.kt)("p",{parentName:"li"},"Now this script gets executed everytime the cluster starts.\nIt will download the config library and put it in a place where the classloader can find it.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Start your cluster, check the event log to see if it's up.\nIf something is wrong with the init script, the cluster will not start.")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"On your local machine, create a second file called application.conf with the following content:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-hocon"},'dataObjects {\n  ab-csv-dbfs {\n    type = CsvFileDataObject\n    path = "file:///dbfs/data/AB_NYC_2019.csv"\n  }\n  ab-reduced-csv-dbfs {\n    type = CsvFileDataObject\n    path = "file:///dbfs/data/~{id}/nyc_reduced.csv"\n  }\n}\n\nactions {\n  loadDbfs2Dbfs {\n    type = CopyAction\n    inputId = ab-csv-dbfs\n    outputId = ab-reduced-csv-dbfs\n    metadata {\n      feed = ab-azure\n    }\n  }\n}\n'))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Upload the file to the conf folder in dbfs:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"databricks fs mkdirs dbfs:/conf\ndatabricks fs cp path-to/application.conf dbfs:/conf/\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Also copy the example CSV file from sdl-examples to the data folder:"),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"databricks fs mkdirs dbfs:/data\ndatabricks fs cp <path-to/AB_NYC_2019.csv> dbfs:/data/\n"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Now create a Job with the following details:"),(0,i.kt)("p",{parentName:"li"}," Task: Upload JAR - Choose the smartdatalake-","<","version",">","-jar-with-dependencies.jar"),(0,i.kt)("p",{parentName:"li"}," Main Class: io.smartdatalake.app.DatabricksSmartDataLakeBuilder\nArguments: -c file:///dbfs/conf/ --feed-sel ab-azure -m yarn"),(0,i.kt)("p",{parentName:"li"}," The option ",(0,i.kt)("em",{parentName:"p"},"--override-jars")," is set automatically to the correct value for DatabricksConfigurableApp.\nIf you want to override any additional libraries, you can provide a list with this option."),(0,i.kt)("p",{parentName:"li"}," If you don't have the JAR file yet, check the README on how to build it (using the Maven profile fat-jar).")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Start the job and check the result, you should now have the output written in dbfs:/data/ab-reduced-csv-dbfs"))))}f.isMDXComponent=!0}}]);