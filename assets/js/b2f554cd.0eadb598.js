"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[1477],{10:function(t){t.exports=JSON.parse('{"blogPosts":[{"id":"sdl-airbyte","metadata":{"permalink":"/blog/sdl-airbyte","editUrl":"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/blog/2022-03-30-SDL-airbyte/2022-03-30-SDL-airbyte.md","source":"@site/blog/2022-03-30-SDL-airbyte/2022-03-30-SDL-airbyte.md","title":"Using Airbyte connector to inspect github data","description":"A short example using Airbyte github connector","date":"2022-03-30T00:00:00.000Z","formattedDate":"March 30, 2022","tags":[{"label":"Airbyte","permalink":"/blog/tags/airbyte"},{"label":"Connector","permalink":"/blog/tags/connector"}],"readingTime":4.46,"truncated":true,"authors":[{"name":"Mandes Sch\xf6nherr","title":"Dr.sc.nat.","url":"https://github.com/mand35"}],"frontMatter":{"title":"Using Airbyte connector to inspect github data","description":"A short example using Airbyte github connector","slug":"sdl-airbyte","authors":[{"name":"Mandes Sch\xf6nherr","title":"Dr.sc.nat.","url":"https://github.com/mand35"}],"tags":["Airbyte","Connector"],"image":"airbyte.png","hide_table_of_contents":false}},"content":"This article presents the deployment of an [Airbyte Connector](https://airbyte.com) with Smart Data Lake Builder (SDLB). \\nIn particular the [github connector](https://docs.airbyte.com/integrations/sources/github) is implemented using the python sources.\\n\\n\x3c!--truncate--\x3e\\n\\nAirbyte is a framework to sync data from a variety of sources (APIs and databases) into data warehouses and data lakes. \\nIn this example an Airbyte connector is utilized to stream data into Smart Data Lake (SDL). \\nTherefore, the [Airbyte dataObject](http://smartdatalake.ch/json-schema-viewer/index.html#viewer-page?v=2-2) is used and will be configured. \\nThe general [Airbyte connector handling](https://docs.airbyte.com/understanding-airbyte/airbyte-specification#source) is implemented in SDL, which includes the 4 main steps:\\n* `spec`: receiving the specification of the connector\\n* `check`: validating the specified configuration\\n* `discover`: gather a catalog of available streams and its schemas\\n* `read`: collect the actual data\\n\\nThe actual connector is not provided in the SDL repository and needs to be obtained from the [Airbyte repository](https://github.com/airbytehq/airbyte). Besides the [list of existing connectors](https://docs.airbyte.com/integrations), custom connectors could be implemented in Python or Javascript. \\n\\nThe following description builds on top of the example setup from the [getting-started](../../docs/getting-started/setup) guide, using [Podman](https://docs.podman.io) as container engine within a [WSL](https://docs.microsoft.com/en-us/windows/wsl/install) Ubuntu image. \\n\\nThe [github connector](https://docs.airbyte.com/integrations/sources/github) is utilized to gather data about a specific repository.\\n\\n## Prerequisites\\nAfter downloading and installing all necessary packages, the connector is briefly tested:\\n* Python\\n* [Podman including `podman-compose`](../../docs/getting-started/troubleshooting/docker-on-windows) or [Docker](https://www.docker.com/get-started)\\n* [SDL example](https://github.com/smart-data-lake/getting-started/archive/refs/heads/master.zip), download and unpack: \\n  ```Bash\\n  git clone https://github.com/smart-data-lake/getting-started.git SDL_airbyte\\n  cd SDL_airbyte\\n  ```\\n* download the [Airbyte repository](https://github.com/airbytehq/airbyte) \\n  ```Bash\\n  git clone https://github.com/airbytehq/airbyte.git\\n  ```\\n  Alternatively, only the target connector can be downloaded:\\n  ```Bash\\n  svn checkout https://github.com/airbytehq/airbyte/trunk/airbyte-integrations/connectors/source-github\\n  ```\\n  Here the Airbyte `airbyte/airbyte-integrations/connectors/source-github/` directory is copied into the `SDL_airbyte` directory for handy calling the connector.\\n\\n## [Optional] Inspect the connector specification\\nThe first connector command `spec` provides the connector specification. This is the basis to create a connector configuration. To run the connector as is, the Python `airbyte-cdk` package needs to be installed and the connector can be launched:\\n\\n* Install Python airbyte-cdk: `pip install airbyte_cdk`\\n* try the connector: \\n  ```Bash\\n  cd SDL_airbyte\\n  python source_github/main.py spec | python -m json.tool\\n  ```\\n  This provides a [JSON string](github_spec_out.json) with the connector specification. The fields listed under `properties` are relevant for the configuration (compare with the configuration  used later). \\n\\n## Configuration\\nTo launch Smart Data Lake Builder (SDLB) with the Airbyte connector the following needs to be modified:\\n\\n* add the Airbyte ***dataObject*** with its configuration to the `config/application.conf`:\\n  ```Python\\n  dataObjects {\\n    ext-commits {\\n      type = AirbyteDataObject\\n      config = {\\n        \\"credentials\\": {\\n          \\"personal_access_token\\": \\"<yourPersonalAccessToken>\\" ### enter your personal access token here\\n        },\\n        \\"repository\\": \\"smart-data-lake/smart-data-lake\\",\\n        \\"start_date\\": \\"2021-02-01T00:00:00Z\\",\\n        \\"branch\\": \\"documentation develop-spark3 develop-spark2\\",\\n        \\"page_size_for_large_streams\\": 100\\n      },\\n      streamName = \\"commits\\",\\n      cmd = {\\n        type = CmdScript\\n        name = \\"airbyte_connector_github\\"\\n        linuxCmd = \\"python3 /mnt/source-github/main.py\\"\\n      }\\n    }\\n  ...\\n    stg-commits {\\n     type = DeltaLakeTableDataObject\\n     path = \\"~{id}\\"\\n     table {\\n      db = \\"default\\"\\n      name = \\"stg_commits\\"\\n      primaryKey = [created_at]\\n      }\\n    }\\n  ```\\n  Note the options set for `ext-commits` which define the Airbyte connector settings. \\n  While the `config` varies from connector to connector, the remaining fields are SDL specific. \\n  The `streamName` selects the stream, exactly one. \\n  If multiple streams should be collected, multiple dataObjects need to be defined. \\n  In `linuxCmd` the actual connector script is called. \\n  In our case we will mount the connector directory into the SDL container. \\n\\n* also add the definition of the data stream ***action*** to pipe the coming data stream into a `DeltaLakeTableDataObject`:\\n  ```Bash\\n    actions {\\n      download-commits {\\n        type = CopyAction\\n        inputId = ext-commits\\n        outputId = stg-commits\\n        metadata {\\n          feed = download\\n        }\\n      }\\n  ...\\n  ```\\n* Since Airbyte will be called as Python script in the sdl container, we need to (re-)build the container with Python support and the Python `airbyte-cdk` package. \\n  Therefore, in the Dockerfile we add:\\n\\t```\\n\\tRUN \\\\\\n  apt update && \\\\\\n  apt --assume-yes install python3 python3-pip && \\\\\\n  pip3 install airbyte-cdk~=0.1.25\\n  ```\\n  and rebuild \\n  ```Bash\\n  podman build . -t sdl-spark\\n  ```\\n\\nNow we are ready to go. My full [SDLB config file](application.conf) additionally includes the pull-request stream.\\n\\n## Run and inspect results\\nSince the data will be streamed into a `DeltaLakeTableDataObject`, the metastore container is necessary. Further, we aim to inspect the data using the Polynote notebook. Thus, first these containers are launched using (in the SDL example base directory):\\n```Bash\\npodman-compose up\\npodman pod ls\\n```\\nWith the second command we can verify the pod name and both running containers in it (should be three including the infra container).\\n\\nThen, the SDLB can be launched using the additional option to mount the Airbyte connector directory:\\n```Bash\\npodman run --hostname localhost --rm --pod sdl_airbyte -v ${PWD}/source-github/:/mnt/source-github -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel download\\n```\\n\\nThe output presents the successful run of the workflow:\\n```Bash\\n2022-03-16 07:54:03 INFO  ActionDAGRun$ActionEventListener - Action~download-commits[CopyAction]: Exec succeeded [dag-1-80]\\n2022-03-16 07:54:03 INFO  ActionDAGRun$ - exec SUCCEEDED for dag 1:\\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2510\\n                 \u2502start\u2502\\n                 \u2514\u2500\u2500\u2500\u252c\u2500\u2518\\n                     \u2502\\n                     v\\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n \u2502download-commits SUCCEEDED PT11.686865S\u2502\\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n     [main]\\n2022-03-16 07:54:03 INFO  LocalSmartDataLakeBuilder$ - LocalSmartDataLakeBuilder finished successfully: SUCCEEDED=1 [main]\\n2022-03-16 07:54:03 INFO  SparkUI - Stopped Spark web UI at http://localhost:4040 [shutdown-hook-0]\\n```\\n\\nLaunching Polynote `localhost:8192` in the browser, we can inspect data and develop further workflows. Here an example, where the commits are listed, which were committed in the name of someone else, excluding the web-flow. See [Polynote Notebook](SelectingData.ipynb)\\n![polynote example](polynote_commits.png)\\n\\n## Summary\\n\\nThe Airbyte connectors provide easy access to a variety of data sources. The connectors can be utilized in SDLB with just a few settings. This also works great for more complex interfaces."}]}')}}]);