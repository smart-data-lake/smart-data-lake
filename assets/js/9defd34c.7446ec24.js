"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[5953],{692:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var a=t(4848),s=t(8453);const i={id:"testing",title:"Testing"},o=void 0,r={id:"reference/testing",title:"Testing",description:"Testing is crucial for software quality and maintenance. This is also true for data pipelines.",source:"@site/docs/reference/testing.md",sourceDirName:"reference",slug:"/reference/testing",permalink:"/docs/reference/testing",draft:!1,unlisted:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/reference/testing.md",tags:[],version:"current",frontMatter:{id:"testing",title:"Testing"},sidebar:"tutorialSidebar",previous:{title:"Deploy on Microsoft Azure Databricks",permalink:"/docs/reference/deploy-microsoft-azure"},next:{title:"Use in Notebooks",permalink:"/docs/reference/notebookCatalog"}},l={},c=[{value:"Config validation",id:"config-validation",level:2},{value:"Dry run",id:"dry-run",level:2},{value:"Custom transformation logic unit tests",id:"custom-transformation-logic-unit-tests",level:2},{value:"Simulation of dataframe data pipeline",id:"simulation-of-dataframe-data-pipeline",level:2}];function d(e){const n={code:"code",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"Testing is crucial for software quality and maintenance. This is also true for data pipelines."}),"\n",(0,a.jsx)(n.p,{children:"SDL provides the following possibilities for Testing:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"config validation"})," -> Basic CI test: configuration syntax test without accessing any environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"dry run"})," -> Integration/Smoke Tests: validation of configuration together with a given environment, including validation of data schemas."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"unit tests"}),": test single transformation logic and UDFs by creating standard unit tests."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"data pipeline simulation"}),": test transformation logic over several DataFrame actions by providing mocked input and validating output produced."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"config-validation",children:"Config validation"}),"\n",(0,a.jsxs)(n.p,{children:["Parsing of configuration can be validated by specifying command line parameter ",(0,a.jsx)(n.code,{children:"--test config"})," or programmatically:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-scala",children:'class ConfigTest extends FunSuite with TestUtil {\n  test("validate configuration") {\n    val (registry, globalConfig) = ConfigToolbox.loadAndParseConfig(Seq("src/main/resources"))\n    assert(registry.getActions.nonEmpty)\n  }\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:"This should be done in continuous integration."}),"\n",(0,a.jsx)(n.h2,{id:"dry-run",children:"Dry run"}),"\n",(0,a.jsxs)(n.p,{children:["A dry-run can be started by specifying command line parameter ",(0,a.jsx)(n.code,{children:"--test dry-run"}),".\nThe dry-run executes only prepare and init phase. It validates configuration, checks connections and validates Spark lineage.\nIt doesn't change anything in the environment."]}),"\n",(0,a.jsx)(n.p,{children:"This is suitable for smoke testing after deployment."}),"\n",(0,a.jsx)(n.h2,{id:"custom-transformation-logic-unit-tests",children:"Custom transformation logic unit tests"}),"\n",(0,a.jsx)(n.p,{children:"Logic of custom transformation can easily be unit tested. Example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-scala",children:'class MyCustomTransformerTest extends FunSuite {\n\n  // init spark session\n  val sparkSession = Map("spark.default.parallelism" -> "1", "spark.sql.shuffle.partitions" -> "1", "spark.task.maxFailures" -> "1")\n  lazy val session: SparkSession = GlobalConfig(enableHive = false, sparkOptions = Some(sparkSession))\n    .createSparkSession("test", Some("local[*]"))\n  import session.implicits._\n\n  test("test my custom transformer") {\n\n    // define input\n    val dfInput = Seq(("joe",1)).toDF("name", "cnt")\n\n    // transform\n    val transformer = new MyCustomTransformer\n    val dfsInput = Map("input" -> dfInput)\n    val dfsTransformed = transformer.transform(session, Map(), dfsInput)\n    val dfOutput = dfsTransformed("output")\n\n    // check\n    assert(dfOutput.count == 1)\n  }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"simulation-of-dataframe-data-pipeline",children:"Simulation of dataframe data pipeline"}),"\n",(0,a.jsx)(n.p,{children:'Instead of testing single transformation logic, it can be interesting to test whole pipelines of DataFrame actions.\nFor this you can start a simulation run programmatically in a test suite by providing all input data frames and get the output data frames of the end nodes of the DAG.\nThe simulation mode only executes the init phase with special modification, so it runs without any environment. Of course there are some exceptions like kafka/confluent schema registry.\nNote that simulation mode only supports DataFrame actions for now, you might need to choose "feedSel" accordingly.'}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-scala",children:'class MyDataPipelineTest extends FunSuite with TestUtil {\n\n  // load config\n  val (registry, globalConfig) = ConfigToolbox.loadAndParseConfig(Seq("src/main/resources"))\n\n  // init spark session\n  val sparkSession = Map("spark.default.parallelism" -> "1", "spark.sql.shuffle.partitions" -> "1", "spark.task.maxFailures" -> "1")\n  lazy val session: SparkSession = GlobalConfig(enableHive = false, sparkOptions = Some(sparkSession))\n    .createSparkSession("test", Some("local[*]"))\n  import session.implicits._\n\n  test("enrich and process: create alarm") {\n  \n    // get input1 from modified data object\n    val input1DO = registry.get[CsvFileDataObject]("input1")\n      .copy(connectionId = None, path = "src/test/resources/sample_input1.csv")\n    val dfInput1 = input1DO.getDataFrame()\n\n    // define input2 manually\n    val dfInput2 = Seq(("joe",1)).toDF("name", "cnt")\n\n    // transform\n    val inputSubFeeds = Seq(\n      SparkSubFeed(Some(dfInput1), DataObjectId("input1"), Seq()),\n      SparkSubFeed(Some(dfInput2), DataObjectId("ipnut2"), Seq())\n    )\n    val config = SmartDataLakeBuilderConfig(feedSel = s"my-feed", applicationName = Some("test"), configuration = Some("test"))\n    val sdlb = new DefaultSmartDataLakeBuilder()\n    val (finalSubFeeds, stats) = sdlb.startSimulation(config, inputSubFeeds)\n    val dfOutput = finalSubFeeds.find(_.dataObjectId.id == s"output").get.dataFrame.get.cache\n\n    // check\n    val output = dfOutput.select($"name", $"test").as[(String,Boolean)].collect.toSeq\n    assert(output == Seq(("joe", true)))\n  }\n}\n'})})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var a=t(6540);const s={},i=a.createContext(s);function o(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);