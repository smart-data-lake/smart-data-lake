"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"sdl-data-mesh","metadata":{"permalink":"/blog/sdl-data-mesh","editUrl":"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/blog/2023-09-17-data-mesh/2023-09-17-data-mesh.md","source":"@site/blog/2023-09-17-data-mesh/2023-09-17-data-mesh.md","title":"Data Mesh with SDL","description":"Build a Data Mesh with SDL","date":"2023-09-17T00:00:00.000Z","formattedDate":"September 17, 2023","tags":[{"label":"data mesh","permalink":"/blog/tags/data-mesh"}],"readingTime":5.555,"hasTruncateMarker":false,"authors":[{"name":"Patrick Gr\xfctter","url":"https://github.com/pgruetter"}],"frontMatter":{"title":"Data Mesh with SDL","description":"Build a Data Mesh with SDL","slug":"sdl-data-mesh","authors":[{"name":"Patrick Gr\xfctter","url":"https://github.com/pgruetter"}],"tags":["data mesh"],"hide_table_of_contents":false},"nextItem":{"title":"Housekeeping","permalink":"/blog/sdl-housekeeping"}},"content":"Data Mesh is an emerging concept gaining momentum across organizations.\\nIt is often described as a _sociotechnical_ paradigm because a paradigm shift towards Data Mesh does not simply involve \\ntechnological changes but also has sociological implications.\\nAs such, discussing a technical framework like Smart Data Lake Builder and analyzing how well it fits the \\nData Mesh paradigm can inherently only be part of the whole picture. \\nNevertheless, we want to do the exercise here to see how a technological framework can support the adoption.\\n\\nIn this article, we\'ll explore how the Smart Data Lake Builder aligns with the four principles as outlined by \\n[Zhamak Dehghani](https://martinfowler.com/articles/data-mesh-principles.html)\\nand assess which key concepts it supports. \\n\\nPlease note that we expect some familiarity with the Data Mesh principles to follow along.\\nThere are plenty of resources (books from [O\'Reilly](https://www.oreilly.com/library/view/data-mesh/9781492092384/)\\nand [Manning](https://www.manning.com/books/data-mesh-in-action), \\n[articles](https://martinfowler.com/articles/data-mesh-principles.html)\\nand [dedicated websites](https://www.datamesh-architecture.com/)) if you want to dive deeper into the topic.\\n\\n\\n## Domain Ownership\\n_Individual domains or business units are responsible for the data they generate and maintain, \\nfostering a sense of ownership and accountability._\\n\\n\\n### Scale Out Complexity\\nWith a Data Mesh, your data organization can scale out as the centralized data team gets reduced. \\nWith increased usage, you will have more data sources, more consumers, more interfaces and with that, \\na large variety of systems and technologies. \\n\\n*SDLB* comes with a large set of connectors out-of-the-box with the addition of \\n[Airbyte](https://airbyte.com/connectors?connector-type=Sources) source connectors.\\nAnything missing can be easily implemented as the whole ecosystem is open and builds on open standards.\\n\\n### Continuous change\\nData Mesh embraces continuous change in your Data Products. \\nChanging source systems or additional requirements all require you to adapt (quickly).\\n\\n*SDLB* is built for changes.\\nWith built-in schema evolution, changing schemas can be handled automatically.\\nThanks to the declarative approach, adding additional data objects can be done in minutes \\nand the dependencies don\'t need to be defined by hand.\\n\\n\\n## Data as a Product\\n_Data is treated as a product, with clear documentation, standardized interfaces, \\nand well-defined quality measures to ensure its usability and value._\\n\\nIt\'s important to note that in the end, data products can be implemented in different technologies\\nas long as they adhere to the agreed upon interfaces.\\n\\nAgain, there are different aspects where *SDLB* can help.\\n\\n### Data Quality\\nA Data Product has well-defined quality measures and should define Service-Level Objectives (SLOs).\\n\\n*SDLB* builds data quality into your pipelines. \\nIt supports [constraints](../../docs/reference/dataQuality#constraints) and \\n[expectations](../../docs/reference/dataQuality#expectations)\\nthat are evaluated each time a pipeline is executed instead of having downstream checks after the execution.\\n\\n*SDLB* also gathers metrics with every run.\\nWith the metrics gathered, you can detect anomalies like number of records or bytes written.\\nThese are often helpful in finding latent problems in your data pipeline.\\n\\n### Schema enforcement\\nAs another aspect of quality, you expect your source system to adhere to the agreed interface.\\n\\n*SDLB* supports schema validation for all its data objects.\\nIn your configuration, you can (optionally) define a schema and enforce it.\\nThis saves you from surprises if an upstream system suddenly changes the schema of a data object you use.\\n\\n### Lineage\\nTo provide transparency about the origin and transformations of data in your Data Product,\\na lineage diagram can greatly help.\\n\\n*SDLB* is a metadata driven framework. \\nYou define Data Objects, you define Actions between them. That\'s it.\\nSDLB will figure out the dependencies at runtime but they can also be easily displayed from static configurations.\\nThis is done i.e. in our [sdl-visualization](https://github.com/smart-data-lake/sdl-visualization), see also the [UI-Demo](https://ui-demo.smartdatalake.ch/).\\nSDL configuration files can be easily displayed and analyzed. \\nThis provides the needed transparency of a Data Product. \\n\\n### Interfaces\\n*SDLB* offers many standard Data Objects for all kinds of interfaces.\\nWith the declarative approach and schema enforcement, you can make sure that your data always adheres to the\\ndefined interface specification.\\n\\nOften in Cloud Environments, we work with Delta or Iceberg Tables today. \\nThese open standards are a powerful tool to share your Data Product, even in multicloud deployments.\\n\\n## Self-Serve infrastructure platform\\n_Data infrastructure is designed to be self-serve, enabling teams to access and manage their data autonomously without \\nrelying on central data teams._\\n\\n### Mobilize more developers\\nWith increased usage of the Mesh, you need more developers in your domain teams building Data Products.\\n\\n*SDLB* has an easy to learn, declarative approach that new users can learn quickly.\\nIt supports SQL, Scala and Python for the definition of transformations so your developers can still\\nuse the language they are most comfortable in.\\nComplex logic and transformations like historization and deduplication are either built-in or can be extended \\nin a generic way so your developers can leverage them easily.\\n\\n### Support your domain teams\\nThe self-serve infrastructure should support your domain team to quickly build new Data Products.\\n\\n*SDLB* is ideal to integrate into DevOps pipelines as all configurations are written in textfile based HOCON format.\\nEven if you have custom code, it will be written in SQL, Scala or Python which also integrates well\\nin any existing DevOps environments. \\nIt is designed to support code reviews, automated testing and deployment.\\nWith the right templates, your domain teams can set up new Data Products quickly.\\n\\n## Federated governance\\n_Governance of data and its quality is distributed across domains, with a focus on collaboration, standardized practices, \\nand federated decision-making processes to maintain data integrity and trust._\\n\\nWhile many of the concepts for a federated governance are on an organizational level and not on a technical level, \\n*SDLB* can again help on various points.\\n\\n\\n### Data Catalogs\\nWhen it comes to Data Catalogs, there is no open, agreed upon standard (yet). \\n\\n*SDLB*\'s configuration files are open and can be visualized as mentioned before.\\nThe concepts of Data Objects and Actions are very similar to many Data Catalog notations and \\nwith that, they also integrate nicely in existing solutions. \\nThere is i.e. an exporter to [Apache Atlas](https://atlas.apache.org/#/) which also builds the basis of \\n[Azure Purview](https://azure.microsoft.com/en-us/products/purview).\\n\\n### Encryption\\nOne aspect with increasing importance is privacy and compliance. \\nFor these topics, it helps to have a unified, generic implementation to prevent your domain teams to each\\nstart their own implementation. \\n\\n*SDLB* now supports encrypted columns that help i.e. with PII data (Personally Identifiable Information).\\n\\n\\n# Summary\\nThere are a lot of aspects to consider when adopting a Data Mesh, many of which are on a technical level.\\nWhile you want to give your domain teams the independence to choose the technologies they know best, \\nyou still want a framework that is easy to learn, quick to adapt and open to work with any other Data Product.\\nThis article hopefully gave you a basic overview of how Smart Data Lake Builder can help you."},{"id":"sdl-housekeeping","metadata":{"permalink":"/blog/sdl-housekeeping","editUrl":"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/blog/2023-06-08-SDL-Housekeeping/2023-06-08-housekeeping.md","source":"@site/blog/2023-06-08-SDL-Housekeeping/2023-06-08-housekeeping.md","title":"Housekeeping","description":"Use advanced housekeeping functions to optimize execution","date":"2023-06-08T00:00:00.000Z","formattedDate":"June 8, 2023","tags":[{"label":"housekeeping","permalink":"/blog/tags/housekeeping"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"partitioning","permalink":"/blog/tags/partitioning"}],"readingTime":5.87,"hasTruncateMarker":false,"authors":[{"name":"Patrick Gr\xfctter","url":"https://github.com/pgruetter"}],"frontMatter":{"title":"Housekeeping","description":"Use advanced housekeeping functions to optimize execution","slug":"sdl-housekeeping","authors":[{"name":"Patrick Gr\xfctter","url":"https://github.com/pgruetter"}],"tags":["housekeeping","performance","partitioning"],"hide_table_of_contents":false},"prevItem":{"title":"Data Mesh with SDL","permalink":"/blog/sdl-data-mesh"},"nextItem":{"title":"Incremental historization using CDC and Airbyte MSSQL connector","permalink":"/blog/sdl-hist"}},"content":"In this article, we\'re taking a look on how we use SDLB\'s housekeeping features to keep our pipelines running efficiently.\\n\\nSome DataObject contain housekeeping features of their own. \\nMake sure you use them!\\nFor example, Delta Tables support commands like `optimize` and `vacuum` to optimize storage and delete no longer needed files.\\n\\nBut usually, those commands do not re-organize your partitions. \\nThis is where SDLBs housekeeping mode comes in.\\n\\nThe example is taken from a real world project we\'ve implemented.\\n\\n## Context\\nIn this particular project we are collecting data from various reporting units and process it in batches.\\nThe reporting units use an Azure Function to upload JSON files to an Azure Data Lake Storage. \\nFrom there, we pick them up for validation and processing. \\nReporting units can upload data anytime, but it is only processed a few times a day.\\n\\nOnce validated, we use Delta Lake tables in Databricks to process data through the layers of the Lakehouse.\\n\\n## Partitioning\\nThe Azure Function puts uploaded JSON files in a subfolder for each reporting unit. \\nAs such, JSON files are already neatly partitioned by `reporting_unit`:\\n```\\nuploadFolder/\\n  reporting_unit=rp01\\n    file1.json\\n    file2.json\\n    file3.json\\n  reporting_unit=rp02\\n    file1.json\\n  reporting_unit=rp03\\n    fileX.json\\n```\\n\\nTo read these JSON files, we can therefore use the following DataObject definition:\\n```\\nimport_json {\\n    type = JsonFileDataObject\\n    path = uploadFolder/\\n    partitions = [reporting_unit]    \\n}\\n```\\n\\nThese files are then processed with a `FileTransferAction` into an output DataObject `stage_json`: \\n```\\nstage_json {\\n    type = FileTransferAction\\n    inputId = import_json\\n    outputId = stage_json\\n    executionMode = { type = FileIncrementalMoveMode }\\n    metadata.feed = stage_json\\n}\\n```\\n\\nEach time we start to process uploaded data, we use the `run_id` to keep track of all batch jobs and version of files delivered.\\nIf you use a state path (see [commandLine](../../docs/reference/commandLine)), \\nyour runs automatically generate a `run_id` to identify the run, and you can use it by extending your DataObject:\\n\\n```\\nstage_json {\\n    type = JsonFileDataObject\\n    path = processedFolder\\n    partitions = [run_id,reporting_unit]\\n    schema = \\"\\"\\"reporting_unit string, run_id string, ....\\"\\"\\"\\n}\\n```\\nNote how we just use run_id as part of the schema without any further declaration.\\nSince we use the state path, SDLB uses a `run_id` internally, and if it\'s referenced as partition column in a DataObject, processed data get automatically assigned the id of the current run.\\n\\n## Drawback\\nLet\'s take a look at the resulting partition layout of `stage_json`:\\n```\\nprocessedFolder/\\n  run_id=1/\\n    reporting_unit=rp01/\\n      file1.json\\n      file2.json\\n      file3.json\\n    reporting_unit=rp02/\\n      file1.json\\n    reporting_unit=rp03/\\n      fileX.json\\n```\\nThis partition layout has many advantages in our case as we know exactly \\nduring which run a particular file was processed and which reporting unit uploaded it.\\nIn further stages we can clearly work with files that were processed in the current run and not touch any old `run_id`s. \\n\\nFor this use case, a few things are important to note:\\n  - Some reporting units don\'t upload data for days. You end up with only a few reporting_unit partitions per run_id.\\n  - File sizes are rather small (< 1 MiB), partition sizes end up very small too.\\n  - If you use hourly runs and run 24/7, you end up with 168 partitions per week, plus sub-partitions for reporting units.\\n  - Once files are correctly processed, we don\'t read the uploaded files anymore. \\n    We still keep them as raw files should we ever need to re-process them.\\n\\nThe drawback becomes apparent when you have actions working with all partitions, they will become very slow.\\nSpark doesn\'t like a lot of small partitions.\\n\\nTo mitigate that, we use SDLB\'s Housekeeping Feature.\\n\\n## HousekeepingMode\\nIf you take a look at DataObject\'s parameters, you will see a `housekeepingMode`. \\nThere are two modes available:\\n  - **PartitionArchiveCompactionMode**: to compact / archive partitions  \\n  - **PartitionRetentionMode**: to delete certain partitions completely\\n\\n### PartitionArchiveCompactionMode\\nIn this mode, you solve two tasks at once:\\n  - You define how many smaller partitions are aggregated into one larger partition (archive)\\n  - Rewrite all files in a partition to combine many small files into larger files (compact)\\n\\n\\n#### Archive\\nIn our example above, we stated that we don\'t want to alter any input files, so we won\'t use compaction.\\nWe want to keep them as is (raw data). \\nBut we do want to get rid of all the small partitions after a certain amount of time. \\nFor that, we extend `stage_json` to include the `housekeepingMode` with a `archivePartitionExpression`:\\n\\n```\\nstage_json {\\n    type = JsonFileDataObject\\n    path = processedFolder\\n    partitions = [run_id,reporting_unit]\\n    schema = \\"\\"\\"reporting_unit string, run_id string, ....\\"\\"\\"\\n    housekeepingMode = {\\n      type = PartitionArchiveCompactionMode\\n      archivePartitionExpression = \\"if( elements.run_id < (runId - 500), map(\'run_id\', (cast(elements.run_id as integer) div 500) * 500, \'reporting_unit\', elements.reporting_unit), elements)\\"\\n    }\\n}\\n```\\n\\nThis expression probably needs some explanation:  \\nThe Spark SQL expression works with attributes of [`PartitionExpressionData`](https://github.com/smart-data-lake/smart-data-lake/blob/master-spark3/sdl-core/src/main/scala/io/smartdatalake/workflow/dataobject/HousekeepingMode.scala#L136).\\nIn this case we use `runId` (the current runId) and `elements` (all partition values as map(string,string)).\\nIt needs to return a map(string,string) to define new partition values. \\nIn our case, it needs to define `run_id` and `reporting_unit` because these are the partitions defined in `stage_json`.\\n\\nLet\'s take the expression apart:  \\n`if(elements.run_id < (runId - 500), ...`  \\nOnly archive the partition if it\'s runId is older than 500 run_ids ago. \\n\\n`map(\'run_id\', (cast(elements.run_id as integer) div 500) * 500, \'reporting_unit\', elements.reporting_unit)`  \\nCreates the map with the new values for the partitions. \\nThe run_id is floored to the next 500 value, so as example, the new value of run_id 1984 will be 1500 (because integer 1984/500=3, 3*500=1500).  \\nRemember that we need to return all partition values in the map, also the ones we don\'t want to alter.\\nFor `reporting_unit` we simply return the existing value `elements.reporting_unit`.\\n\\n`..., elements)`  \\nThis is the else condition and simply returns the existing partition values if there is nothing to archive.\\n\\n:::info\\nThe housekeeping mode is applied after writing a DataObject.\\nKeep in mind, that it is executed with every run.\\n:::\\n\\n#### Compaction\\nWe don\'t want to compact files in our case. \\nBut from the documentation you can see that compaction works very similarly:  \\nYou also work with attributes from `PartitionExpressionData` but instead of new partition values, \\nyou return a boolean to indicate for each partition if it should be compacted or not. \\n\\n### PartitionRetentionMode\\nAgain, not used in our example as we never delete old files.\\nBut if you need to, you define a Spark SQL expression returning a boolean indicating if a partition should be retained or deleted.\\n\\n```\\nstage_json {\\n    type = JsonFileDataObject\\n    path = processedFolder\\n    partitions = [run_id,reporting_unit]\\n    schema = \\"\\"\\"reporting_unit string, run_id string, ....\\"\\"\\"\\n    housekeepingMode = {\\n      type = PartitionRetentionMode\\n      retentionCondition = \\"elements.run_id > (runId - 500)\\"\\n    }\\n}\\n```\\n\\n## Result\\nIn our example, we had performance gradually decreasing because Spark had to read more than 10\'000 partitions and subpartitions.\\nJust listing all available partitions, even if you only worked with the most recent one, took a few minutes and these operations added up.\\n\\nWith the housekeeping mode enabled, older partitions continuously get merged into larger partitions containing up to 500 runs. \\nThis brought the duration of list operations back to a few seconds.\\n\\nThe operations are fully automated, no manual intervention is required."},{"id":"sdl-hist","metadata":{"permalink":"/blog/sdl-hist","editUrl":"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/blog/2022-05-11-SDL-historization/2022-05-11-historization.md","source":"@site/blog/2022-05-11-SDL-historization/2022-05-11-historization.md","title":"Incremental historization using CDC and Airbyte MSSQL connector","description":"Tracking Data Changes of MSSQL databases with and without CDC","date":"2022-05-11T00:00:00.000Z","formattedDate":"May 11, 2022","tags":[{"label":"historization","permalink":"/blog/tags/historization"},{"label":"MSSQL","permalink":"/blog/tags/mssql"},{"label":"incremental","permalink":"/blog/tags/incremental"},{"label":"CDC","permalink":"/blog/tags/cdc"}],"readingTime":12.525,"hasTruncateMarker":true,"authors":[{"name":"Mandes Sch\xf6nherr","title":"Dr.sc.nat.","url":"https://github.com/mand35"}],"frontMatter":{"title":"Incremental historization using CDC and Airbyte MSSQL connector","description":"Tracking Data Changes of MSSQL databases with and without CDC","slug":"sdl-hist","authors":[{"name":"Mandes Sch\xf6nherr","title":"Dr.sc.nat.","url":"https://github.com/mand35"}],"tags":["historization","MSSQL","incremental","CDC"],"hide_table_of_contents":false},"prevItem":{"title":"Housekeeping","permalink":"/blog/sdl-housekeeping"},"nextItem":{"title":"Deployment on Databricks","permalink":"/blog/sdl-databricks"}},"content":"In many cases datasets have no constant live. New data points are created, values changed and data expires. We are interested in keeping track of all these changes.\\nThis article first presents collecting data utilizing **JDBC** and **deduplication on the fly**. Then, a **Change Data Capture** (CDC) enabled (MS)SQL table will be transferred and historized in the data lake using the **Airbyte MS SQL connector** supporting CDC. Methods for reducing the computational and storage efforts are mentioned.\\n\\n\x3c!--truncate--\x3e\\n\\nIn the [getting-started -> part2 -> keeping historical data](../../docs/getting-started/part-2/historical-data) historization is already introduced briefly. Here, we go in slightly more detail and track data originating from an MS SQL database. For the sake of simplicity, the tools and systems are deployed in Podman containers, including SDLB, MSSQL server, as well as the metastore and polynote. \\n\\nHere, a workflow is modeled, gathering data from a (MS)SQL database to the Data Lake. Therefore, the following steps will be performed:\\n\\n* initializing a MS SQL server\\n* importing data into MS SQL table\\n* injecting data into the data lake\\n* modifying data on the SQL server side\\n* re-copy / update data into data lake\\n\\nThe data will be inspected and monitored using a Polynote notebook.\\n\\n\\n## Test Case\\nAs a test case a [Chess Game Dataset](https://www.kaggle.com/datasets/datasnaek/chess) is selected. This data is a set of 20058 rows, in total 7MB. This is still faily small, but should be kind of representative.\\n\\nThe dataset will be imported into the SQL server using the [db_init_chess.sql](db_init_chess.sql) script, which should be copied into the `config` directory. \\n\\nIt should be noted that there are a duplicates in the dataset. In the first case, the *deduplication* will be performed when the data is ported into the data lake (see configuration below). The procedure for table creation and modification is described below. \\n\\n## Prerequisites\\n\\n* Podman installation\\n* SDLB with metastore and Polynote by cloning the getting-started example: \\n  ```Bash\\n  git clone https://github.com/smart-data-lake/getting-started.git SDL_sql\\n\\tcd SDL_sql\\n\\tunzip part2.additional-files.zip\\n  ```\\n  :::note \\"Directory Naming\\"\\n  Note: The directory name \\"SDL_sql\\" will be related to the pod created later and thus to the specified commands below.\\n  :::\\n* Utilizing JDBC to MS SQL server, SDLB required this additional dependency. Therefore, add the following dependency to the `pom.xml`:\\n\\t```xml\\n\\t<dependency>\\n\\t\\t<groupId>com.microsoft.sqlserver</groupId>\\n\\t\\t<artifactId>mssql-jdbc</artifactId>\\n\\t\\t<version>10.2.0.jre11</version>\\n\\t</dependency>\\n\\t```\\n* build sdl-spark: `podman build -t sdl-spark .`\\n* build the SDLB objects: \\n\\t```Bash\\n\\tmkdir .mvnrepo\\n\\tpodman run -v ${PWD}:/mnt/project -v ${PWD}/.mvnrepo:/mnt/.mvnrepo maven:3.6.0-jdk-11-slim -- mvn -f /mnt/project/pom.xml \\"-Dmaven.repo.local=/mnt/.mvnrepo\\" package\\n\\t```\\n* download the test case data from [Kaggle](https://www.kaggle.com/datasets/datasnaek/chess/download) and unzip into `SDL_sql/data` directory\\n* copy polynote notebook [sql_data_monitor.ipynb](sql_data_monitor.ipynb) for later inspection into the `polynote/notebooks` directory\\n\\n:::warning\\n  The notebook will only be editable if the permissions are changed to be writable by other users `chmod -R 777 polynote/notebooks`\\n:::\\n\\n* script for creating table on the SQL server: [db_init_chess.sql](db_init_chess.sql) into the `config` directory\\n* script for modifying the table on the SQL server: [db_mod_chess.sql](db_mod_chess.sql) into the `config` directory\\n* a restart script [restart_databases.sh](restart_databases.sh) is provided to clean and restart from scratch, including: stopping the containers, cleaning databases, freshly starting the containers and initializing the SQL database \\n\\n## Prepare Source Database\\n* start the pod with the metastore and polynote: \\n  ```Bash\\n  mkdir -p data/_metastore\\n  ./part2/podman-compose.sh #use the script from the getting-started guide\\n  ```\\n* start the MS SQL server: \\n  ```Bash\\n  podman run -d --pod sdl_sql --hostname mssqlserver --add-host mssqlserver:127.0.0.1 --name mssql -v ${PWD}/data:/data  -v ${PWD}/config:/config -e \\"ACCEPT_EULA=Y\\" -e \\"SA_PASSWORD=%abcd1234%\\" mcr.microsoft.com/mssql/server:2017-latest\\n  ```\\n* initialize the database: \\n  ```Bash\\n  podman exec -it mssql /opt/mssql-tools/bin/sqlcmd -S mssqlserver -U sa -P \'%abcd1234%\' -i /config/db_init_chess.sql\\n  ```\\n* list the table: \\n  ```Bash\\n  podman exec -it mssql /opt/mssql-tools/bin/sqlcmd -S mssqlserver -U sa -P \'%abcd1234%\' -Q \\"SELECT count(*) FROM foobar.dbo.chess\\"\\n  podman exec -it mssql /opt/mssql-tools/bin/sqlcmd -S mssqlserver -U sa -P \'%abcd1234%\' -Q \\"SELECT * FROM foobar.dbo.chess WHERE id = \'079kHDqh\'\\"\\n  ```\\n  This should report 20058 row and we see an example of duplicates.\\n\\n:::note\\n  This could be shortened, by just calling the [`bash restart_databases.sh`](restart_databases.sh), which contains the above commands, after stopping the containers and cleaning directories. \\n:::\\n\\n## Define Workflow\\nThe SDLB configuration file consists of:\\n* global settings for the metastore \\n* connection details \\n* data objects and\\n* action\\n\\nCreate the `config/chess.conf` file with the following described sections or copy the [full script](chess.conf).\\n\\n### Spark Settings\\nFor the metastore, the location, driver and access is defined. Further, the amount of tasks and partitions are limited, due to our reasonable small problem size.\\n\\n```hocon\\nglobal {\\n  spark-options {\\n    \\"spark.hadoop.javax.jdo.option.ConnectionURL\\" = \\"jdbc:derby://metastore:1527/db;create=true\\"\\n    \\"spark.hadoop.javax.jdo.option.ConnectionDriverName\\" = \\"org.apache.derby.jdbc.ClientDriver\\"\\n    \\"spark.hadoop.javax.jdo.option.ConnectionUserName\\" = \\"sa\\"\\n    \\"spark.hadoop.javax.jdo.option.ConnectionPassword\\" = \\"1234\\"\\n    \\"spark.databricks.delta.snapshotPartitions\\" = 2\\n    \\"spark.sql.shuffle.partitions\\" = 2\\n  }\\n}  \\n```\\n\\n### Connection\\nThe connection to the local MS SQL server is specified using JDBC settings and clear text authentication specification. In practice, a more secure authentication mode should be selected, e.g. injection by environment variables. \\n\\n```hocon\\nconnections {\\n  localSql {\\n    type = JdbcTableConnection\\n    url = \\"jdbc:sqlserver://mssqlserver:1433;encrypt=true;trustServerCertificate=true\\"\\n    driver = com.microsoft.sqlserver.jdbc.SQLServerDriver\\n    authMode {\\n      type = BasicAuthMode\\n      userVariable = \\"CLEAR#sa\\"\\n      passwordVariable = \\"CLEAR#%abcd1234%\\"\\n    }\\n  }\\n}\\n```\\n\\n\\n### DataObjects\\n\\nIn a first place, two DataObjects are defined. The `ext-chess` defining the external source (the table on the MS SQL server), using JDBC connection. The `int-chess` defines a delta lake table object as integration layer as targets for our ingestion/historization action. \\n\\n```hocon\\ndataObjects {\\n  ext-chess {\\n    type = JdbcTableDataObject\\n    connectionId = localSql\\n    table = {\\n      name = \\"dbo.chess\\"\\n      db = \\"foobar\\"\\n    }\\n  }\\n  int-chess {\\n    type = DeltaLakeTableDataObject\\n    path = \\"~{id}\\"\\n    table {\\n      db = \\"default\\"\\n      name = \\"int_chess\\"\\n      primaryKey = [id]\\n    }\\n  }\\n  #...\\n}\\n```\\n\\n### Actions\\nThe `histData` action specifies the copy and historization of data, by setting the type **HistorizeAction**. Therewith, the incoming data will be joined with the existing data. Each data point gets two additional values: **dl_ts_captured** (time the data is captured in the data lake) and **dl_ts_delimited** (time of invalidation). In case of a data record change, the original row gets invalidated (with the current time) and a new row is added with the current time as capturing value. As long as the data point is active, **dl_ts_delimited** is set to max date `9999-12-31 23:59:59.999999.\\n\\nThe default HistorizationAction algorithm compares all new data with all the existing data, row by row **AND** column by column. Further, the complete joined table is re-written to the data lake. \\nFor DataObjects supporting transactions HistorizeAction provides an algorithm using a merge operation to do the historization. By selecting `mergeModeEnable = true` the resulting table gets another column with `dl_hash`. This hash is used to compare rows much more efficiently. Not every column need to be compared, only the hash. Further, already existing rows (identified by the hash), do not need to be re-written. The merge operation applies only the needed inserts and updates to the output DataObject.\\n\\nFurthermore, a *transformer* needs to be added to deduplicate the input data, which has duplicated rows with slightly different game times. This is needed as HistorizationAction expects the input to be unique over the primary key of the output DataObject.\\n\\n```hocon\\nactions {\\n  histData {\\n    type = HistorizeAction\\n    mergeModeEnable = true\\n    inputId = ext-chess\\n    outputId = int-chess\\n    transformers = [{\\n      type = ScalaCodeDfTransformer\\n      code = \\"\\"\\"\\n        import org.apache.spark.sql.{DataFrame, SparkSession}\\n        (session:SparkSession, options:Map[String, String], df:DataFrame, dataObjectId:String) => {\\n          import session.implicits._\\n          df.dropDuplicates(Seq(\\"id\\"))\\n        }\\n      \\"\\"\\"\\n   }]\\n   metadata {\\n      feed = download\\n    }\\n  }\\n  #...\\n}\\n```\\n\\nThe full configuration looks like [chess.conf](chess.conf). Note that there are already further DataObjects and Actions defined, described and used later. \\n\\n## Run\\n\\nThe Pod with metastore, polynote and the \\"external\\" SQL server should already being running. Now the SDLB container is launched within the same POD and the action histData ingests the data into the data lake:\\n\\n```Bash\\npodman run --hostname localhost -e SPARK_LOCAL_HOSTNAME=localhost --rm --pod sdl_sql -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark --config /mnt/config --feed-sel ids:histData\\n```\\n\\nThe data can be inspected using the Polynote notebook (`sql_data_monitor.ipynb` downloaded above), which can be launched using [Polynote (click here)](http://localhost:8192/notebook/sql_data_monitor.ipynb). \\n\\nNow, let\'s assume a change in the source database. Here the `victory_status` `outoftime` is renamed to `overtime`. Furthermore one entry is deleted. Run script using:\\n\\n```Bash\\npodman exec -it mssql /opt/mssql-tools/bin/sqlcmd -S mssqlserver -U sa -P \'%abcd1234%\' -i /config/db_mod_chess.sql\\n```\\nNote: 1680 rows were changed + 1 row deleted.\\n\\nFurthermore, the data lake gets updated using the same command as above:\\n```Bash\\npodman run --hostname localhost -e SPARK_LOCAL_HOSTNAME=localhost --rm --pod sdl_sql -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark --config /mnt/config --feed-sel ids:histData\\n```\\n\\nIn the [Polynote (click here)](http://localhost:8192/notebook/sql_data_monitor.ipynb) sql_data_monitor.ipynb, the data lake table can be inspected again. The table and its additional columns are presented, as well as the original and updated rows for a modified row (*id = QQ3iIM2V*) and the deleted row (*id = 009mKOEz*).\\n\\n![polynote example output](historization_res.png)\\n\\n## Change Data Capture\\n\\nSo far the whole table is collected and compared with the existing data lake table. \\nVarious databases support Change Data Capture (CDC). CDC already keeps track of changes similar to the comparision done by the historization feature of SDL. \\nThis can be used to optimize performance of the historization feature, gathering only database updates, reducing the amount of transferred and compared data significantly. Therewith, only data changed (created, modified, or deleted) since the last synchronisation will be read from the database. It needs the status of the last synchronisation to be attached to the successful stream. This **state** is handled in SDLB. \\n\\nIn the following, an example is presented utilizing the MS SQL server CDC feature. Since the implemented JDBC connector cannot handle CDC data, an Airbyte connector is utilized, see [Airbyte CDC](https://docs.airbyte.com/understanding-airbyte/cdc/) for details.\\n\\n### Enable CDC on the SQL Server\\nFirst the SQL server need to be configured to have a table [CDC enabled](https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/enable-and-disable-change-data-capture-sql-server?view=sql-server-ver15). Additionally to the used table *chess*, another table is created with the CDC feature enabled. This table *chess_cdc* is created by copying and deduplicating the original table. The table creation is already performed in the above introduced and used [MS SQL initalization](db_init_chess.sql) script. In practice, further SQL settings should be considered, including proper user and permissions specification, and an adaptation of the retention period (3 days default). \\n\\nFurthermore, the SQL agent need to be enabled. Therefore, the database need to be restarted (here container is restarted). This is handled in the above mentioned [restart_databases.sh](restart_databases.sh) script. If not already used above, run:\\n\\n```Bash\\nbash restart_databases.sh\\n```\\n\\n\\nLet\'s double check the CDC enabled table:\\n\\n```Bash\\npodman exec -it mssql /opt/mssql-tools/bin/sqlcmd -S mssqlserver -U sa -P \'%abcd1234%\' -Q \\"SELECT * FROM foobar.cdc.dbo_chess_cdc_CT where id = \'079kHDqh\'\\"\\n```\\n\\nHere the first 5 columns are added for the CDC. \\nIt should be noted that CDC additional data vary by implementation from different DB products.\\n\\n### Airbyte MSSQL connector\\nSince Sparks JDBC data source (used above) does not support CDC data, the connector is changed to [Airbyte MSSQL](https://docs.airbyte.com/integrations/sources/mssql). \\nIn contrast to the article [Using Airbyte connector to inspect github data](sdl-airbyte), where Airbyte github connector ran as python script, here the connector runs as a container within the SDLB container. \\nI targeted a setup using: WSL2, SDLB with podman, and the Airbyte container with podman in the SDLB container. Unfortunately, there are issues with fuse overlay filesystem when using container in container with podman. Therefore, I switched to [buildah](https://buildah.io/) in the SDLB container. Unfortunately, the entrypoint is not recognized as expected. As a workaround the following script corrects this. \\nI guess in another environment, e.g. in a cloud environment or just using docker in docker, this would work out of the box. \\n\\nHere are my steps to get it running:\\n* add buildah and the Airbyte container to the SDLB [Dockerfile](Dockerfile.txt) (just before the entrypoint):\\n  ```Bash\\n  RUN apt-get update\\n  RUN apt-get -y install buildah\\n  RUN echo \'unqualified-search-registries=[\\"docker.io\\"]\' >> /etc/containers/registries.conf\\n  RUN buildah --storage-driver=vfs from --name airbyte-mssql docker.io/airbyte/source-mssql\\n  ```\\n* rebuild the SDLB container: `podman build -t sdl-spark .`\\n* workaround script to parse all arguments correctly in the Airbyte container while using buildah without the proper entrypoint. Copy [start_buildah.sh](start_buildah.sh) script into `config` directory\\n\\n### SDLB configuration\\nNow, the related DataObjects and Action are added to the SDLB configuration [config/chess.conf](chess.conf).\\n\\n#### DataObject\\nAs a source object the AirbyteDataObject is used. Again the MSSQL server with the user credentials are specified. Further, in the streamName the table is selected. The cmd specifies how to run Airbyte. Here the mentioned workaround script is called in the container. As target, again a Delta Lake table is chosen, here called `int_chess_cdc`. Practically, we would prevent of duplicating tables, here we create a new one to provide the possibility to compare both results. \\n```hocon\\ndataObjects {\\n    ext-chess-cdc {\\n    type = AirbyteDataObject\\n    config = {\\n      host = \\"mssqlserver\\"\\n      port = 1433\\n      database = \\"foobar\\"\\n      username = \\"sa\\"\\n      password = \\"%abcd1234%\\"\\n      replication_method = \\"CDC\\"\\n    },\\n    streamName = \\"chess_cdc\\",\\n    cmd = {\\n      type = DockerRunScript\\n      name = \\"airbyte_source-mssql\\"\\n      image = \\"airbyte-mssql\\"\\n      linuxDockerCmd = \\"bash /mnt/config/start_buildah.sh\\"\\n    }\\n  }\\n  int-chess-cdc {\\n    type = DeltaLakeTableDataObject\\n    path = \\"~{id}\\"\\n    table {\\n      db = \\"default\\"\\n      name = \\"int_chess_cdc\\"\\n      primaryKey = [id]\\n    }\\n  }\\n}\\n```\\n#### Action\\nAgain the SDLB action *HistorizeAction* with *mergeModeEnabled* is selected. Further, the incremental execution mode is enabled. With CDC we get only changed data, which will be merged with the SDL existing table. Further, the column and value need to be specified to identify deleted data points. Depending on the CDC implementation this could be the deletion date (like we have here) or an operation flag mentioning the deletion operation as code, or even differently. SDLB expects a fixed value for deletion. That\'s why we specify an `AdditionalColumnsTransformer` transformer to first create an intermediate column mapping any date to *true*. \\n```hocon\\nactions {\\n  histDataAirbyte {\\n    type = HistorizeAction\\n    mergeModeEnable = true\\n    executionMode = { type = DataObjectStateIncrementalMode }\\n    inputId = ext-chess-cdc\\n    outputId = int-chess-cdc\\n    mergeModeCDCColumn = \\"cdc_deleted\\"\\n    mergeModeCDCDeletedValue = true\\n    transformers = [{\\n      type = AdditionalColumnsTransformer\\n      additionalDerivedColumns = {cdc_deleted = \\"_ab_cdc_deleted_at is not null\\"}\\n    }]\\n    metadata {\\n      feed = download\\n    }\\n  }\\n}\\n```\\n\\nFinally, SDLB is launched using the same settings as above, now with the feed `ids:histDataAirbyte` and specifying the *state* directory and name:\\n```Bash\\npodman run --hostname localhost -e SPARK_LOCAL_HOSTNAME=localhost --rm --pod sdl_sql -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark --config /mnt/config --feed-sel ids:histDataAirbyte --state-path /mnt/data/state -n SDL_sql\\n```\\nAgain the SQL database modification is processed using [config/db_mod_chess_cdc.sql](db_mod_chess_cdc.sql): \\n```Bash\\npodman exec -it mssql /opt/mssql-tools/bin/sqlcmd -S mssqlserver -U sa -P \'%abcd1234%\' -i /config/db_mod_chess_cdc.sql\\n```\\nAnd the SDL update with the same command as just used:\\n```Bash\\npodman run --hostname localhost -e SPARK_LOCAL_HOSTNAME=localhost --rm --pod sdl_sql -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark --config /mnt/config --feed-sel ids:histDataAirbyte --state-path /mnt/data/state -n SDL_sql\\n```\\n\\nAgain the delta lake tables can be inspected using [Polynote (click here)](http://localhost:8192/notebook/sql_data_monitor.ipynb).\\n\\n## Summary\\n\\nSmart Data Lake Builder (SDLB) provides a powerful tool to capture data from SQL databases and provides features to track changes, as well as optimized procedures to process Change Data Capture (CDC) data. There are various connectors to interact with SQL databases (e.g. JDBC and Airbyte). Powerful transformers help to handle the data stream within the action, e.g. for deduplication. Furthermore, the join and merge of new data with existing tables, as well as the writing of the data is optimized, reducing the computational an IO efforts with standard HistorizeAction."},{"id":"sdl-databricks","metadata":{"permalink":"/blog/sdl-databricks","editUrl":"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/blog/2022-04-07-SDL_databricks/2022-04-07-Databricks.md","source":"@site/blog/2022-04-07-SDL_databricks/2022-04-07-Databricks.md","title":"Deployment on Databricks","description":"A brief example of deploying SDL on Databricks","date":"2022-04-07T00:00:00.000Z","formattedDate":"April 7, 2022","tags":[{"label":"Databricks","permalink":"/blog/tags/databricks"},{"label":"Cloud","permalink":"/blog/tags/cloud"}],"readingTime":5.895,"hasTruncateMarker":true,"authors":[{"name":"Mandes Sch\xf6nherr","title":"Dr.sc.nat.","url":"https://github.com/mand35"}],"frontMatter":{"title":"Deployment on Databricks","description":"A brief example of deploying SDL on Databricks","slug":"sdl-databricks","authors":[{"name":"Mandes Sch\xf6nherr","title":"Dr.sc.nat.","url":"https://github.com/mand35"}],"tags":["Databricks","Cloud"],"hide_table_of_contents":false},"prevItem":{"title":"Incremental historization using CDC and Airbyte MSSQL connector","permalink":"/blog/sdl-hist"},"nextItem":{"title":"Combine Spark and Snowpark to ingest and transform data in one pipeline","permalink":"/blog/sdl-snowpark"}},"content":"Many analytics applications are ported to the cloud, Data Lakes and Lakehouses in the cloud becoming more and more popular. \\nThe [Databricks](https://databricks.com) platform provides an easy accessible and easy configurable way to implement a modern analytics platform. \\nSmart Data Lake Builder on the other hand provides an open source, portable automation tool to load and transform the data.\\n\\nIn this article the deployment of Smart Data Lake Builder (SDLB) on [Databricks](https://databricks.com) is described. \\n\\n\x3c!--truncate--\x3e\\n\\nBefore jumping in, it should be mentioned, that there are also many other methods to deploy SDLB in the cloud, e.g. using containers on Azure, Azure Kubernetes Service, Azure Synapse Clusters, Google Dataproc...\\nThe present method provides the advantage of having many aspects taken care of by Databricks like Cluster management, Job scheduling and integrated data science notebooks.\\nFurther, the presented SDLB pipeline is just a simple example, focusing on the integration into Databricks environment. \\nSDLB provides a wide range of features and its full power is not revealed here. \\n\\nLet\'s get started:\\n\\n1. [**Databricks**](https://databricks.com) accounts can be created as [Free Trial](https://databricks.com/try-databricks) or as [Community Account](https://community.databricks.com/s/login/SelfRegister)\\n    - Account and Workspace creation are described in detail [here](https://docs.databricks.com/getting-started/account-setup.html), there are few hints and modifications presented below.\\n    - I selected AWS backend, but there are conceptually no differences to the other providers. If you already have an Azure, AWS or Google Cloud account/subscription this can be used, otherwise you can register a trial subscription there. \\n1. **Workspace stack** is created using the Quickstart as described in the documentation. When finished launch the Workspace.\\n1. **Databricks CLI**: for file transfer of configuration files, scripts and data, the [Databricks CLI](https://docs.databricks.com/dev-tools/cli/index.html) is installed locally. **Configure** the CLI, using the Workspace URL and in the Workspace \\"Settings\\" -> \\"User Settings\\" -> \\"Access tokens\\" create a new token.\\n1. **Cluster** creation, in the Workspace open the *Cluster* Creation form.\\n    - Spark version: When selecting the *Databricks version* pay attention to the related Spark version. \\n      This needs to match the Spark version we build SDLB with later. Here, `10.4 LTS` is selected with `Spark 3.2.1` and `Scala 2.12`. \\n      Alternatively, SDLB can be build with a different Spark version, see also [Architecture](../../docs/architecture) for supported versions. \\n    - typesafe library version correction script: the workspace currently includes version 1.2.1 from com.typesafe:config java library. \\n      SDLB relies on functions of a newer version (>1.3.0) of this library. \\n      Thus, we provide a newer version of the com.typesafe:config java library in an initialization script: *Advanced options* -> *Init Scripts* specify `dbfs:/databricks/scripts/config-install.sh`\\n        + Further, the script needs to be created and uploaded. You can use the following script in a local terminal:\\n        ```\\n        cat << EOF >> ./config-install.sh\\n        #!/bin/bash\\n        wget -O /databricks/jars/-----config-1.4.1.jar https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar\\n        EOF\\n        databricks fs mkdirs dbfs:/databricks/scripts\\n        databricks fs cp ./config-install.sh dbfs:/databricks/scripts/\\n        ```\\n\\t\\t\\n\\t\\tAlternatively, you can also use a Databricks notebook for the script upload by executing the following cell:\\n\\t\\t```\\n\\t\\t%sh\\n\\t\\tcat << EOF >> ./config-install.sh\\n\\t\\t#!/bin/bash\\n\\t\\twget -O /databricks/jars/-----config-1.4.1.jar https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar\\n\\t\\tEOF\\n\\t\\tmkdir /dbfs/databricks/scripts\\n\\t\\tcp ./config-install.sh /dbfs/databricks/scripts/\\n\\t\\t```\\n\\n        Note: to double-check the library version I ran `grep typesafe pom.xml` in the [SmartDataLake](https://github.com/smart-data-lake/smart-data-lake.git) source\\n\\n        Note: the added `-----` will ensure that this `.jar` is preferred before the default Workspace Spark version (which starts with `----`). \\n        If you are curious you could double-check e.g. with a Workspace Shell Notebook running `ls /databricks/jars/*config*`\\n\\n2. **fat-jar**:\\n       We need to provide the SDLB sources and all required libraries. Therefore, we compile and pack the Scala code into a Jar including the dependencies. We use the [getting-started](https://github.com/smart-data-lake/getting-started.git) as dummy project, which itself pulls the SDLB sources. \\n    - download the [getting-started](https://github.com/smart-data-lake/getting-started.git) source and build it with the `-P fat-jar` profile\\n    ```\\n    podman run -v ${PWD}:/mnt/project -v ${PWD}/.mvnrepo:/mnt/.mvnrepo maven:3.6.0-jdk-11-slim -- mvn -DskipTests  -P fat-jar  -f /mnt/project/pom.xml \\"-Dmaven.repo.local=/mnt/.mvnrepo\\" package\\n    ```\\n    General build instructions can be found in the [getting-started](../../docs/getting-started/setup#compile-scala-classes) documentation. \\n    Therewith, the file `target/getting-started-1.0-jar-with-dependencies.jar` is created. \\n    The *fat-jar* profile will include all required dependencies. The profile is defined in the [smart-data-lake](https://github.com/smart-data-lake/smart-data-lake) pom.xml.\\n\\n3. upload files\\n\\t- JAR: in the \\"Workspace\\" -> your user -> create a directory `jars` and \\"import\\" the library using the link in \\"(To import a library, such as a jar or egg, click here)\\" and select the above created fat-jar to upload. As a result the jar will be listed in the Workspace directory. \\n\\t- **SDLB application**: As an example a dataset from Airbnb NYC will be downloaded from Github, first written into a CSV file and later partially ported into a table. Therefore, the pipeline is defined first locally in a new file `application.conf`:\\n\\t```\\n\\tdataObjects {\\n\\t  ext-ab-csv-web {\\n\\t    type = WebserviceFileDataObject\\n\\t    url = \\"https://raw.githubusercontent.com/adishourya/Airbnb/master/new-york-city-airbnb-open-data/AB_NYC_2019.csv\\"\\n\\t    followRedirects = true\\n\\t    readTimeoutMs=200000\\n\\t  }\\n\\t  stg-ab {\\n\\t    type = CsvFileDataObject\\n\\t    schema = \\"\\"\\"id integer, name string, host_id integer, host_name string, neighbourhood_group string, neighbourhood string, latitude double, longitude double, room_type  string, price integer, minimum_nights integer, number_of_reviews integer, last_review timestamp, reviews_per_month double, calculated_host_listings_count integer,          availability_365 integer\\"\\"\\"\\n\\t    path = \\"file:///dbfs/data/~{id}\\"\\n\\t  }\\n\\t  int-ab {\\n\\t    type = DeltaLakeTableDataObject\\n\\t    path = \\"~{id}\\"\\n\\t    table {\\n\\t      db = \\"default\\"\\n\\t      name = \\"int_ab\\"\\n\\t      primaryKey = [id]\\n\\t    }\\n\\t  }\\n\\t}\\n\\n\\tactions {\\n\\t  loadWeb2Csv {\\n\\t    type = FileTransferAction\\n\\t    inputId = ext-ab-csv-web\\n\\t    outputId = stg-ab\\n\\t    metadata {\\n\\t      feed = download\\n\\t    }\\n\\t  }\\n\\t  loadCsvLoc2Db {\\n\\t    type = CopyAction\\n\\t    inputId = stg-ab\\n\\t    outputId = int-ab\\n\\t    transformers = [{\\n\\t      type = SQLDfTransformer\\n\\t      code = \\"select id, name, host_id,host_name,neighbourhood_group,neighbourhood,latitude,longitude from stg_ab\\"\\n\\t    }]\\n\\t    metadata {\\n\\t      feed = copy\\n\\t    }\\n\\t  }\\n\\t}\\n\\t```\\n\\t- upload using Databricks CLI \\n\\t```\\n\\tdatabricks fs mkdirs dbfs:/conf/\\n\\tdatabricks fs cp application.conf dbfs:/conf/application.conf\\n\\t```\\n\\n1. **Job creation**:\\n\\tHere, the Databricks job gets defined, specifying the SDL library and, the entry point and the arguments. Here we specify only the download feed. \\n\\tTherefore, open in the sidebar *Jobs* -> *Create Job*: \\n\\t- **Type**: `JAR`\\n\\t- **Main Class**: `io.smartdatalake.app.LocalSmartDataLakeBuilder`\\n\\t- **add** *Dependent Libraries*: \\"Workspace\\" -> select the file previously uploaded \\"getting-started...\\" file in the \\"jars\\" directory\\n\\t![jar select](add_library.png)\\n\\t- **Cluster** select the cluster created above with the corrected typesafe library\\n\\t- **Parameters**: `[\\"-c\\", \\"file:///dbfs/conf/\\", \\"--feed-sel\\", \\"download\\"]`, which specifies the location of the SDLB configuration and selects the feed \\"download\\"\\n\\t![download task](download_task.png)\\n\\n1. **Launch** the job: \\n\\tLaunch the job. \\n\\tWhen finished in the \\"Runs\\" section of that job we can verify the successful run status\\n\\n1. **Results**\\n    After running the SDLB pipeline the data should be downloaded into the staging file `stg_ab/result.csv` and selected parts into the table `int_ab`\\n    - csv file: in the first step we downloaded the CSV file. This can be verified, e.g. by inspecting the data directory in the Databricks CLI using `databricks fs ls dbfs:/data/stg-ab` or running in a Workspace shell notebook `ls /dbfs/data/stg-ab`\\n    - database: in the second phase specific columns are put into the database. This can be verified in the Workspace -> Data -> default -> int_ab\\n    ![select table](select_table.png)\\n    ![table](table.png)\\n\\n    :::info\\n    Note that our final table was defined as `DeltaLakeTableDataObject`.\\n\\t\\tWith that, Smart Data Lake Builder automatically generates a Delta Lake Table in your Databricks workspace. \\n\\n\\n## Lessons Learned\\nThere are a few steps necessary, including building and uploading SDLB. \\nFurther, we need to be careful with the used versions of the underlying libraries. \\nWith these few steps we can reveal the power of SDLB and Databricks, creating a portable and reproducible pipeline into a Databricks Lakehouse."},{"id":"sdl-snowpark","metadata":{"permalink":"/blog/sdl-snowpark","editUrl":"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/blog/2022-04-06-SDL-snowpark/2022-04-06-SDL-snowpark.md","source":"@site/blog/2022-04-06-SDL-snowpark/2022-04-06-SDL-snowpark.md","title":"Combine Spark and Snowpark to ingest and transform data in one pipeline","description":"An example to create one unified data pipeline that uses Spark to ingest data into Snowflake, and Snowpark to transform data inside Snowflake.","date":"2022-04-06T00:00:00.000Z","formattedDate":"April 6, 2022","tags":[{"label":"Snowpark","permalink":"/blog/tags/snowpark"},{"label":"Snowflake","permalink":"/blog/tags/snowflake"}],"readingTime":7.095,"hasTruncateMarker":true,"authors":[{"name":"Zach Kull","title":"Data Expert","url":"https://www.linkedin.com/in/zacharias-kull-94705886/"}],"frontMatter":{"title":"Combine Spark and Snowpark to ingest and transform data in one pipeline","description":"An example to create one unified data pipeline that uses Spark to ingest data into Snowflake, and Snowpark to transform data inside Snowflake.","slug":"sdl-snowpark","authors":[{"name":"Zach Kull","title":"Data Expert","url":"https://www.linkedin.com/in/zacharias-kull-94705886/"}],"tags":["Snowpark","Snowflake"],"hide_table_of_contents":false},"prevItem":{"title":"Deployment on Databricks","permalink":"/blog/sdl-databricks"},"nextItem":{"title":"Using Airbyte connector to inspect github data","permalink":"/blog/sdl-airbyte"}},"content":"This article shows how to create one unified data pipeline that uses Spark to ingest data into Snowflake, and Snowpark to transform data inside Snowflake.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\nRecent developments in Smart Data Lake Builder (SDLB) included refactorings to integrate alternative execution engines to Spark.\\r\\nIn particular [Snowpark](https://docs.snowflake.com/en/developer-guide/snowpark/index.html) integration was implemented, a Spark like DataFrame API for implementing transformations in Snowflake.\\r\\n\\r\\nImplementing transformations in Snowflake has big performance and cost benefits. And using a DataFrame API is much more powerful than coding in SQL, see also [Modern Data Stack: Which Place for Spark?](https://medium.com/towards-data-science/modern-data-stack-which-place-for-spark-8e10365a8772).\\r\\n\\r\\nSnowpark is good for transforming data inside Snowflake, but not all data might be located in Snowflake and suitable for Snowflake. \\r\\nHere it is interesting to use Spark and its many connectors, in particular to ingest and export data.\\r\\n\\r\\nCombining Spark and Snowpark in a smart data pipeline using a DataFrame API would be the ideal solution.\\r\\nWith the integration of Snowpark as engine in SDLB we created just that. \\r\\n\\r\\nThis blog post will show how to migrate our example data pipeline of the [Getting Started](../../docs/getting-started/setup) guide Part 1 to use Spark for ingestion and Snowpark for transformation.\\r\\n\\r\\n## Prerequisites\\r\\n\\r\\n* Create a Snowflake trial account on https://signup.snowflake.com/ and note the following connection informations:\\r\\n  * Account URL (copy by navigating to \\"Organization\\" and clicking the link symbol on the right of the account name)\\r\\n  * Username\\r\\n  * Password\\r\\n* Create database \\"testdb\\" in Snowflake: `create database testdb;`\\r\\n* Create schema \\"testdb.test\\" in Snowflake: `create schema testdb.test;`\\r\\n* Setup running SDLB docker image with part-1 configuration as described in [Getting Started](../../docs/getting-started/setup)\\r\\n  * build sdl-spark image\\r\\n  * copy final application.conf of part-1: `cp config/application.conf.part-1-solution config/application.conf`\\r\\n  * run download actions with parameter `--feed-sel download`\\r\\n  * run compute actions with parameter `--feed-sel compute`\\r\\n\\r\\n## Goal\\r\\n\\r\\nThe example of part-1 has the following DataObjects\\r\\n\\r\\nStaging Layer\\r\\n* stg-departures: JsonFileDataObject\\r\\n* stg-airports: CsvFileDataObject\\r\\n\\r\\nIntegration Layer\\r\\n* int-airports: CsvFileDataObject\\r\\n\\r\\nBusiness Transformation Layer\\r\\n* btl-departures-arrivals-airports: CsvFileDataObject\\r\\n* btl-distances: CsvFileDataObject\\r\\n\\r\\nIn this example we will migrate Integration and Business Transformation Layer to Snowflake.\\r\\nWe will use Spark to fill Staging and Integration Layer, and Snowpark for transformation from Integration to Business Transformation Layer.\\r\\n\\r\\n## Prepare the Snowflake library\\r\\n\\r\\nFirst we have add SDLBs Snowflake library to the projects pom.xml dependencies section:\\r\\n\\r\\n      <dependencies>\\r\\n        ....\\r\\n        <dependency>\\r\\n          <groupId>io.smartdatalake</groupId>\\r\\n          <artifactId>sdl-snowflake_${scala.minor.version}</artifactId>\\r\\n          <version>${project.parent.version}</version>\\r\\n        </dependency>\\r\\n        ...\\r\\n      </dependencies>\\r\\n\\r\\nThen SDLB version needs to be updated to version 2.3.0-SNAPSHOT at least in the parent section:\\r\\n\\r\\n      <parent>\\r\\n        <groupId>io.smartdatalake</groupId>\\r\\n        <artifactId>sdl-parent</artifactId>\\r\\n        <version>2.3.0-SNAPSHOT</version>\\r\\n      </parent>\\r\\n\\r\\n## Define Snowflake connection\\r\\n\\r\\nTo define the Snowflake connection in config/application.conf, add connections section with connection \\"sf-con\\", and fill in informations according to prerequisits:\\r\\n\\r\\n      connections {\\r\\n        sf-con {\\r\\n          type = SnowflakeTableConnection\\r\\n          url = \\"<accountUrl>\\",\\r\\n          warehouse = \\"COMPUTE_WH\\",\\r\\n          database = \\"testdb\\",\\r\\n          role = \\"ACCOUNTADMIN\\",\\r\\n          authMode = {\\r\\n            type = BasicAuthMode\\r\\n            userVariable = \\"CLEAR#<username>\\"\\r\\n            passwordVariable = \\"CLEAR#<pwd>\\"\\r\\n        }\\r\\n      }\\r\\n\\r\\n## Migrate DataObjects\\r\\n\\r\\nNow we can change the DataObject type to SnowflakeTableDataObject and the new Snowflake connection, adding the definition of the table:\\r\\n\\r\\n      int-airports {\\r\\n        type = SnowflakeTableDataObject\\r\\n        connectionId = sf-con\\r\\n        table {\\r\\n          db = \\"test\\"\\r\\n          name = \\"int_airports\\"\\r\\n        }\\r\\n      }\\r\\n    \\r\\n      btl-departures-arrivals-airports {\\r\\n        type = SnowflakeTableDataObject\\r\\n        connectionId = sf-con\\r\\n        table {\\r\\n          db = \\"test\\"\\r\\n          name = \\"btl_departures_arrivals_airports\\"\\r\\n        }\\r\\n      }\\r\\n    \\r\\n      btl-distances {\\r\\n        type = SnowflakeTableDataObject\\r\\n        connectionId = sf-con\\r\\n        table {\\r\\n          db = \\"test\\"\\r\\n          name = \\"btl_distances\\"\\r\\n        }\\r\\n      }\\r\\n\\r\\nNote that the attribute `db` of the SnowflakeTableDataObject should be filled with the schema of the Snowflake table and that this is *not* the same as the attribute `database` of SnowflakeTableConnection. \\r\\n\\r\\n## Migrating Actions\\r\\n\\r\\nThe new SDLB version introduced some naming changes:\\r\\n- The CustomSparkAction can now also process Snowpark-DataFrames and is therefore renamed to CustomDataFrameAction.\\r\\n- The ScalaClassDfTransformer was specific for Spark. In the new SDLB version there is a specific scala-class DataFrame transformer for Spark and Snowpark, e.g. ScalaClassSparkDfTransformer and ScalaClassSnowparkDfTransformer. And there is even a ScalaClassGenericDfTransformer to implement transformations using a unified API. In our case we will migrate the transformation to use Snowpark and set the type to ScalaClassSnowparkDfTransformer.\\r\\n\\r\\n      join-departures-airports {\\r\\n        type = CustomSparkAction -> CustomDataFrameAction\\r\\n        ...\\r\\n\\r\\n      compute-distances {\\r\\n        ...\\r\\n        transformers = [{\\r\\n          type = ScalaClassDfTransformer -> ScalaClassSnowparkDfTransformer\\r\\n\\r\\nThere is no need to change the SQL transformtions of join-departures-airport, as the SQL should run on Snowpark aswell.\\r\\n\\r\\nOn the other hand the ComputeDistanceTransformer was implemented with the Spark DataFrame API. We need to migrate it to Snowpark DataFrame API to run this Action with Snowpark. Luckily the API\'s are very similar. Often it\'s sufficient to change the import statement, the class we\'re extending and the session parameters type:\\r\\n\\r\\n      import com.snowflake.snowpark.functions._\\r\\n      import com.snowflake.snowpark.{DataFrame, Session}\\r\\n      import io.smartdatalake.workflow.action.snowflake.customlogic.CustomSnowparkDfTransformer\\r\\n\\r\\n      class ComputeDistanceTransformer extends CustomSnowparkDfTransformer {\\r\\n        def transform(session: Session, options: Map[String, String], df: DataFrame, dataObjectId: String) : DataFrame = {\\r\\n          ...\\r\\n\\r\\nIf you have UDFs in your code, it gets trickier. The UDF Code gets serialized to Snowflake, details see [Snowpark UDFs](https://docs.snowflake.com/en/developer-guide/snowpark/scala/creating-udfs.html). Special care must be taken to minimize the scope the UDF is defined in. Thats why we move the function into the companion object.\\r\\n\\r\\n      object ComputeDistanceTransformer {\\r\\n        def calculateDistanceInKilometer(depLat: Double, depLng: Double, arrLat: Double, arrLng: Double): Double = {\\r\\n          val AVERAGE_RADIUS_OF_EARTH_KM = 6371\\r\\n          val latDistance = Math.toRadians(depLat - arrLat)\\r\\n          val lngDistance = Math.toRadians(depLng - arrLng)\\r\\n          val a = Math.sin(latDistance / 2) * Math.sin(latDistance / 2) + Math.cos(Math.toRadians(depLat)) * Math.cos(Math.toRadians(arrLat)) * Math.sin(lngDistance / 2) * Math.sin(lngDistance / 2)\\r\\n          val c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a))\\r\\n          AVERAGE_RADIUS_OF_EARTH_KM * c\\r\\n        }\\r\\n        def getCalculateDistanceInKilometerUdf(session: Session) = {\\r\\n          // using only udf(...) function results in \\"SnowparkClientException: Error Code: 0207, Error message: No default Session found. Use <session>.udf.registerTemporary() to explicitly refer to a session.\\"\\r\\n          session.udf.registerTemporary(ComputeDistanceTransformer.calculateDistanceInKilometer _)\\r\\n        }\\r\\n      }\\r\\n\\r\\nNote that we need to pass the Session to a function for registering the UDF. There is an Error 0207 if we use \\"udf\\" function (at least in snowpark version 1.2.0).\\r\\nFinally we need to adapt the call of the UDF as follows:\\r\\n\\r\\n    df.withColumn(\\"distance\\", ComputeDistanceTransformer.getCalculateDistanceInKilometerUdf(session)(col(\\"dep_latitude_deg\\"),col(\\"dep_longitude_deg\\"),col(\\"arr_latitude_deg\\"), col(\\"arr_longitude_deg\\")))\\r\\n\\r\\n## Compile and run\\r\\n\\r\\nTime to see if it works.\\r\\nLets build an update SDLB docker image with the updated SDLB version:\\r\\n\\r\\n      podman build -t sdl-spark .\\r\\n\\r\\nThen compile the code with the UDF:\\r\\n\\r\\n      mkdir .mvnrepo\\r\\n      podman run -v ${PWD}:/mnt/project -v ${PWD}/.mvnrepo:/mnt/.mvnrepo maven:3.6.0-jdk-11-slim -- mvn -f /mnt/project/pom.xml \\"-Dmaven.repo.local=/mnt/.mvnrepo\\" package\\r\\n\\r\\nDownload initial data with `--feed-sel download`:\\r\\n\\r\\n      podman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel download\\r\\n\\r\\nCompute with `--feed-sel compute`:\\r\\n\\r\\n      podman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel compute\\r\\n\\r\\nIf the SDLB run was SUCCESSFUL, you should now see TEST.BTL_DISTANCES table in Snowpark.\\r\\nTo check that Spark was used for Action select-airport-cols and Snowpark for Action compute-distances, look for the following logs, e.g. SnowparkSubFeed for Action~compute-distances: \\r\\n\\r\\n      INFO  CopyAction - (Action~compute-distances) selected subFeedType SnowparkSubFeed [init-compute-distances]\\r\\n\\r\\n# Engine selection - uncover the magic\\r\\n\\r\\nBrowsing through the logs it turns out that the Action~join-departures-airports was still executed with Spark (SparkSubFeed)!\\r\\n\\r\\n      INFO  CustomDataFrameAction - (Action~join-departures-airports) selected subFeedType SparkSubFeed [init-join-departures-airports]\\r\\n\\r\\nAn Action determines the engine to use in Init-phase by checking the supported types of inputs, outputs and transformations. In our case we have input DataObject stg-departures which is still a JsonFileDataObject, that can not create a Snowpark-DataFrame. As we would like to execute this join as well in Snowflake with Snowpark for performance reasons, lets create a SnowflakeTableDataObject int-departures and use it as input for Action~join-departures-airports.\\r\\n\\r\\nAdd a DataObject int-departures:\\r\\n\\r\\n      int-departures {\\r\\n        type = SnowflakeTableDataObject\\r\\n        connectionId = sf-con\\r\\n        table {\\r\\n          db = \\"test\\"\\r\\n          name = \\"int_departures\\"\\r\\n        }\\r\\n      }\\r\\n\\r\\nAdd an Action copy-departures:\\r\\n\\r\\n      copy-departures {\\r\\n        type = CopyAction\\r\\n        inputId = stg-departures\\r\\n        outputId = int-departures\\r\\n        metadata {\\r\\n          feed = compute\\r\\n        }\\r\\n      }\\r\\n\\r\\nFix inputs of Action join-departures-airports:\\r\\n\\r\\n      inputIds = [int-departures, int-airports]\\r\\n\\r\\n... and code of the first SQL transformer:\\r\\n\\r\\n      code = {\\r\\n        btl-connected-airports = \\"\\"\\"\\r\\n          select int_departures.estdepartureairport, int_departures.estarrivalairport, airports.*\\r\\n          from int_departures join int_airports airports on int_departures.estArrivalAirport = airports.ident\\r\\n        \\"\\"\\"\\r\\n\\r\\nCompute with Spark and Snowpark again by using `--feed-sel compute` and browsing the logs, we can see that Action~join-departures-airports was executed with Snowpark:\\r\\n\\r\\n      (Action~join-departures-airports) selected subFeedType SnowparkSubFeed [init-join-departures-airports]\\r\\n\\r\\n# Summary\\r\\n\\r\\nWe have seen that its quite easy to migrate SDLB pipelines to use Snowpark instead of Spark, also only partially for selected Actions. SDLB\'s support of different DataFrame-API-Engines allows to still benefit of all other features of SDLB, like having full early validation over the whole pipeline by checking the schemas needed by Actions later in the pipeline.\\r\\n\\r\\nMigrating Scala code of custom transformations using Spark DataFrame API needs some adaptions of import statements, but the rest stays mostly 1:1 the same. UDFs are also supported and dont need changes, but there might be surprises regarding data types (Snowparks Variant-type is not the same as Sparks nested datatypes) and deployment of needed libraries. We might investigate that in future blog post."},{"id":"sdl-airbyte","metadata":{"permalink":"/blog/sdl-airbyte","editUrl":"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/blog/2022-03-18-SDL-airbyte/2022-03-18-SDL-airbyte.md","source":"@site/blog/2022-03-18-SDL-airbyte/2022-03-18-SDL-airbyte.md","title":"Using Airbyte connector to inspect github data","description":"A short example using Airbyte github connector","date":"2022-03-18T00:00:00.000Z","formattedDate":"March 18, 2022","tags":[{"label":"Airbyte","permalink":"/blog/tags/airbyte"},{"label":"Connector","permalink":"/blog/tags/connector"}],"readingTime":4.48,"hasTruncateMarker":true,"authors":[{"name":"Mandes Sch\xf6nherr","title":"Dr.sc.nat.","url":"https://github.com/mand35"}],"frontMatter":{"title":"Using Airbyte connector to inspect github data","description":"A short example using Airbyte github connector","slug":"sdl-airbyte","authors":[{"name":"Mandes Sch\xf6nherr","title":"Dr.sc.nat.","url":"https://github.com/mand35"}],"tags":["Airbyte","Connector"],"image":"airbyte.png","hide_table_of_contents":false},"prevItem":{"title":"Combine Spark and Snowpark to ingest and transform data in one pipeline","permalink":"/blog/sdl-snowpark"}},"content":"This article presents the deployment of an [Airbyte Connector](https://airbyte.com) with Smart Data Lake Builder (SDLB). \\nIn particular the [github connector](https://docs.airbyte.com/integrations/sources/github) is implemented using the python sources.\\n\\n\x3c!--truncate--\x3e\\n\\nAirbyte is a framework to sync data from a variety of sources (APIs and databases) into data warehouses and data lakes. \\nIn this example an Airbyte connector is utilized to stream data into Smart Data Lake (SDL). \\nTherefore, the [Airbyte DataObject](http://smartdatalake.ch/json-schema-viewer/index.html#viewer-page?v=2-2) is used and will be configured. \\nThe general [Airbyte connector handling](https://docs.airbyte.com/understanding-airbyte/airbyte-specification#source) is implemented in SDL, which includes the 4 main steps:\\n* `spec`: receiving the specification of the connector\\n* `check`: validating the specified configuration\\n* `discover`: gather a catalog of available streams and its schemas\\n* `read`: collect the actual data\\n\\nThe actual connector is not provided in the SDL repository and needs to be obtained from the [Airbyte repository](https://github.com/airbytehq/airbyte). Besides the [list of existing connectors](https://docs.airbyte.com/integrations), custom connectors could be implemented in Python or Javascript. \\n\\nThe following description builds on top of the example setup from the [getting-started](../../docs/getting-started/setup) guide, using [Podman](https://docs.podman.io) as container engine within a [WSL](https://docs.microsoft.com/en-us/windows/wsl/install) Ubuntu image. \\n\\nThe [github connector](https://docs.airbyte.com/integrations/sources/github) is utilized to gather data about a specific repository.\\n\\n## Prerequisites\\nAfter downloading and installing all necessary packages, the connector is briefly tested:\\n* Python\\n* [Podman](../../docs/getting-started/troubleshooting/docker-on-windows) or [Docker](https://www.docker.com/get-started)\\n* [SDL example](https://github.com/smart-data-lake/getting-started/archive/refs/heads/master.zip), download and unpack: \\n  ```Bash\\n  git clone https://github.com/smart-data-lake/getting-started.git SDL_airbyte\\n  cd SDL_airbyte\\n  ```\\n* download the [Airbyte repository](https://github.com/airbytehq/airbyte) \\n  ```Bash\\n  git clone https://github.com/airbytehq/airbyte.git\\n  ```\\n  Alternatively, only the target connector can be downloaded:\\n  ```Bash\\n  svn checkout https://github.com/airbytehq/airbyte/trunk/airbyte-integrations/connectors/source-github\\n  ```\\n  Here the Airbyte `airbyte/airbyte-integrations/connectors/source-github/` directory is copied into the `SDL_airbyte` directory for handy calling the connector.\\n\\n## [Optional] Inspect the connector specification\\nThe first connector command `spec` provides the connector specification. This is the basis to create a connector configuration. To run the connector as is, the Python `airbyte-cdk` package needs to be installed and the connector can be launched:\\n\\n* Install Python airbyte-cdk: `pip install airbyte_cdk`\\n* try the connector: \\n  ```Bash\\n  cd SDL_airbyte\\n  python source_github/main.py spec | python -m json.tool\\n  ```\\n  This provides a [JSON string](github_spec_out.json) with the connector specification. The fields listed under `properties` are relevant for the configuration (compare with the configuration  used later). \\n\\n## Configuration\\nTo launch Smart Data Lake Builder (SDLB) with the Airbyte connector the following needs to be modified:\\n\\n* add the Airbyte ***DataObject*** with its configuration to the `config/application.conf`:\\n  ```Python\\n  dataObjects {\\n    ext-commits {\\n      type = AirbyteDataObject\\n      config = {\\n        \\"credentials\\": {\\n          \\"personal_access_token\\": \\"<yourPersonalAccessToken>\\" ### enter your personal access token here\\n        },\\n        \\"repository\\": \\"smart-data-lake/smart-data-lake\\",\\n        \\"start_date\\": \\"2021-02-01T00:00:00Z\\",\\n        \\"branch\\": \\"documentation develop-spark3 develop-spark2\\",\\n        \\"page_size_for_large_streams\\": 100\\n      },\\n      streamName = \\"commits\\",\\n      cmd = {\\n        type = CmdScript\\n        name = \\"airbyte_connector_github\\"\\n        linuxCmd = \\"python3 /mnt/source-github/main.py\\"\\n      }\\n    }\\n  ...\\n    stg-commits {\\n     type = DeltaLakeTableDataObject\\n     path = \\"~{id}\\"\\n     table {\\n      db = \\"default\\"\\n      name = \\"stg_commits\\"\\n      primaryKey = [created_at]\\n      }\\n    }\\n  ```\\n  Note the options set for `ext-commits` which define the Airbyte connector settings. \\n  While the `config` varies from connector to connector, the remaining fields are SDL specific. \\n  The `streamName` selects the stream, exactly one. \\n  If multiple streams should be collected, multiple dataObjects need to be defined. \\n  In `linuxCmd` the actual connector script is called. \\n  In our case we will mount the connector directory into the SDL container. \\n\\n* also add the definition of the data stream ***action*** to pipe the coming data stream into a `DeltaLakeTableDataObject`:\\n  ```Bash\\n    actions {\\n      download-commits {\\n        type = CopyAction\\n        inputId = ext-commits\\n        outputId = stg-commits\\n        metadata {\\n          feed = download\\n        }\\n      }\\n  ...\\n  ```\\n* Since Airbyte will be called as Python script in the sdl container, we need to (re-)build the container with Python support and the Python `airbyte-cdk` package. \\n  Therefore, in the Dockerfile we add:\\n\\t```\\n\\tRUN \\\\\\n  apt update && \\\\\\n  apt --assume-yes install python3 python3-pip && \\\\\\n  pip3 install airbyte-cdk~=0.1.25\\n  ```\\n  and rebuild \\n  ```Bash\\n  podman build . -t sdl-spark\\n  ```\\n\\nNow we are ready to go. My full [SDLB config file](application.conf) additionally includes the pull-request stream.\\n\\n## Run and inspect results\\nSince the data will be streamed into a `DeltaLakeTableDataObject`, the metastore container is necessary. Further, we aim to inspect the data using the Polynote notebook. Thus, first these containers are launched using (in the SDL example base directory):\\n```Bash\\n./part2/podman-compose.sh #use the script from the getting-started guide\\npodman pod ls\\n```\\nWith the second command we can verify the pod name and both running containers in it (should be three including the infra container).\\n\\nThen, the SDLB can be launched using the additional option to mount the Airbyte connector directory:\\n```Bash\\npodman run --hostname localhost --rm --pod sdl_airbyte -v ${PWD}/source-github/:/mnt/source-github -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel download\\n```\\n\\nThe output presents the successful run of the workflow:\\n```Bash\\n2022-03-16 07:54:03 INFO  ActionDAGRun$ActionEventListener - Action~download-commits[CopyAction]: Exec succeeded [dag-1-80]\\n2022-03-16 07:54:03 INFO  ActionDAGRun$ - exec SUCCEEDED for dag 1:\\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2510\\n                 \u2502start\u2502\\n                 \u2514\u2500\u2500\u2500\u252c\u2500\u2518\\n                     \u2502\\n                     v\\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n \u2502download-commits SUCCEEDED PT11.686865S\u2502\\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n     [main]\\n2022-03-16 07:54:03 INFO  LocalSmartDataLakeBuilder$ - LocalSmartDataLakeBuilder finished successfully: SUCCEEDED=1 [main]\\n2022-03-16 07:54:03 INFO  SparkUI - Stopped Spark web UI at http://localhost:4040 [shutdown-hook-0]\\n```\\n\\nLaunching Polynote `localhost:8192` in the browser, we can inspect data and develop further workflows. Here an example, where the commits are listed, which were committed in the name of someone else, excluding the web-flow. See [Polynote Notebook](SelectingData.ipynb)\\n![polynote example](polynote_commits.png)\\n\\n## Summary\\n\\nThe Airbyte connectors provide easy access to a variety of data sources. The connectors can be utilized in SDLB with just a few settings. This also works great for more complex interfaces."}]}')}}]);