"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[2114],{9992:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>d});var i=n(5893),s=n(1151);const t={id:"troubleshooting",title:"Troubleshooting"},r=void 0,a={id:"reference/troubleshooting",title:"Troubleshooting",description:"If you have problems with the getting started guide, note that there's a separate troubleshooting section for that.",source:"@site/docs/reference/troubleshooting.md",sourceDirName:"reference",slug:"/reference/troubleshooting",permalink:"/docs/reference/troubleshooting",draft:!1,unlisted:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/reference/troubleshooting.md",tags:[],version:"current",frontMatter:{id:"troubleshooting",title:"Troubleshooting"},sidebar:"tutorialSidebar",previous:{title:"Testing",permalink:"/docs/reference/testing"}},l={},d=[{value:"Windows: missing winutils",id:"windows-missing-winutils",level:2},{value:"Windows: <code>/tmp/hive</code> is not writable",id:"windows-tmphive-is-not-writable",level:2},{value:"Windows: winutils.exe is not working correctly",id:"windows-winutilsexe-is-not-working-correctly",level:2},{value:"Java IllegalAccessError (Java 17)",id:"java-illegalaccesserror-java-17",level:2},{value:"Resources not copied",id:"resources-not-copied",level:2},{value:"Maven compile error: tools.jar",id:"maven-compile-error-toolsjar",level:2},{value:"How can I test Hadoop / HDFS locally ?",id:"how-can-i-test-hadoop--hdfs-locally-",level:2}];function c(e){const o={a:"a",admonition:"admonition",br:"br",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.admonition,{type:"info",children:(0,i.jsxs)(o.p,{children:["If you have problems with the getting started guide, note that there's a separate ",(0,i.jsx)(o.a,{href:"/docs/getting-started/troubleshooting/common-problems",children:"troubleshooting section"})," for that."]})}),"\n",(0,i.jsx)(o.h2,{id:"windows-missing-winutils",children:"Windows: missing winutils"}),"\n",(0,i.jsxs)(o.p,{children:["Error:",(0,i.jsx)(o.br,{}),"\n",(0,i.jsx)(o.code,{children:"java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries"})]}),"\n",(0,i.jsxs)(o.p,{children:["Cause:",(0,i.jsx)(o.br,{}),"\n","The ",(0,i.jsx)(o.code,{children:"winutils.exe"})," executable can not be found."]}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsxs)(o.li,{children:["Download hadoop winutils binaries (e.g ",(0,i.jsx)(o.a,{href:"https://github.com/cdarlint/winutils/archive/refs/heads/master.zip",children:"https://github.com/cdarlint/winutils/archive/refs/heads/master.zip"}),")"]}),"\n",(0,i.jsx)(o.li,{children:"Extract binaries for desired hadoop version into folder (e.g. hadoop-3.2.2\\bin)"}),"\n",(0,i.jsx)(o.li,{children:"Set HADOOP_HOME evironment variable (e.g. HADOOP_HOME=...\\hadoop-3.2.2).\nNote that the binary files need to be located at %HADOOP_HOME%\\bin!"}),"\n",(0,i.jsx)(o.li,{children:"Add %HADOOP_HOME%\\bin to PATH variable."}),"\n"]}),"\n",(0,i.jsxs)(o.h2,{id:"windows-tmphive-is-not-writable",children:["Windows: ",(0,i.jsx)(o.code,{children:"/tmp/hive"})," is not writable"]}),"\n",(0,i.jsxs)(o.p,{children:["Error:",(0,i.jsx)(o.br,{}),"\n",(0,i.jsx)(o.code,{children:"RuntimeException: Error while running command to get file permissions"}),(0,i.jsx)(o.br,{}),"\n","Solution:",(0,i.jsx)(o.br,{}),"\n","Change to ",(0,i.jsx)(o.code,{children:"%HADOOP_HOME%\\bin"})," and execute ",(0,i.jsx)(o.code,{children:"winutils chmod 777 /tmp/hive"}),"."]}),"\n",(0,i.jsx)(o.h2,{id:"windows-winutilsexe-is-not-working-correctly",children:"Windows: winutils.exe is not working correctly"}),"\n",(0,i.jsxs)(o.p,{children:["Error:",(0,i.jsx)(o.br,{}),"\n",(0,i.jsx)(o.code,{children:"winutils.exe - System Error The code execution cannot proceed because MSVCR100.dll was not found. Reinstalling the program may fix this problem."})]}),"\n",(0,i.jsx)(o.p,{children:"Other errors are also possible:"}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"Similar error message when double clicking on winutils.exe (Popup)"}),"\n",(0,i.jsx)(o.li,{children:"Errors when providing a path to the configuration instead of a single configuration file"}),"\n",(0,i.jsx)(o.li,{children:"ExitCodeException exitCode=-1073741515 when executing SDL even though everything ran without errors"}),"\n"]}),"\n",(0,i.jsxs)(o.p,{children:["Solution:",(0,i.jsx)(o.br,{}),"\n","Install VC++ Redistributable Package from Microsoft:",(0,i.jsx)(o.br,{}),"\n",(0,i.jsx)(o.a,{href:"http://www.microsoft.com/en-us/download/details.aspx?id=5555",children:"http://www.microsoft.com/en-us/download/details.aspx?id=5555"})," (x86)",(0,i.jsx)(o.br,{}),"\n",(0,i.jsx)(o.a,{href:"http://www.microsoft.com/en-us/download/details.aspx?id=14632",children:"http://www.microsoft.com/en-us/download/details.aspx?id=14632"})," (x64)"]}),"\n",(0,i.jsx)(o.h2,{id:"java-illegalaccesserror-java-17",children:"Java IllegalAccessError (Java 17)"}),"\n",(0,i.jsx)(o.p,{children:"Symptom:\nStarting an SDLB pipeline fails with the following exception:"}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{children:"java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x343570b7) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x343570b7\n        at org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213)\n        ...\n"})}),"\n",(0,i.jsxs)(o.p,{children:["Solution:\nJava 17 is more restrictive regarding usage of module exports. Unfortunately Spark uses classes from unexported packages. Packages can be exported manually. To fix above exception add ",(0,i.jsx)(o.code,{children:"--add-exports java.base/sun.nio.ch=ALL-UNNAMED"})," to the java command line, see also ",(0,i.jsx)(o.a,{href:"https://stackoverflow.com/questions/72230174/java-17-solution-for-spark-java-lang-noclassdeffounderror-could-not-initializ",children:"Stackoverflow"}),"."]}),"\n",(0,i.jsx)(o.h2,{id:"resources-not-copied",children:"Resources not copied"}),"\n",(0,i.jsxs)(o.p,{children:["Symptom:",(0,i.jsx)(o.br,{}),"\n","Tests fail due to missing or outdated resources or the execution starts but can not find the feeds specified.\nIntelliJ might not copy the resource files to the target directory."]}),"\n",(0,i.jsxs)(o.p,{children:["Solution:",(0,i.jsx)(o.br,{}),"\n","Execute the maven goal ",(0,i.jsx)(o.code,{children:"resources:resources"})," (",(0,i.jsx)(o.code,{children:"mvn resources:resources"}),") manually after you changed any resource file."]}),"\n",(0,i.jsx)(o.h2,{id:"maven-compile-error-toolsjar",children:"Maven compile error: tools.jar"}),"\n",(0,i.jsxs)(o.p,{children:["Error:",(0,i.jsx)(o.br,{}),"\n",(0,i.jsx)(o.code,{children:"Could not find artifact jdk.tools:jdk.tools:jar:1.7 at specified path ..."})]}),"\n",(0,i.jsxs)(o.p,{children:["Context:",(0,i.jsx)(o.br,{}),"\n","Hadoop/Spark has a dependency on the tools.jar file which is installed as part of the JDK installation."]}),"\n",(0,i.jsx)(o.p,{children:"Possible Reasons:"}),"\n",(0,i.jsxs)(o.ol,{children:["\n",(0,i.jsxs)(o.li,{children:["Your system does not have a JDK installed (only a JRE).","\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"Fix: Make sure a JDK is installed and your PATH and JAVA_HOME environment variables are pointing to the JDK installation."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(o.li,{children:["You are using a Java 9 JDK or higher. The tools.jar has been removed in JDK 9. See: ",(0,i.jsx)(o.a,{href:"https://openjdk.java.net/jeps/220",children:"https://openjdk.java.net/jeps/220"}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"Fix: Downgrade your JDK to Java 8."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(o.h2,{id:"how-can-i-test-hadoop--hdfs-locally-",children:"How can I test Hadoop / HDFS locally ?"}),"\n",(0,i.jsxs)(o.p,{children:["When using ",(0,i.jsx)(o.code,{children:"local://"})," URIs, file permissions on Windows, or certain actions, local Hadoop binaries are required."]}),"\n",(0,i.jsxs)(o.ol,{children:["\n",(0,i.jsxs)(o.li,{children:["Download your desired Apache Hadoop binary release from ",(0,i.jsx)(o.a,{href:"https://hadoop.apache.org/releases.html",children:"https://hadoop.apache.org/releases.html"}),"."]}),"\n",(0,i.jsxs)(o.li,{children:["Extract the contents of the Hadoop distribution archive to a location of your choice, e.g., ",(0,i.jsx)(o.code,{children:"/path/to/hadoop"})," (Unix) or ",(0,i.jsx)(o.code,{children:"C:\\path\\to\\hadoop"})," (Windows)."]}),"\n",(0,i.jsxs)(o.li,{children:["Set the environment variable ",(0,i.jsx)(o.code,{children:"HADOOP_HOME=/path/to/hadoop"})," (Unix) or ",(0,i.jsx)(o.code,{children:"HADOOP_HOME=C:\\path\\to\\hadoop"})," (Windows)."]}),"\n",(0,i.jsxs)(o.li,{children:[(0,i.jsx)(o.strong,{children:"Windows only"}),": Download a Hadoop winutils distribution corresponding to your Hadoop version from ",(0,i.jsx)(o.a,{href:"https://github.com/steveloughran/winutils",children:"https://github.com/steveloughran/winutils"})," (for newer Hadoop releases at: ",(0,i.jsx)(o.a,{href:"https://github.com/cdarlint/winutils",children:"https://github.com/cdarlint/winutils"}),") and extract the contents to ",(0,i.jsx)(o.code,{children:"%HADOOP_HOME%\\bin"}),"."]}),"\n"]})]})}function h(e={}){const{wrapper:o}={...(0,s.a)(),...e.components};return o?(0,i.jsx)(o,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},1151:(e,o,n)=>{n.d(o,{Z:()=>a,a:()=>r});var i=n(7294);const s={},t=i.createContext(s);function r(e){const o=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(o):{...o,...e}}),[o,e])}function a(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(t.Provider,{value:o},e.children)}}}]);