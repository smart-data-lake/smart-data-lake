"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[2114],{3905:(e,t,o)=>{o.d(t,{Zo:()=>c,kt:()=>h});var n=o(7294);function a(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function r(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,n)}return o}function i(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?r(Object(o),!0).forEach((function(t){a(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):r(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function l(e,t){if(null==e)return{};var o,n,a=function(e,t){if(null==e)return{};var o,n,a={},r=Object.keys(e);for(n=0;n<r.length;n++)o=r[n],t.indexOf(o)>=0||(a[o]=e[o]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)o=r[n],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(a[o]=e[o])}return a}var s=n.createContext({}),p=function(e){var t=n.useContext(s),o=t;return e&&(o="function"==typeof e?e(t):i(i({},t),e)),o},c=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var o=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=p(o),m=a,h=d["".concat(s,".").concat(m)]||d[m]||u[m]||r;return o?n.createElement(h,i(i({ref:t},c),{},{components:o})):n.createElement(h,i({ref:t},c))}));function h(e,t){var o=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=o.length,i=new Array(r);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[d]="string"==typeof e?e:a,i[1]=l;for(var p=2;p<r;p++)i[p]=o[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,o)}m.displayName="MDXCreateElement"},2722:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var n=o(7462),a=(o(7294),o(3905));const r={id:"troubleshooting",title:"Troubleshooting"},i=void 0,l={unversionedId:"reference/troubleshooting",id:"reference/troubleshooting",title:"Troubleshooting",description:"If you have problems with the getting started guide, note that there's a separate troubleshooting section for that.",source:"@site/docs/reference/troubleshooting.md",sourceDirName:"reference",slug:"/reference/troubleshooting",permalink:"/docs/reference/troubleshooting",draft:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/reference/troubleshooting.md",tags:[],version:"current",frontMatter:{id:"troubleshooting",title:"Troubleshooting"},sidebar:"docs",previous:{title:"Testing",permalink:"/docs/reference/testing"}},s={},p=[{value:"Windows: missing winutils",id:"windows-missing-winutils",level:2},{value:"Windows: <code>/tmp/hive</code> is not writable",id:"windows-tmphive-is-not-writable",level:2},{value:"Windows: winutils.exe is not working correctly",id:"windows-winutilsexe-is-not-working-correctly",level:2},{value:"Java IllegalAccessError (Java 17)",id:"java-illegalaccesserror-java-17",level:2},{value:"Resources not copied",id:"resources-not-copied",level:2},{value:"Maven compile error: tools.jar",id:"maven-compile-error-toolsjar",level:2},{value:"How can I test Hadoop / HDFS locally ?",id:"how-can-i-test-hadoop--hdfs-locally-",level:2}],c={toc:p},d="wrapper";function u(e){let{components:t,...o}=e;return(0,a.kt)(d,(0,n.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("admonition",{type:"info"},(0,a.kt)("p",{parentName:"admonition"},"If you have problems with the getting started guide, note that there's a separate ",(0,a.kt)("a",{parentName:"p",href:"/docs/getting-started/troubleshooting/common-problems"},"troubleshooting section")," for that.")),(0,a.kt)("h2",{id:"windows-missing-winutils"},"Windows: missing winutils"),(0,a.kt)("p",null,"Error:",(0,a.kt)("br",{parentName:"p"}),"\n",(0,a.kt)("inlineCode",{parentName:"p"},"java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries")),(0,a.kt)("p",null,"Cause:",(0,a.kt)("br",{parentName:"p"}),"\n","The ",(0,a.kt)("inlineCode",{parentName:"p"},"winutils.exe")," executable can not be found."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Download hadoop winutils binaries (e.g ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/cdarlint/winutils/archive/refs/heads/master.zip"},"https://github.com/cdarlint/winutils/archive/refs/heads/master.zip"),")"),(0,a.kt)("li",{parentName:"ul"},"Extract binaries for desired hadoop version into folder (e.g. hadoop-3.2.2\\bin)"),(0,a.kt)("li",{parentName:"ul"},"Set HADOOP_HOME evironment variable (e.g. HADOOP_HOME=...\\hadoop-3.2.2).\nNote that the binary files need to be located at %HADOOP_HOME%\\bin!"),(0,a.kt)("li",{parentName:"ul"},"Add %HADOOP_HOME%\\bin to PATH variable.")),(0,a.kt)("h2",{id:"windows-tmphive-is-not-writable"},"Windows: ",(0,a.kt)("inlineCode",{parentName:"h2"},"/tmp/hive")," is not writable"),(0,a.kt)("p",null,"Error:",(0,a.kt)("br",{parentName:"p"}),"\n",(0,a.kt)("inlineCode",{parentName:"p"},"RuntimeException: Error while running command to get file permissions"),(0,a.kt)("br",{parentName:"p"}),"\n","Solution:",(0,a.kt)("br",{parentName:"p"}),"\n","Change to ",(0,a.kt)("inlineCode",{parentName:"p"},"%HADOOP_HOME%\\bin")," and execute ",(0,a.kt)("inlineCode",{parentName:"p"},"winutils chmod 777 /tmp/hive"),"."),(0,a.kt)("h2",{id:"windows-winutilsexe-is-not-working-correctly"},"Windows: winutils.exe is not working correctly"),(0,a.kt)("p",null,"Error:",(0,a.kt)("br",{parentName:"p"}),"\n",(0,a.kt)("inlineCode",{parentName:"p"},"winutils.exe - System Error The code execution cannot proceed because MSVCR100.dll was not found. Reinstalling the program may fix this problem."),"  "),(0,a.kt)("p",null,"Other errors are also possible:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Similar error message when double clicking on winutils.exe (Popup)"),(0,a.kt)("li",{parentName:"ul"},"Errors when providing a path to the configuration instead of a single configuration file"),(0,a.kt)("li",{parentName:"ul"},"ExitCodeException exitCode=-1073741515 when executing SDL even though everything ran without errors")),(0,a.kt)("p",null,"Solution:",(0,a.kt)("br",{parentName:"p"}),"\n","Install VC++ Redistributable Package from Microsoft:",(0,a.kt)("br",{parentName:"p"}),"\n",(0,a.kt)("a",{parentName:"p",href:"http://www.microsoft.com/en-us/download/details.aspx?id=5555"},"http://www.microsoft.com/en-us/download/details.aspx?id=5555")," (x86)",(0,a.kt)("br",{parentName:"p"}),"\n",(0,a.kt)("a",{parentName:"p",href:"http://www.microsoft.com/en-us/download/details.aspx?id=14632"},"http://www.microsoft.com/en-us/download/details.aspx?id=14632")," (x64)"),(0,a.kt)("h2",{id:"java-illegalaccesserror-java-17"},"Java IllegalAccessError (Java 17)"),(0,a.kt)("p",null,"Symptom:\nStarting an SDLB pipeline fails with the following exception:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x343570b7) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x343570b7\n        at org.apache.spark.storage.StorageUtils$.<init>(StorageUtils.scala:213)\n        ...\n")),(0,a.kt)("p",null,"Solution:\nJava 17 is more restrictive regarding usage of module exports. Unfortunately Spark uses classes from unexported packages. Packages can be exported manually. To fix above exception add ",(0,a.kt)("inlineCode",{parentName:"p"},"--add-exports java.base/sun.nio.ch=ALL-UNNAMED")," to the java command line, see also ",(0,a.kt)("a",{parentName:"p",href:"https://stackoverflow.com/questions/72230174/java-17-solution-for-spark-java-lang-noclassdeffounderror-could-not-initializ"},"Stackoverflow"),"."),(0,a.kt)("h2",{id:"resources-not-copied"},"Resources not copied"),(0,a.kt)("p",null,"Symptom:",(0,a.kt)("br",{parentName:"p"}),"\n","Tests fail due to missing or outdated resources or the execution starts but can not find the feeds specified.\nIntelliJ might not copy the resource files to the target directory."),(0,a.kt)("p",null,"Solution:",(0,a.kt)("br",{parentName:"p"}),"\n","Execute the maven goal ",(0,a.kt)("inlineCode",{parentName:"p"},"resources:resources")," (",(0,a.kt)("inlineCode",{parentName:"p"},"mvn resources:resources"),") manually after you changed any resource file."),(0,a.kt)("h2",{id:"maven-compile-error-toolsjar"},"Maven compile error: tools.jar"),(0,a.kt)("p",null,"Error:",(0,a.kt)("br",{parentName:"p"}),"\n",(0,a.kt)("inlineCode",{parentName:"p"},"Could not find artifact jdk.tools:jdk.tools:jar:1.7 at specified path ...")),(0,a.kt)("p",null,"Context:",(0,a.kt)("br",{parentName:"p"}),"\n","Hadoop/Spark has a dependency on the tools.jar file which is installed as part of the JDK installation."),(0,a.kt)("p",null,"Possible Reasons:  "),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Your system does not have a JDK installed (only a JRE).",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Fix: Make sure a JDK is installed and your PATH and JAVA_HOME environment variables are pointing to the JDK installation."))),(0,a.kt)("li",{parentName:"ol"},"You are using a Java 9 JDK or higher. The tools.jar has been removed in JDK 9. See: ",(0,a.kt)("a",{parentName:"li",href:"https://openjdk.java.net/jeps/220"},"https://openjdk.java.net/jeps/220"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Fix: Downgrade your JDK to Java 8.")))),(0,a.kt)("h2",{id:"how-can-i-test-hadoop--hdfs-locally-"},"How can I test Hadoop / HDFS locally ?"),(0,a.kt)("p",null,"When using ",(0,a.kt)("inlineCode",{parentName:"p"},"local://")," URIs, file permissions on Windows, or certain actions, local Hadoop binaries are required."),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Download your desired Apache Hadoop binary release from ",(0,a.kt)("a",{parentName:"li",href:"https://hadoop.apache.org/releases.html"},"https://hadoop.apache.org/releases.html"),"."),(0,a.kt)("li",{parentName:"ol"},"Extract the contents of the Hadoop distribution archive to a location of your choice, e.g., ",(0,a.kt)("inlineCode",{parentName:"li"},"/path/to/hadoop")," (Unix) or ",(0,a.kt)("inlineCode",{parentName:"li"},"C:\\path\\to\\hadoop")," (Windows)."),(0,a.kt)("li",{parentName:"ol"},"Set the environment variable ",(0,a.kt)("inlineCode",{parentName:"li"},"HADOOP_HOME=/path/to/hadoop")," (Unix) or ",(0,a.kt)("inlineCode",{parentName:"li"},"HADOOP_HOME=C:\\path\\to\\hadoop")," (Windows)."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("strong",{parentName:"li"},"Windows only"),": Download a Hadoop winutils distribution corresponding to your Hadoop version from ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/steveloughran/winutils"},"https://github.com/steveloughran/winutils")," (for newer Hadoop releases at: ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/cdarlint/winutils"},"https://github.com/cdarlint/winutils"),") and extract the contents to ",(0,a.kt)("inlineCode",{parentName:"li"},"%HADOOP_HOME%\\bin"),".")))}u.isMDXComponent=!0}}]);