"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[2816],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>m});var i=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},o=Object.keys(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=i.createContext({}),p=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=p(e.components);return i.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},u=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=p(n),m=a,f=u["".concat(l,".").concat(m)]||u[m]||c[m]||o;return n?i.createElement(f,r(r({ref:t},d),{},{components:n})):i.createElement(f,r({ref:t},d))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,r=new Array(o);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,r[1]=s;for(var p=2;p<o;p++)r[p]=n[p];return i.createElement.apply(null,r)}return i.createElement.apply(null,n)}u.displayName="MDXCreateElement"},6679:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var i=n(7462),a=(n(7294),n(3905));const o={id:"executionModes",title:"Execution Modes"},r=void 0,s={unversionedId:"reference/executionModes",id:"reference/executionModes",title:"Execution Modes",description:"This page is under review",source:"@site/docs/reference/executionModes.md",sourceDirName:"reference",slug:"/reference/executionModes",permalink:"/docs/reference/executionModes",draft:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/reference/executionModes.md",tags:[],version:"current",frontMatter:{id:"executionModes",title:"Execution Modes"},sidebar:"docs",previous:{title:"Execution Engines",permalink:"/docs/reference/executionEngines"},next:{title:"Transformations",permalink:"/docs/reference/transformations"}},l={},p=[{value:"Execution modes",id:"execution-modes",level:2},{value:"Partitions",id:"partitions",level:2},{value:"Default Behavior",id:"default-behavior",level:3},{value:"FailIfNoPartitionValuesMode",id:"failifnopartitionvaluesmode",level:3},{value:"PartitionDiffMode: Dynamic partition values filter",id:"partitiondiffmode-dynamic-partition-values-filter",level:3},{value:"CustomPartitionMode",id:"custompartitionmode",level:3},{value:"Incremental load",id:"incremental-load",level:2},{value:"SparkStreamingMode: Incremental load",id:"sparkstreamingmode-incremental-load",level:3},{value:"SparkIncrementalMode: Incremental Load",id:"sparkincrementalmode-incremental-load",level:3},{value:"ProcessAllMode",id:"processallmode",level:2},{value:"DataObjectStateIncrementalMode",id:"dataobjectstateincrementalmode",level:2},{value:"Execution Condition",id:"execution-condition",level:2}],d={toc:p};function c(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,i.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("admonition",{type:"warning"},(0,a.kt)("p",{parentName:"admonition"},"This page is under review ")),(0,a.kt)("h2",{id:"execution-modes"},"Execution modes"),(0,a.kt)("p",null,"Execution modes select the data to be processed. By default, if you start SmartDataLakeBuilder, there is no filter applied. This means every Action reads all data from its input DataObjects."),(0,a.kt)("p",null,'You can set an execution mode by defining attribute "executionMode" of an Action. Define the chosen ExecutionMode by setting type as follows:'),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"executionMode {\n  type = PartitionDiffMode\n  attribute1 = ...\n}\n")),(0,a.kt)("p",null,"If no explicit executionMode is specified, the default behavior is used (see chapter below)."),(0,a.kt)("p",null,"There are 2 major types of Execution Modes selecting the subset of data based on:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"partitions "),(0,a.kt)("li",{parentName:"ul"},"updates (incremental)")),(0,a.kt)("p",null,"With partitions, the user selects the targeted partition or selecting all different partitions.\nWith incremental, the difference between input and output can be selected. "),(0,a.kt)("h2",{id:"partitions"},"Partitions"),(0,a.kt)("h3",{id:"default-behavior"},"Default Behavior"),(0,a.kt)("p",null,"A filter based on partitions can be applied manually by specifying the command line parameter ",(0,a.kt)("inlineCode",{parentName:"p"},"--partition-values")," or ",(0,a.kt)("inlineCode",{parentName:"p"},"--multi-partition-values"),", see ",(0,a.kt)("a",{parentName:"p",href:"/docs/reference/commandLine"},"Command Line"),". The partition values specified are passed to ",(0,a.kt)("strong",{parentName:"p"},"all")," start-Actions of a DAG and filtered for every input DataObject by its defined partition columns.\nOn execution every Action takes the partition values of the input and filters them again for every output DataObject by its defined partition columns, which serve again as partition values for the input of the next Action.\nThis can be used without providing any explicit Execution Mode in the config of your Actions. It is the Default Behavior of all Actions.\nNote that during execution of the dag, no new partition values are added, they are only filtered. An exception is if you place a ",(0,a.kt)("inlineCode",{parentName:"p"},"PartitionDiffMode")," in the middle of your pipeline, see section ",(0,a.kt)("a",{parentName:"p",href:"#partitiondiffmode-dynamic-partition-values-filter"},"PartitionDiffMode")," below.\nIf the parameters ",(0,a.kt)("inlineCode",{parentName:"p"},"--partition-values")," or ",(0,a.kt)("inlineCode",{parentName:"p"},"--multi-partition-values")," are not specified, SDLB will process all available data."),(0,a.kt)("h3",{id:"failifnopartitionvaluesmode"},"FailIfNoPartitionValuesMode"),(0,a.kt)("p",null,"The ",(0,a.kt)("em",{parentName:"p"},"FailIfNoPartitionValuesMode")," enforces to specified partition values. It simply checks if partition values are present and fails otherwise.\nThis is useful to prevent potential reprocessing of whole tables due to wrong usage."),(0,a.kt)("h3",{id:"partitiondiffmode-dynamic-partition-values-filter"},"PartitionDiffMode: Dynamic partition values filter"),(0,a.kt)("p",null,"In contrast to specifying the partitions manually, you can let SmartDataLakeBuilder find missing partitions and set partition values automatically by specifying execution mode ",(0,a.kt)("em",{parentName:"p"},"PartitionDiffMode"),". This mode has a couple of options to fine tune:"),(0,a.kt)("p",null,"By defining the ",(0,a.kt)("strong",{parentName:"p"},"applyCondition")," attribute you can give a condition to decide at runtime if the PartitionDiffMode should be applied or not.\nDefault is to apply the PartitionDiffMode if the given partition values are empty (partition values from command line or passed from previous action).\nDefine an applyCondition by a spark sql expression working with attributes of DefaultExecutionModeExpressionData returning a boolean."),(0,a.kt)("p",null,"By defining the ",(0,a.kt)("strong",{parentName:"p"},"failCondition")," attribute you can give a condition to fail application of execution mode if ",(0,a.kt)("em",{parentName:"p"},"true"),".\nIt can be used to fail a run based on expected partitions, time and so on.\nThe expression is evaluated after execution of PartitionDiffMode. In the condition are attributes available, like ",(0,a.kt)("strong",{parentName:"p"},"inputPartitionValues"),", ",(0,a.kt)("strong",{parentName:"p"},"outputPartitionValues")," and ",(0,a.kt)("strong",{parentName:"p"},"selectedPartitionValues")," , amongst others, to make the decision.\nDefault is that the application of the PartitionDiffMode does not fail the action. If there is no data to process, the following actions are skipped.\nDefine a failCondition by a spark sql expression working with attributes of PartitionDiffModeExpressionData returning a boolean."),(0,a.kt)("p",null,'Example - fail if partitions are not processed in strictly increasing order of partition column "dt":'),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'  failCondition = "(size(selectedPartitionValues) > 0 and array_min(transform(selectedPartitionValues, x -&gt x.dt)) &lt array_max(transform(outputPartitionValues, x > x.dt)))"\n')),(0,a.kt)("p",null,"Sometimes the failCondition can become quite complex with multiple terms concatenated by or-logic.\nTo improve interpretability of error messages, multiple fail conditions can be configured as array with attribute ",(0,a.kt)("strong",{parentName:"p"},"failConditions"),". For every condition you can also define a description which will be inserted into the error message."),(0,a.kt)("p",null,"The option ",(0,a.kt)("strong",{parentName:"p"},"selectExpression")," defines the selection of custom partitions.\nDefine a spark sql expression working with attributes of PartitionDiffModeExpressionData returning a Seq(Map(String,String)).\nExample - only process the last selected partition:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'  selectExpression = "slice(selectedPartitionValues,-1,1)"\n')),(0,a.kt)("p",null,"By defining ",(0,a.kt)("strong",{parentName:"p"},"alternativeOutputId")," attribute you can define another DataObject which will be used to check for already existing data.\nThis can be used to select data to process against a DataObject later in the pipeline."),(0,a.kt)("h3",{id:"custompartitionmode"},"CustomPartitionMode"),(0,a.kt)("p",null,"This prepares for custom specification of partition logic in scala.\nImplement trait CustomPartitionModeLogic by defining a function which receives main input&output DataObject and returns partition values to process as Seq[Map","[String,String]","]"),(0,a.kt)("h2",{id:"incremental-load"},"Incremental load"),(0,a.kt)("h3",{id:"sparkstreamingmode-incremental-load"},"SparkStreamingMode: Incremental load"),(0,a.kt)("p",null,'Some DataObjects are not partitioned, but nevertheless you don\'t want to read all data from the input on every run. You want to load it incrementally.\nThis can be accomplished by specifying execution mode SparkStreamingMode. Under the hood it uses "Spark Structured Streaming".\nIn streaming mode this an Action with SparkStreamingMode is an asynchronous action. Its rhythm can be configured by setting triggerType and triggerTime.\nIf not in streaming mode SparkStreamingMode triggers a single microbatch by using triggerType=Once and is fully synchronized. Synchronous execution can be forced for streaming mode as well by explicitly setting triggerType=Once.\n"Spark Structured Streaming" is keeping state information about processed data. It needs a checkpointLocation configured which can be given as parameter to SparkStreamingMode.'),(0,a.kt)("p",null,'Note that "Spark Structured Streaming" needs an input DataObject supporting the creation of streaming DataFrames.\nFor the time being, only the input sources delivered with Spark Streaming are supported.\nThis are KafkaTopicDataObject and all SparkFileDataObjects, see also ',(0,a.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#creating-streaming-dataframes-and-streaming-datasets"},"Spark StructuredStreaming"),"."),(0,a.kt)("h3",{id:"sparkincrementalmode-incremental-load"},"SparkIncrementalMode: Incremental Load"),(0,a.kt)("p",null,"As not every input DataObject supports the creation of streaming DataFrames, there is an other execution mode called SparkIncrementalMode.\nYou configure it by defining the attribute ",(0,a.kt)("strong",{parentName:"p"},"compareCol")," with a column name present in input and output DataObject.\nSparkIncrementalMode then compares the maximum values between input and output and creates a filter condition.\nOn execution the filter condition is applied to the input DataObject to load the missing increment.\nNote that compareCol needs to have a sortable datatype."),(0,a.kt)("p",null,"By defining ",(0,a.kt)("strong",{parentName:"p"},"applyCondition")," attribute you can give a condition to decide at runtime if the SparkIncrementalMode should be applied or not.\nDefault is to apply the SparkIncrementalMode. Define an applyCondition by a spark sql expression working with attributes of DefaultExecutionModeExpressionData returning a boolean."),(0,a.kt)("p",null,"By defining ",(0,a.kt)("strong",{parentName:"p"},"alternativeOutputId")," attribute you can define another DataObject which will be used to check for already existing data.\nThis can be used to select data to process against a DataObject later in the pipeline."),(0,a.kt)("h2",{id:"processallmode"},"ProcessAllMode"),(0,a.kt)("p",null,"An execution mode which forces processing all data from it's inputs, removing partitionValues and filter conditions received from previous actions."),(0,a.kt)("h2",{id:"dataobjectstateincrementalmode"},"DataObjectStateIncrementalMode"),(0,a.kt)("p",null,"An execution mode for incremental processing by remembering DataObjects state from last increment. The state is saved in a .json file for which the user needs to provide a path using the ",(0,a.kt)("inlineCode",{parentName:"p"},"--state-path")," option. An application name must also be specified with the ",(0,a.kt)("inlineCode",{parentName:"p"},"-n")," option. The state file will be named after the application name."),(0,a.kt)("h2",{id:"execution-condition"},"Execution Condition"),(0,a.kt)("p",null,"For every Action an executionCondition can be defined. The execution condition allows to define if an action is executed or skipped. The default behaviour is that an Action is skipped if at least one input SubFeed is skipped.\nDefine an executionCondition by a spark sql expression working with attributes of SubFeedsExpressionData returning a boolean.\nThe Action is skipped if the executionCondition is evaluated to false. In that case dependent actions get empty SubFeeds marked with isSkipped=true as input."),(0,a.kt)("p",null,"Example - skip Action only if input1 and input2 SubFeed are skipped:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'  executionCondition = "!inputSubFeeds.input1.isSkipped or !inputSubFeeds.input2.isSkipped"\n')),(0,a.kt)("p",null,"Example - Always execute Action and use all existing data as input:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"  executionCondition = true\n  executionMode = ProcessAllMode\n")))}c.isMDXComponent=!0}}]);