"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[130],{3905:function(e,t,a){a.d(t,{Zo:function(){return u},kt:function(){return m}});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},u=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),d=c(a),m=o,h=d["".concat(l,".").concat(m)]||d[m]||p[m]||r;return a?n.createElement(h,i(i({ref:t},u),{},{components:a})):n.createElement(h,i({ref:t},u))}));function m(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var c=2;c<r;c++)i[c]=a[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},8215:function(e,t,a){var n=a(7294);t.Z=function(e){var t=e.children,a=e.hidden,o=e.className;return n.createElement("div",{role:"tabpanel",hidden:a,className:o},t)}},9877:function(e,t,a){a.d(t,{Z:function(){return u}});var n=a(7462),o=a(7294),r=a(2389),i=a(9548),s=a(6010),l="tabItem_LplD";function c(e){var t,a,r,c=e.lazy,u=e.block,p=e.defaultValue,d=e.values,m=e.groupId,h=e.className,f=o.Children.map(e.children,(function(e){if((0,o.isValidElement)(e)&&void 0!==e.props.value)return e;throw new Error("Docusaurus error: Bad <Tabs> child <"+("string"==typeof e.type?e.type:e.type.name)+'>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.')})),k=null!=d?d:f.map((function(e){var t=e.props;return{value:t.value,label:t.label,attributes:t.attributes}})),g=(0,i.lx)(k,(function(e,t){return e.value===t.value}));if(g.length>0)throw new Error('Docusaurus error: Duplicate values "'+g.map((function(e){return e.value})).join(", ")+'" found in <Tabs>. Every value needs to be unique.');var v=null===p?p:null!=(t=null!=p?p:null==(a=f.find((function(e){return e.props.default})))?void 0:a.props.value)?t:null==(r=f[0])?void 0:r.props.value;if(null!==v&&!k.some((function(e){return e.value===v})))throw new Error('Docusaurus error: The <Tabs> has a defaultValue "'+v+'" but none of its children has the corresponding value. Available values are: '+k.map((function(e){return e.value})).join(", ")+". If you intend to show no default tab, use defaultValue={null} instead.");var b=(0,i.UB)(),w=b.tabGroupChoices,y=b.setTabGroupChoices,N=(0,o.useState)(v),x=N[0],O=N[1],j=[],T=(0,i.o5)().blockElementScrollPositionUntilNextRender;if(null!=m){var D=w[m];null!=D&&D!==x&&k.some((function(e){return e.value===D}))&&O(D)}var S=function(e){var t=e.currentTarget,a=j.indexOf(t),n=k[a].value;n!==x&&(T(t),O(n),null!=m&&y(m,n))},P=function(e){var t,a=null;switch(e.key){case"ArrowRight":var n=j.indexOf(e.currentTarget)+1;a=j[n]||j[0];break;case"ArrowLeft":var o=j.indexOf(e.currentTarget)-1;a=j[o]||j[j.length-1]}null==(t=a)||t.focus()};return o.createElement("div",{className:"tabs-container"},o.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.Z)("tabs",{"tabs--block":u},h)},k.map((function(e){var t=e.value,a=e.label,r=e.attributes;return o.createElement("li",(0,n.Z)({role:"tab",tabIndex:x===t?0:-1,"aria-selected":x===t,key:t,ref:function(e){return j.push(e)},onKeyDown:P,onFocus:S,onClick:S},r,{className:(0,s.Z)("tabs__item",l,null==r?void 0:r.className,{"tabs__item--active":x===t})}),null!=a?a:t)}))),c?(0,o.cloneElement)(f.filter((function(e){return e.props.value===x}))[0],{className:"margin-vert--md"}):o.createElement("div",{className:"margin-vert--md"},f.map((function(e,t){return(0,o.cloneElement)(e,{key:t,hidden:e.props.value!==x})}))))}function u(e){var t=(0,r.Z)();return o.createElement(c,(0,n.Z)({key:String(t)},e))}},1593:function(e,t,a){a.r(t),a.d(t,{contentTitle:function(){return u},default:function(){return h},frontMatter:function(){return c},metadata:function(){return p},toc:function(){return d}});var n=a(7462),o=a(3366),r=(a(7294),a(3905)),i=a(9877),s=a(8215),l=["components"],c={title:"Select Columns"},u=void 0,p={unversionedId:"getting-started/part-1/select-columns",id:"getting-started/part-1/select-columns",title:"Select Columns",description:"Goal",source:"@site/docs/getting-started/part-1/select-columns.md",sourceDirName:"getting-started/part-1",slug:"/getting-started/part-1/select-columns",permalink:"/docs/getting-started/part-1/select-columns",editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/getting-started/part-1/select-columns.md",tags:[],version:"current",frontMatter:{title:"Select Columns"},sidebar:"docs",previous:{title:"Get Airports",permalink:"/docs/getting-started/part-1/get-airports"},next:{title:"Joining It Together",permalink:"/docs/getting-started/part-1/joining-it-together"}},d=[{value:"Goal",id:"goal",children:[],level:2},{value:"Define output object",id:"define-output-object",children:[],level:2},{value:"Define select-airport-cols action",id:"define-select-airport-cols-action",children:[],level:2},{value:"Try it out",id:"try-it-out",children:[],level:2},{value:"More on Feeds",id:"more-on-feeds",children:[],level:2},{value:"Example of Common Mistake",id:"example-of-common-mistake",children:[],level:2}],m={toc:d};function h(e){var t=e.components,c=(0,o.Z)(e,l);return(0,r.kt)("wrapper",(0,n.Z)({},m,c,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"goal"},"Goal"),(0,r.kt)("p",null,"In this step we write our first Action that modifies data.\nWe will continue based upon the config file available ",(0,r.kt)("a",{target:"_blank",href:a(3656).Z},"here"),".\nWhen you look at the data in the folder ",(0,r.kt)("em",{parentName:"p"},"data/stg-airports/result.csv"),", you will notice that we\ndon't need most of the columns. In this step, we will write a simple ",(0,r.kt)("em",{parentName:"p"},"CopyAction")," that selects only the columns we\nare interested in."),(0,r.kt)("p",null,"As usual, we need to define an output DataObject and an action.\nWe don't need to define a new input DataObject as we will wire our new action to the existing DataObject ",(0,r.kt)("em",{parentName:"p"},"stg-airports"),". "),(0,r.kt)("h2",{id:"define-output-object"},"Define output object"),(0,r.kt)("p",null,"Let's use CsvFileDataObject again because that makes it easy for us to check the result.\nIn more advanced (speak: real-life) scenarios, we would use one of numerous other possibilities,\nsuch as HiveTableDataObject, SplunkDataObject...\nSee ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/smart-data-lake/smart-data-lake/blob/develop-spark3/docs/Reference.md#data-objects"},"this list")," for an overview.\nYou can also consult the ",(0,r.kt)("a",{parentName:"p",href:"https://smartdatalake.ch/json-schema-viewer/index.html#viewer-page?v=2"},"Configuration Schema Browser")," to get a list of all Data Objects and related properties."),(0,r.kt)("p",null,"In a first step, we want to make the airport data more understandable by removing any columns we don't need.\nSince we don't introduce any business logic into the transformation,\nthe resulting data object will reside in the integration layer and thus will be called ",(0,r.kt)("em",{parentName:"p"},"int-airports"),".\nPut this in the existing dataObjects section:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'  int-airports {\n    type = CsvFileDataObject\n    path = "~{id}"\n  }\n')),(0,r.kt)("h2",{id:"define-select-airport-cols-action"},"Define select-airport-cols action"),(0,r.kt)("p",null,"Next, add these lines in the existing actions section:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'  select-airport-cols {\n    type = CopyAction\n    inputId = stg-airports\n    outputId = int-airports\n    transformers = [{\n        type = SQLDfTransformer\n        code = "select ident, name, latitude_deg, longitude_deg from stg_airports"\n    }]\n    metadata {\n      feed = compute\n    }\n  }\n')),(0,r.kt)("p",null,"A couple of things to note here:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"We just defined a new action called ",(0,r.kt)("em",{parentName:"li"},"select-airport-cols"),". "),(0,r.kt)("li",{parentName:"ul"},"We wired it together with the two DataObjects ",(0,r.kt)("em",{parentName:"li"},"stg-airports")," and ",(0,r.kt)("em",{parentName:"li"},"int-airports"),"."),(0,r.kt)("li",{parentName:"ul"},"A new type of Action was used: CopyAction. This action is intended to copy data from one data object to another\nwith some optional transformations of the data along the way."),(0,r.kt)("li",{parentName:"ul"},"To define the transformations of an action, you define a list of HOCON Objects.\nHOCON-Objects are just like JSON-Objects (with a few added features, but more on that later)."),(0,r.kt)("li",{parentName:"ul"},"Instead of allowing for just one transformer, we could potentially have multiple transformers within the same action that\nget executed one after the other. That's why we have the bracket followed by the curly brace ",(0,r.kt)("inlineCode",{parentName:"li"},"[{")," :\nthe CustomSparkAction expects it's field ",(0,r.kt)("em",{parentName:"li"},"transformers")," to be a list of HOCON Objects."),(0,r.kt)("li",{parentName:"ul"},"There's different kinds of transformers, in this case we defined a ",(0,r.kt)("em",{parentName:"li"},"SQLDfTransformer")," and provided it with a custom SQL-Code.\nThere are other transformer types such as ",(0,r.kt)("em",{parentName:"li"},"ScalaCodeDfTransformer"),", ",(0,r.kt)("em",{parentName:"li"},"PythonCodeDfTransformer"),"... More on that later.")),(0,r.kt)("div",{className:"admonition admonition-caution alert alert--warning"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"caution")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},'Notice that we call our input DataObject stg-airports with a hyphen "-", but in the sql, we call it "stg',"_",'airports" with an underscore "',"_",'".\nThis is due to the SQL standard not allowing "-" in unquoted identifiers (e.g. table names).\nUnder the hood, Apache Spark SQL is used to execute the query, which implements SQL standard.\nSDL works around this by replacing special chars in DataObject names used in SQL statements for you.\nIn this case, it automatically replaced ',(0,r.kt)("inlineCode",{parentName:"p"},"-")," with ",(0,r.kt)("inlineCode",{parentName:"p"},"_")))),(0,r.kt)("p",null,"There are numerous other options available for the CopyAction, which you can view in the ",(0,r.kt)("a",{parentName:"p",href:"https://smartdatalake.ch/json-schema-viewer/index.html#viewer-page?v=3-0"},"Configuration Schema Browser"),"."),(0,r.kt)("h2",{id:"try-it-out"},"Try it out"),(0,r.kt)("p",null,"Note that we used a different feed this time, we called it ",(0,r.kt)("em",{parentName:"p"},"compute"),".\nWe will keep expanding the feed ",(0,r.kt)("em",{parentName:"p"},"compute")," in the next few steps.\nThis allows us to keep the data we downloaded in the previous steps in our local files and just\ntry out our new actions."),(0,r.kt)("p",null,"To execute the pipeline, use the same command as before, but change the feed to compute:"),(0,r.kt)(i.Z,{groupId:"docker-podman-switch",defaultValue:"docker",values:[{label:"Docker",value:"docker"},{label:"Podman",value:"podman"}],mdxType:"Tabs"},(0,r.kt)(s.Z,{value:"docker",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-jsx"},"docker run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel compute\n"))),(0,r.kt)(s.Z,{value:"podman",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-jsx"},"podman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel compute\n")))),(0,r.kt)("div",{className:"admonition admonition-caution alert alert--warning"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"caution")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"If you encounter an error that looks like this:"),(0,r.kt)("pre",{parentName:"div"},(0,r.kt)("code",{parentName:"pre"},"Exception in thread \"main\" io.smartdatalake.util.dag.TaskFailedException: Task select-airport-cols failed. \nRoot cause is 'IllegalArgumentException: requirement failed: (DataObject~stg-airports) DataObject schema \nis undefined. A schema must be defined if there are no existing files.'\nCaused by: java.lang.IllegalArgumentException: requirement failed: (DataObject~stg-airports) DataObject \nschema is undefined. A schema must be defined if there are no existing files.\n")),(0,r.kt)("p",{parentName:"div"},"Execute the ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("inlineCode",{parentName:"strong"},"download")),"-feed again. After that feed was successfully executed, the execution of the feed ",(0,r.kt)("inlineCode",{parentName:"p"},".*")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"compute")," will work.\nMore on this problem in the list of ",(0,r.kt)("a",{parentName:"p",href:"/docs/getting-started/troubleshooting/common-problems"},"Common Problems"),"."))),(0,r.kt)("p",null,"Now you should see multiple files in the folder ",(0,r.kt)("em",{parentName:"p"},"data/int-airports"),". Why is it split accross multiple files?\nThis is due to the fact that the query runs with Apache Spark under the hood which computes the query in parallel for different portions of the data.\nWe might work on a small data set for now, but keep in mind that this would scale up horizontally for large amounts of data."),(0,r.kt)("h2",{id:"more-on-feeds"},"More on Feeds"),(0,r.kt)("p",null,"SDL gives you precise control on which actions you want to execute.\nFor instance if you only want to execute the action that we just wrote, you can type"),(0,r.kt)(i.Z,{groupId:"docker-podman-switch",defaultValue:"docker",values:[{label:"Docker",value:"docker"},{label:"Podman",value:"podman"}],mdxType:"Tabs"},(0,r.kt)(s.Z,{value:"docker",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-jsx"},"docker run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel ids:select-airport-cols\n"))),(0,r.kt)(s.Z,{value:"podman",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-jsx"},"podman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel ids:select-airport-cols\n")))),(0,r.kt)("p",null,"Of course, at this stage, the feed ",(0,r.kt)("em",{parentName:"p"},"compute")," only contains this one action, so the result will be the same."),(0,r.kt)("p",null,"SDL also allows you to use combinations of expressions to select the actions you want to execute. You can run"),(0,r.kt)(i.Z,{groupId:"docker-podman-switch",defaultValue:"docker",values:[{label:"Docker",value:"docker"},{label:"Podman",value:"podman"}],mdxType:"Tabs"},(0,r.kt)(s.Z,{value:"docker",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-jsx"},"docker run --rm sdl-spark:latest --help\n"))),(0,r.kt)(s.Z,{value:"podman",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-jsx"},"podman run --rm sdl-spark:latest --help\n")))),(0,r.kt)("p",null,"to see all options that are available. For your convenience, here is the current output of the help command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"  -f, --feed-sel <operation?><prefix:?><regex>[,<operation?><prefix:?><regex>...]\n                           Select actions to execute by one or multiple expressions separated by comma (,). Results from multiple expressions are combined from left to right.\n                           Operations:\n                           - pipe symbol (|): the two sets are combined by union operation (default)\n                           - ampersand symbol (&): the two sets are combined by intersection operation\n                           - minus symbol (-): the second set is subtracted from the first set\n                           Prefixes:\n                           - 'feeds': select actions where metadata.feed is matched by regex pattern (default)\n                           - 'names': select actions where metadata.name is matched by regex pattern\n                           - 'ids': select actions where id is matched by regex pattern\n                           - 'layers': select actions where metadata.layer of all output DataObjects is matched by regex pattern\n                           - 'startFromActionIds': select actions which with id is matched by regex pattern and any dependent action (=successors)\n                           - 'endWithActionIds': select actions which with id is matched by regex pattern and their predecessors\n                           - 'startFromDataObjectIds': select actions which have an input DataObject with id is matched by regex pattern and any dependent action (=successors)\n                           - 'endWithDataObjectIds': select actions which have an output DataObject with id is matched by regex pattern and their predecessors\n                           All matching is done case-insensitive.\n                           Example: to filter action 'A' and its successors but only in layer L1 and L2, use the following pattern: \"startFromActionIds:a,&layers:(l1|l2)\"\n  -n, --name <value>       Optional name of the application. If not specified feed-sel is used.\n  -c, --config <file1>[,<file2>...]\n                           One or multiple configuration files or directories containing configuration files, separated by comma. Entries must be valid Hadoop URIs or a special URI with scheme \"cp\" which is treated as classpath entry.\n  --partition-values <partitionColName>=<partitionValue>[,<partitionValue>,...]\n                           Partition values to process for one single partition column.\n  --multi-partition-values <partitionColName1>=<partitionValue>,<partitionColName2>=<partitionValue>[;(<partitionColName1>=<partitionValue>,<partitionColName2>=<partitionValue>;...]\n                           Partition values to process for multiple partitoin columns.\n  -s, --streaming          Enable streaming mode for continuous processing.\n  --parallelism <int>      Parallelism for DAG run.\n  --state-path <path>      Path to save run state files. Must be set to enable recovery in case of failures.\n  --override-jars <jar1>[,<jar2>...]\n                           Comma separated list of jar filenames for child-first class loader. The jars must be present in classpath.\n  --test <config|dry-run>  Run in test mode: config -> validate configuration, dry-run -> execute prepare- and init-phase only to check environment and spark lineage\n  --help                   Display the help text.\n  --version                Display version information.\n  -m, --master <value>     The Spark master URL passed to SparkContext (default=local[*], yarn, spark://HOST:PORT, mesos://HOST:PORT, k8s://HOST:PORT).\n  -x, --deploy-mode <value>\n                           The Spark deploy mode passed to SparkContext (default=client, cluster).\n  -d, --kerberos-domain <value>\n                           Kerberos-Domain for authentication (USERNAME@KERBEROS-DOMAIN) in local mode.\n  -u, --username <value>   Kerberos username for authentication (USERNAME@KERBEROS-DOMAIN) in local mode.\n  -k, --keytab-path <value>\n                           Path to the Kerberos keytab file for authentication in local mode.\n")),(0,r.kt)("p",null,"One popular option is to use regular expressions to execute multiple feeds together.\nIn our case, we can run the entire data pipeline with the following command : "),(0,r.kt)(i.Z,{groupId:"docker-podman-switch",defaultValue:"docker",values:[{label:"Docker",value:"docker"},{label:"Podman",value:"podman"}],mdxType:"Tabs"},(0,r.kt)(s.Z,{value:"docker",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-jsx"},"docker run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel .*\n"))),(0,r.kt)(s.Z,{value:"podman",mdxType:"TabItem"},(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-jsx"},"podman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest --config /mnt/config --feed-sel .*\n")))),(0,r.kt)("h2",{id:"example-of-common-mistake"},"Example of Common Mistake"),(0,r.kt)("p",null,"One common mistake is mixing up the types of Data Objects.\nTo give you some experience on how to debug your config, you can also try out what happens if you change the type of ",(0,r.kt)("em",{parentName:"p"},"stg-airports")," to JsonFileDataObject.\nYou will get an error message which indicates that there might be some format problem, but it is hard to spot :"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"}," Error: cannot resolve '`ident`' given input columns: [stg_airports._corrupt_record]; line 1 pos 7;\n")),(0,r.kt)("p",null,"The FileTransferAction will save the result from the Webservice with the JsonFileDataObject as file with filetype ","*",".json.\nThen Spark tries to parse the CSV-records in the ","*",".json file with a JSON-Parser. It is unable to properly read the data.\nHowever, it generates a column named ",(0,r.kt)("em",{parentName:"p"},"_corrupt_record")," describing what went wrong.\nIf you know Apache Spark, this column will look very familiar to you.\nAfter that, the query fails, because it only finds that column with error messages instead of the actual data."),(0,r.kt)("p",null,"One way to get a better error message is to tell Spark that it should promptly fail when reading a corrupt file.\nYou can do that with the option ",(0,r.kt)("a",{parentName:"p",href:"https://smartdatalake.ch/json-schema-viewer/index.html#viewer-page?v=2-13-3"},"jsonOptions"),",\nwhich allows you to directly pass on settings to Spark."),(0,r.kt)("p",null,"In our case, we would end up with a faulty dataObject that looks like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'  stg-airports {\n    type = JsonFileDataObject\n    path = "~{id}"\n    jsonOptions {\n      "mode"="failfast"\n    }\n  }\n')),(0,r.kt)("p",null,"This time, it will fail with this error message:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Exception in thread \"main\" io.smartdatalake.workflow.TaskFailedException: Task select-airport-cols failed. \nRoot cause is 'SparkException: Malformed records are detected in schema inference. \nParse Mode: FAILFAST. Reasons: Failed to infer a common schema. Struct types are expected, but `string` was found.'\n")))}h.isMDXComponent=!0},3656:function(e,t,a){t.Z=a.p+"assets/files/application-part1-download-b129c4e7b7f455917f3f1941b51b191b.conf"}}]);