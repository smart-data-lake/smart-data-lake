"use strict";(self.webpackChunksmart_data_lake=self.webpackChunksmart_data_lake||[]).push([[4281],{3308:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>c,toc:()=>o});var n=a(4848),s=a(8453);const r={id:"dataObjects",title:"Data Objects"},i=void 0,c={id:"reference/dataObjects",title:"Data Objects",description:"DataObjects are the core element of Smart data Lake Builder. These objects define data entities and how they can be accessed by properties including location, type and others.",source:"@site/docs/reference/dataObjects.md",sourceDirName:"reference",slug:"/reference/dataObjects",permalink:"/docs/reference/dataObjects",draft:!1,unlisted:!1,editUrl:"https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/reference/dataObjects.md",tags:[],version:"current",frontMatter:{id:"dataObjects",title:"Data Objects"},sidebar:"tutorialSidebar",previous:{title:"Command Line",permalink:"/docs/reference/commandLine"},next:{title:"Actions",permalink:"/docs/reference/actions"}},l={},o=[{value:"Object Types",id:"object-types",level:2},{value:"Schema",id:"schema",level:2}];function d(e){const t={a:"a",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.p,{children:"DataObjects are the core element of Smart data Lake Builder. These objects define data entities and how they can be accessed by properties including location, type and others."}),"\n",(0,n.jsx)(t.p,{children:"Furthermore, a section with metadata can be used."}),"\n",(0,n.jsxs)(t.p,{children:["All available DataObjects and their parameters are listed in the ",(0,n.jsx)(t.a,{href:"http://smartdatalake.ch/json-schema-viewer/index.html",children:"Schema Viewer"}),"."]}),"\n",(0,n.jsx)(t.h2,{id:"object-types",children:"Object Types"}),"\n",(0,n.jsx)(t.p,{children:"Smart Data Lake Builder supports beside a list of file types also database objects and general connectors."}),"\n",(0,n.jsx)(t.p,{children:"Examples:"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"file based: CSV, JSON, XML, Parquet, ..."}),"\n",(0,n.jsx)(t.li,{children:"database based: JDBC, DeltaLake, Hive ..."}),"\n",(0,n.jsx)(t.li,{children:"connectors: Airbyte, sFTP, Webservice, ..."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"If necessary, additional formats can be implemented by providing an own DataObject implementation (Java/Scala), or implementing an Airbyte connector (Python)."}),"\n",(0,n.jsx)(t.h2,{id:"schema",children:"Schema"}),"\n",(0,n.jsx)(t.p,{children:"SDLB requires a schema for data object that can create DataFrames, if they are used as starting point of a DAG. The schema can be"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"specified manually with an option in the dataObject"}),"\n",(0,n.jsx)(t.li,{children:"specified by adding a sample file or"}),"\n",(0,n.jsxs)(t.li,{children:["inferred from data (sampling).\r\nThe latter enables fast development, but should be avoided in production.\r\nBy default SDLB will verify that new data fits the existing schema, otherwise an error will be thrown. If desired schema evolution can be enabled using ",(0,n.jsx)(t.strong,{children:"allowSchemaEvolution"})," on several DataObjects, e.g. JdbcTableDataObject and DeltaLakeTableObject. Then, old rows will get null in new columns and new rows get null in old (not existing anymore) columns."]}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:["See also details in ",(0,n.jsx)(t.a,{href:"/docs/reference/schema",children:"Schema"})]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,t,a)=>{a.d(t,{R:()=>i,x:()=>c});var n=a(6540);const s={},r=n.createContext(s);function i(e){const t=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);