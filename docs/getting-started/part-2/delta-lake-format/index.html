<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<title data-react-helmet="true">Delta Lake - a better data format | Smart Data Lake Builder</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://www.smartdatalake.ch/docs/getting-started/part-2/delta-lake-format"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Delta Lake - a better data format | Smart Data Lake Builder"><meta data-react-helmet="true" name="description" content="Goal"><meta data-react-helmet="true" property="og:description" content="Goal"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://www.smartdatalake.ch/docs/getting-started/part-2/delta-lake-format"><link data-react-helmet="true" rel="alternate" href="https://www.smartdatalake.ch/docs/getting-started/part-2/delta-lake-format" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://www.smartdatalake.ch/docs/getting-started/part-2/delta-lake-format" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.f0e4dc4d.css">
<link rel="preload" href="/assets/js/runtime~main.a3d10d32.js" as="script">
<link rel="preload" href="/assets/js/main.33d7ec33.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/sdl_logo.png" alt="Smart Data Lake Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/sdl_logo.png" alt="Smart Data Lake Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Smart Data Lake</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/smart-data-lake/smart-data-lake" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/">Smart Data Lake</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/getting-started/setup">Getting Started</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/setup">Technical Setup</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/get-input-data">Inputs</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" tabindex="0" href="/docs/getting-started/part-1/get-departures">Part 1</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" tabindex="0" href="/docs/getting-started/part-2/industrializing">Part 2</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/part-2/industrializing">Industrializing our data pipeline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/getting-started/part-2/delta-lake-format">Delta Lake - a better data format</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/getting-started/part-2/historical-data">Keeping historical data</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" tabindex="0" href="/docs/getting-started/part-3/custom-webservice">Part 3</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" tabindex="0" href="/docs/getting-started/troubleshooting/common-problems">Troubleshooting</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/reference/build">Reference</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/JsonSchemaViewer">Configuration Schema Viewer</a></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Delta Lake - a better data format</h1></header><h2 class="anchor anchorWithStickyNavbar_mojV" id="goal">Goal<a class="hash-link" href="#goal" title="Direct link to heading">â€‹</a></h2><p>Up to now we have used CSV with CsvFileDataObject as file format. We will switch to a more modern data format in this step which supports a catalog, compression and even transactions.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="file-formats">File formats<a class="hash-link" href="#file-formats" title="Direct link to heading">â€‹</a></h2><p>Smart Data Lake Builder has built in support for many data formats and technologies.
An important one is storing files on a Hadoop filesystem, supporting standard file formats such as CSV, Json, Avro or Parquet.
In Part 1 we have used CSV through the CsvFileDataObject. CSV files can be easily checked in an editor or Excel, but the format also has many problems, e.g. support of multi-line strings or lack of data type definition.
Often Parquet format is used, as it includes a schema definition and is very space efficient through its columnar compression.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="catalog">Catalog<a class="hash-link" href="#catalog" title="Direct link to heading">â€‹</a></h2><p>Just storing files on Hadoop filesystem makes it difficult to use them in a SQL engine such as Spark SQL. You need a metadata layer on top which stores table definitions. This is also called a metastore or catalog.
If you start a Spark session, a configuration to connect to an external catalog can be set, or otherwise Spark creates an internal catalog for the session.
We could register our CSV files in this catalog by creating a table via a DDL-statement, including the definition of all columns, a path and the format of our data.
But you could also directly create and write into a table by using Spark Hive tables.
Smart Data Lake Builder supports this by the HiveTableDataObject. It always uses Parquet file format in the background as a best practice, although Hive tables could also be created on top of CSV files.</p><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</h5></div><div class="admonition-content"><p>Hive is a Metadata layer and SQL engine on top of a Hadoop filesystem. Spark uses the metadata layer of Hive, but implements its own SQL engine.</p></div></div><h2 class="anchor anchorWithStickyNavbar_mojV" id="transactions">Transactions<a class="hash-link" href="#transactions" title="Direct link to heading">â€‹</a></h2><p>Hive tables with Parquet format are lacking transactions. This means for example that writing and reading the table at the same time could result in failure or empty results.
In consequence </p><ul><li>consecutive jobs need to by synchronized</li><li>it&#x27;s not recommended having end-user accessing the table while data processing jobs are running</li><li>update and deletes are not supported</li></ul><p>There are other options like classical databases which always had a metadata layer, offer transactions but don&#x27;t integrate easily with Hive metastore and cheap, scalable Hadoop file storage.
Nevertheless, Smart Data Lake Builder supports classical databases through the JdbcTableDataObject.
Fortunately there is a new technology called Delta Lake, see also <a href="https://delta.io/" target="_blank" rel="noopener noreferrer">delta.io</a>. It integrates into a Hive metastore, supports transactions and stores Parquet files and a transaction log on hadoop filesystems.
Smart Data Lake Builder supports this by the DeltaLakeTableDataObject, and this is what we are going to use for our airport and departure data now.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="deltalaketabledataobject">DeltaLakeTableDataObject<a class="hash-link" href="#deltalaketabledataobject" title="Direct link to heading">â€‹</a></h2><p>Switching to Delta Lake format is easy with Smart Data Lake Builder, just replace <code>CsvFileDataObject</code> with <code>DeltaLakeTableDataObject</code> and define the table&#x27;s db and name.
Let&#x27;s start by changing the existing definition for <code>int-airports</code>:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">int-airports {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = DeltaLakeTableDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    path = &quot;~{id}&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        db = default</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        name = int_airports</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Then create a new, similar data object <code>int-departures</code>: </p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">int-departures {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = DeltaLakeTableDataObject</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    path = &quot;~{id}&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    table = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        db = default</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        name = int_departures</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Next, create a new action <code>prepare-departures</code> in the <code>actions</code> section to fill the new table with the data:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">prepare-departures {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    type = CopyAction</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    inputId = stg-departures</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    outputId = int-departures</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    metadata {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        feed = compute</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>Finally, adapt the action definition for <code>join-departures-airports</code>:</p><ul><li>change <code>stg-departures</code> to <code>int-departures</code> in inputIds</li><li>change <code>stg_departures</code> to <code>int_departures</code> in the first SQLDfsTransformer (watch out, you need to replace the string 4 times)</li></ul><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Explanation</h5></div><div class="admonition-content"><ul><li>We changed <code>int-airports</code> from CSV to Delta Lake format</li><li>Created an additional table <code>int-departures</code></li><li>Created an action <code>prepare-departures</code>  to fill the new integration layer table <code>int-departures</code></li><li>Adapted the existing action <code>join-departures-airports</code> to use the new table <code>int-departures</code></li></ul></div></div><p>To run our data pipeline, first delete the data directory - otherwise DeltaLakeTableDataObject will fail because of existing files in different format.
Then you can execute the usual <em>docker run</em> command for all feeds:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">mkdir -f data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest -c /mnt/config --feed-sel &#x27;download*&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config sdl-spark:latest -c /mnt/config --feed-sel &#x27;^(?!download).*&#x27;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</h5></div><div class="admonition-content"><p>Why two separate commands?<br>
<!-- -->Because you deleted all data first.   </p><p>Remember from part 1 that we either need to define a schema for our downloaded files or we need to execute the download steps separately on the first run.
The first command only executes the download steps, the second command executes everything but the download steps (regex with negative lookahead).
See <a href="/docs/getting-started/troubleshooting/common-problems">Common Problems</a> for more Info.</p></div></div><p>Getting an error like <code>io.smartdatalake.util.webservice.WebserviceException: Read timed out</code>? Check the list of <a href="/docs/getting-started/troubleshooting/common-problems">Common Problems</a> for a workaround.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="reading-delta-lake-format-with-spark">Reading Delta Lake Format with Spark<a class="hash-link" href="#reading-delta-lake-format-with-spark" title="Direct link to heading">â€‹</a></h2><p>Checking our results gets more complicated now - we can&#x27;t just open delta lake format in a text editor like we used to do for CSV files.
We could now use SQL to query our results, that would be even better.
One option is to use a Spark session, i.e. by starting a spark-shell.
But state-of-the-art is to use notebooks like Jupyter for this.
One of the most advanced notebooks for Scala code we found is Polynote, see <a href="https://polynote.org/" target="_blank" rel="noopener noreferrer">polynote.org</a>.</p><p>We will now start Polynote in a docker container, and an external Metastore (Derby database) in another container to share the catalog between our experiments and the notebook.
To do so you need to add additional files to the project. Change to the projects root directory and <strong>unzip part2.additional-files.zip</strong> into the project&#x27;s root directoy, then run the following commands in the projects root directory:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">docker-compose build</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mkdir -p data/_metastore</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">docker-compose up</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>This might take multiple minutes.
You should now be able to access Polynote at <code>localhost:8192</code>. </p><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Docker on Windows</h5></div><div class="admonition-content"><p>If you use Windows, please read our note on <a href="/docs/getting-started/troubleshooting/docker-on-windows">Docker for Windows</a>.</p></div></div><p>But when you walk through the prepared notebook &quot;SelectingData&quot;, you won&#x27;t see any tables and data yet.
Can you guess why?<br>
<!-- -->This is because your last pipeline run used an internal metastore, and not the external metastore we started with docker-compose yet.
To configure Spark to use our external metastore, add the following spark properties to the application.conf under global.spark-options.
You probably don&#x27;t have a global section in your application.conf yet, so here is the full block you need to add at the top of the file:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">global {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  spark-options {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;spark.hadoop.javax.jdo.option.ConnectionURL&quot; = &quot;jdbc:derby://metastore:1527/db;create=true&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;spark.hadoop.javax.jdo.option.ConnectionDriverName&quot; = &quot;org.apache.derby.jdbc.ClientDriver&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;spark.hadoop.javax.jdo.option.ConnectionUserName&quot; = &quot;sa&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &quot;spark.hadoop.javax.jdo.option.ConnectionPassword&quot; = &quot;1234&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>This instructs Spark to use the external metastore you started with docker-compose.
Your Smart Data Lake container doesn&#x27;t have access to the other containers just yet.
So when you run your data pipeline again, you need to add a parameter <code>--network getting-started_default</code> to join the virtual network where the metastore is located:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">docker run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config --network getting-started_default sdl-spark:latest -c /mnt/config --feed-sel &#x27;.*&#x27;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>When using podman you need to join the pod where the metastore is located with <code>--pod getting-started</code>:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">podman run --rm -v ${PWD}/data:/mnt/data -v ${PWD}/target:/mnt/lib -v ${PWD}/config:/mnt/config --pod getting-started sdl-spark:latest -c /mnt/config --feed-sel &#x27;.*&#x27;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p>After you run your data pipeline again, you should now be able to see our DataObjects data in Polynote.
No need to restart Polynote, just open it again and run all cells.
<a target="_blank" href="/assets/files/application-part2-deltalake-d1d7d6eef2fc2f225f2f87fff7ec0481.conf">This</a> is how the final configuration file should look like. Feel free to play around.</p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Delta Lake tuning</h5></div><div class="admonition-content"><p>You might have seen that our data pipeline with DeltaTableDataObject runs a Spark stage with 50 tasks several times.
This is delta lake reading it&#x27;s transaction log with Spark. For our data volume, 50 tasks are way too much.
You can reduce the number of snapshot partitions to speed up the execution by setting the following Spark property in your <code>application.conf</code> under <code>global.spark-options</code>:</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">&quot;spark.databricks.delta.snapshotPartitions&quot; = 2</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div></div></div><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Spark UI from Polynote</h5></div><div class="admonition-content"><p>On the right side of Polynote you find a link to the Spark UI for the current notebooks Spark session.
If it doesn&#x27;t work, try to replace 127.0.0.1 with localhost. If it still doesn&#x27;t work, replace with IP address of WSL (<code>wsl hostname -I</code>). </p></div></div><p>In the next step, we are going to take a look at keeping historical data...</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/smart-data-lake/smart-data-lake/tree/documentation/docs/getting-started/part-2/delta-lake-format.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/getting-started/part-2/industrializing"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Industrializing our data pipeline</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/getting-started/part-2/historical-data"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Keeping historical data</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#goal" class="table-of-contents__link toc-highlight">Goal</a></li><li><a href="#file-formats" class="table-of-contents__link toc-highlight">File formats</a></li><li><a href="#catalog" class="table-of-contents__link toc-highlight">Catalog</a></li><li><a href="#transactions" class="table-of-contents__link toc-highlight">Transactions</a></li><li><a href="#deltalaketabledataobject" class="table-of-contents__link toc-highlight">DeltaLakeTableDataObject</a></li><li><a href="#reading-delta-lake-format-with-spark" class="table-of-contents__link toc-highlight">Reading Delta Lake Format with Spark</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/">Getting started</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/smart-data-lake/smart-data-lake" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://github.com/smart-data-lake/smart-data-lake/issues" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub Issues<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://www.elca.ch" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>ELCA<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 Smart Data Lake, Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.a3d10d32.js"></script>
<script src="/assets/js/main.33d7ec33.js"></script>
</body>
</html>